{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMKzfSjdPGAs9ZVCYxpN1Fq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moathzyout/Project/blob/main/Binary_Classfication_tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A2fOnKV92Mm"
      },
      "source": [
        "'''from androguard.core.bytecodes.apk import APK\n",
        "import re\n",
        "from androguard import misc\n",
        "import os, sys\n",
        "from androguard.core.analysis import  analysis\n",
        "import csv\n",
        "import re\n",
        "import os, sys\n",
        "import subprocess\n",
        "from androguard.misc import AnalyzeAPK\n",
        "path = 'U:\\\\Data_T\\\\Benign'\n",
        "dirs = os.listdir(path)\n",
        "#This would print all the files and directories\n",
        "\n",
        "#This would print all the files and directories\n",
        "dic_perm={}\n",
        "dic_feat={}\n",
        "dic_act={}\n",
        "dic_ser={}\n",
        "dic_method={}\n",
        "dic_class={}\n",
        "\n",
        "i=0\n",
        "for subdir in dirs:\n",
        "    for i,file in enumerate(os.listdir(os.path.join(path,subdir))):\n",
        "        print(i)\n",
        "        new=os.path.join(path,subdir)\n",
        "        new=os.path.join(new,file)\n",
        "\n",
        "        try:\n",
        "\n",
        "                print(new)\n",
        "                a, d, dx = AnalyzeAPK(new)\n",
        "\n",
        "               # d, dx = analysis(\"./apks/classes.dex\")\n",
        "               # dic.update({'name':file,'permissions':a.get_permissions(),'activities':a.get_activities(),'Services':a.get_services(),'receivers':a.get_receivers(),'features':a.get_features()})\n",
        "                print(i,file)\n",
        "                string=' '\n",
        "                for  m in a.get_permissions():\n",
        "                     string= string+m+'  '\n",
        "                dic_perm.update({file:string})\n",
        "                feat=' '\n",
        "                for  m in a.get_features():\n",
        "                    feat=feat+m+'  '\n",
        "                dic_feat.update({file:feat})\n",
        "\n",
        "                serv=' '\n",
        "                for  m in a.get_services():\n",
        "                    serv=serv+m+' '\n",
        "                dic_ser.update({file:serv})\n",
        "                act=' '\n",
        "                for  m in a.get_activities():\n",
        "                    act=act+m+' '\n",
        "                dic_act.update({file:act})\n",
        "                clas=' '\n",
        "                for  m in  dx.get_classes():\n",
        "                    clas = clas + str(m.name) +'  '\n",
        "                dic_class.update({file:clas})\n",
        "                method='  '\n",
        "                for c in  dx.get_classes():\n",
        "                    for mc in c.get_methods():\n",
        "                        method = method + str(mc.name) +'  '\n",
        "                dic_method.update({file:method})\n",
        "        except FileNotFoundError:\n",
        "                print('File not found!!')\n",
        "                pass\n",
        "                continue\n",
        "        except :\n",
        "                print('File not found!!')\n",
        "                pass\n",
        "                continue \n",
        "\n",
        "\n",
        "        print('complete......'+str(subdir))\n",
        "\n",
        "import pandas as pd\n",
        "Mal_perm = pd.DataFrame(list(dic_perm.items()),columns = ['APK_Name','permissions'])'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtD0dyDshnX0"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TCaee-GiRf9"
      },
      "source": [
        "\n",
        "from sklearn import metrics\n",
        "import re\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation,Bidirectional,MaxPool3D,MaxPool2D,SpatialDropout1D,UpSampling2D,Embedding,Reshape,MaxPooling2D,Convolution2D,Convolution1D,Input,LSTM,Dropout,Flatten,BatchNormalization,MaxPooling1D,GlobalMaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop\n",
        "import pandas as pd \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation,BatchNormalization,Masking,UpSampling2D,Reshape,MaxPooling2D,Convolution2D,Convolution1D,Input,LSTM,Dropout,Flatten,BatchNormalization,MaxPooling1D,GlobalMaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-RWSFfAb6Cn"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znQ0q_xCpl17"
      },
      "source": [
        "#from _typeshed import OpenTextMode\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJX498ojrXIR"
      },
      "source": [
        "binary_label=[ ]\n",
        " \n",
        "per_level=pd.read_csv('/content/drive/My Drive/Mal-prem.csv')\n",
        "per_level['binary']=' '\n",
        "for i,m in enumerate(per_level['type']) :\n",
        "  if m=='Benign':\n",
        "     binary_label.append('B')\n",
        "  else:\n",
        "    binary_label.append('M')\n",
        "per_level['binary']= binary_label\n",
        "per_level.binary\n",
        "import  tensorflow as tf \n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "encode_label=encoder.fit_transform(per_level['binary'])\n",
        "labels_full=to_categorical(encode_label)\n",
        "labels=np.unique(per_level.binary).tolist()\n",
        "labels\n",
        "per_level=per_level.drop('type',axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfQ3Rk82kWll"
      },
      "source": [
        "#permiss = per_char['char_level'].values\n",
        "per_level['permmisions']=per_level['permmisions'].apply(lambda x:str(x))\n",
        "permiss=per_level['permmisions']\n",
        "Count_perm=per_level['permmisions'].apply(lambda x:len(x.strip().replace('android.permission.','').replace('android.','').replace(',','.').replace('_','.').split('.')))\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_length = max([len(s.replace('.','-').split('_')) for s in permiss])\n",
        "max_length\n",
        "#permiss_char=per_char['char_level'].apply(lambda x:str(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFRveH1aXpHB"
      },
      "source": [
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "#max_length_char = max([len(s) for s in permiss_char])\n",
        "#max_length_char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMmBGEn8PP_0"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "def tokn(text):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(text)\n",
        "\n",
        "    train = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "    vocab_size\n",
        "    return train,vocab_size\n",
        "train,vocab_size_word=tokn(permiss)\n",
        "print (vocab_size_word)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( train, labels_full, test_size=0.33,random_state=42,shuffle=True)\n",
        "#perm_train_ch,perm_test_ch, y_train_ch, y_test_ch = train_test_split( train_char, labels_full, test_size=0.33, random_state=42,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoded_docs,vocab_size_word=tokn(permiss)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 249\n",
        "\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "def conv_1d():\n",
        "    model_c1 = Sequential()\n",
        "    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n",
        "    \n",
        "    model_c1.add(Dropout(0.4))\n",
        "   \n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Flatten())\n",
        "    \n",
        "    model_c1.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model_c1.add(Dense(2, activation='softmax'))\n",
        "    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "              tf. keras.metrics.FalsePositives(name='fp'),\n",
        "              tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "              tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "              tf.keras.metrics.Precision(name='precision'),\n",
        "              tf.keras.metrics.Recall(name='recall'),\n",
        "              tf.keras.metrics.AUC(name='auc')]\n",
        "\n",
        "    model_c1.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=metrics1)\n",
        "    return model_c1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_c1=conv_1d()\n",
        "metrics2=[\"accuracy\"]\n",
        "start = time.process_time()\n",
        "# fit network\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
        "history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])\n",
        "plot_history(history_c1)"
      ],
      "metadata": {
        "id": "hpaaeoQVqhT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn9_SxfTYm_2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6JLFlNgbF28"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "maxlen = 249\n",
        "\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "#maxlen_char  = max_length_char\n",
        "#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)\n",
        "#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)\n",
        "\n",
        "def model_lstm(embedding_dim ,vocab_size,maxlen):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(input_dim=vocab_size, \n",
        "                                output_dim=embedding_dim, \n",
        "                                input_length=maxlen))\n",
        "      model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n",
        "\n",
        "      model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "      model.add(SpatialDropout1D(0.7))\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "      model.add(Flatten())\n",
        "      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "      model.add(Dense(maxlen, activation='relu'))\n",
        "      model.add(Dropout(0.8))\n",
        "      model.add(Dense(2, activation='softmax'))\n",
        "      return model\n",
        "epochs = 50\n",
        "model=model_lstm(110,vocab_size_word,maxlen)\n",
        "batch_size = 256\n",
        "metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "metrics2=[\"accuracy\"]\n",
        "model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)\n",
        "model.summary()\n",
        "#print(model.layers[0].get_weights()[0])\n",
        "import time\n",
        "start = time.process_time()\n",
        "print(start)\n",
        "history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n",
        "\n",
        "#loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history_lstm)\n",
        "end = time.process_time()\n",
        "print(\"Time for LSTM model : {} \".format((end-start)))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}