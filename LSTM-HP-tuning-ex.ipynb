pip install scikeras[tensorflow]
from keras.preprocessing.sequence import pad_sequences

import scikeras
from scikeras.wrappers import KerasClassifier

maxlen = 249
from keras.preprocessing.text import Tokenizer
def tokn(text):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(text)

    train = tokenizer.texts_to_sequences(text)


    vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index
    vocab_size
    return train,vocab_size
train,vocab_size_word=tokn(permiss)
print (vocab_size_word)
X_train = pad_sequences(train, padding='post', maxlen=maxlen)

#maxlen_char  = max_length_char
#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)
#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)
import time
start = time.process_time()
print(start)
def model_lstm(embedding_dim ,vocab_size,maxlen):
      model = Sequential()
      model.add(Embedding(input_dim=vocab_size, 
                                output_dim=embedding_dim, 
                                input_length=maxlen))
      model.add(SpatialDropout1D(0.3))

      model.add((LSTM(128,dropout=0.5,return_sequences=True)))

      model.add(SpatialDropout1D(0.6))


      model.add((LSTM(128,dropout=0.5, return_sequences=True)))
      model.add(SpatialDropout1D(0.7))
      
      


      model.add(Flatten())
      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer="gaussian"))
      model.add(Dense(maxlen, activation='relu'))
      model.add(Dropout(0.8))
      model.add(Dense(5, activation='softmax'))
      epochs = 50
     
      metrics1=[tf.keras.metrics.TruePositives(name='tp'),
            tf. keras.metrics.FalsePositives(name='fp'),
            tf.keras.metrics.TrueNegatives(name='tn'),
            tf.keras.metrics.FalseNegatives(name='fn'),
            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall'),
            tf.keras.metrics.AUC(name='auc')]
      metrics2=["accuracy"]
      model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)
      return model
for b in [32,64,128,256,512,1024]:
    model=model_lstm(110,vocab_size_word,maxlen)
    clf = KerasClassifier(model, epochs=50, batch_size=b, verbose=0)
    from sklearn.model_selection import StratifiedKFold, cross_val_score

    trans = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

    import pandas as pd

    # Keras classifiers work with one hot encoded categorical columns (e.g. [[1 0 0], [0 1 0], ...]).
    # StratifiedKFold works with categorical encoded columns (e.g. [1 2 3 1 ...]).
    # This requires juggling the representation at shuffle time versus at runtime.
    scores = []
    history_model=[]
    for train_idx, test_idx in trans.split(X_train, np.argmax(labels_full,axis=1)):
        X_cv, y_cv = X_train[train_idx],pd.get_dummies(np.argmax(labels_full,axis=1)[train_idx]).values
        test_cv,y_test_cv =X_train[test_idx],pd.get_dummies(np.argmax(labels_full,axis=1)[test_idx]).values
        history=clf.fit(X_cv, y_cv,callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)],validation_data=(test_cv,y_test_cv))
        history_model.append(history.history_)
        scores.append(clf.score(X_cv, y_cv))
    print("batches is " + str(b))
    print(np.mean(history.history_['val_recall']))
    print(np.mean(history.history_['val_precision']))
    print(np.mean(history.history_['val_accuracy']))
    end = time.process_time()
    print("Time for LSTM model : {} ".format((end-start)))