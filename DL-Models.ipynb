{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1uBf4N8-FrYUO66bjGr32Er2X9leRtb8p",
      "authorship_tag": "ABX9TyMajxBU9dB54V+fVxKY8/Qi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moathzyout/Project/blob/main/DL-Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A2fOnKV92Mm"
      },
      "source": [
        "'''from androguard.core.bytecodes.apk import APK\n",
        "import re\n",
        "from androguard import misc\n",
        "import os, sys\n",
        "from androguard.core.analysis import  analysis\n",
        "import csv\n",
        "import re\n",
        "import os, sys\n",
        "import subprocess\n",
        "from androguard.misc import AnalyzeAPK\n",
        "path = 'U:\\\\Data_T\\\\Benign'\n",
        "dirs = os.listdir(path)\n",
        "#This would print all the files and directories\n",
        "\n",
        "#This would print all the files and directories\n",
        "dic_perm={}\n",
        "dic_feat={}\n",
        "dic_act={}\n",
        "dic_ser={}\n",
        "dic_method={}\n",
        "dic_class={}\n",
        "\n",
        "i=0\n",
        "for subdir in dirs:\n",
        "    for i,file in enumerate(os.listdir(os.path.join(path,subdir))):\n",
        "        print(i)\n",
        "        new=os.path.join(path,subdir)\n",
        "        new=os.path.join(new,file)\n",
        "\n",
        "        try:\n",
        "\n",
        "                print(new)\n",
        "                a, d, dx = AnalyzeAPK(new)\n",
        "\n",
        "               # d, dx = analysis(\"./apks/classes.dex\")\n",
        "               # dic.update({'name':file,'permissions':a.get_permissions(),'activities':a.get_activities(),'Services':a.get_services(),'receivers':a.get_receivers(),'features':a.get_features()})\n",
        "                print(i,file)\n",
        "                string=' '\n",
        "                for  m in a.get_permissions():\n",
        "                     string= string+m+'  '\n",
        "                dic_perm.update({file:string})\n",
        "                feat=' '\n",
        "                for  m in a.get_features():\n",
        "                    feat=feat+m+'  '\n",
        "                dic_feat.update({file:feat})\n",
        "\n",
        "                serv=' '\n",
        "                for  m in a.get_services():\n",
        "                    serv=serv+m+' '\n",
        "                dic_ser.update({file:serv})\n",
        "                act=' '\n",
        "                for  m in a.get_activities():\n",
        "                    act=act+m+' '\n",
        "                dic_act.update({file:act})\n",
        "                clas=' '\n",
        "                for  m in  dx.get_classes():\n",
        "                    clas = clas + str(m.name) +'  '\n",
        "                dic_class.update({file:clas})\n",
        "                method='  '\n",
        "                for c in  dx.get_classes():\n",
        "                    for mc in c.get_methods():\n",
        "                        method = method + str(mc.name) +'  '\n",
        "                dic_method.update({file:method})\n",
        "        except FileNotFoundError:\n",
        "                print('File not found!!')\n",
        "                pass\n",
        "                continue\n",
        "        except :\n",
        "                print('File not found!!')\n",
        "                pass\n",
        "                continue \n",
        "\n",
        "\n",
        "        print('complete......'+str(subdir))\n",
        "\n",
        "import pandas as pd\n",
        "Mal_perm = pd.DataFrame(list(dic_perm.items()),columns = ['APK_Name','permissions'])'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtD0dyDshnX0"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TCaee-GiRf9"
      },
      "source": [
        "\n",
        "from sklearn import metrics\n",
        "import re\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation,Bidirectional,MaxPool3D,MaxPool2D,SpatialDropout1D,UpSampling2D,Embedding,Reshape,MaxPooling2D,Convolution2D,Convolution1D,Input,LSTM,Dropout,Flatten,BatchNormalization,MaxPooling1D,GlobalMaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop\n",
        "import pandas as pd \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation,BatchNormalization,Masking,UpSampling2D,Reshape,MaxPooling2D,Convolution2D,Convolution1D,Input,LSTM,Dropout,Flatten,BatchNormalization,MaxPooling1D,GlobalMaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-RWSFfAb6Cn"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znQ0q_xCpl17"
      },
      "source": [
        "#from _typeshed import OpenTextMode\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.backend import batch_get_value\n",
        "def plot_history_opt(history,opt,batch):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy for '+ opt+' with '+batch)\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss for '+ opt+' with '+batch)\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "l4XNgQgijuDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCfV4RKmRbq1"
      },
      "source": [
        "import sklearn\n",
        "import seaborn as sns \n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "def plot_history_loss(history):\n",
        "   \n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(loss) + 1)\n",
        "  \n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "def plot_multiclass_roc(clf,x_test, y_test, n_classes, figsize=(17, 10)):\n",
        "\n",
        "    # structures\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    \n",
        "    roc_auc = dict()\n",
        "    y_score1 = clf.predict(x_test)\n",
        "    # calculate dummies once\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = sklearn.metrics.roc_curve(y_test[:, i].ravel(), y_score1[:, i])\n",
        "        roc_auc[i] = sklearn.metrics.auc(fpr[i], tpr[i])\n",
        "\n",
        "    # roc for each class\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.plot([0, 1], [0, 1], 'k--')\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title('Receiver operating characteristic example')\n",
        "    for i in range(n_classes):\n",
        "        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for label %s' % (roc_auc[i], labels[i]))\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.grid(alpha=.4)\n",
        "    sns.despine()\n",
        "    plt.show()\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 10)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "def plot_metrics(history):\n",
        "    import matplotlib.pyplot as plt\n",
        "    metrics = ['loss', 'auc', 'precision', 'recall']\n",
        "    for n, metric in enumerate(metrics):\n",
        "        name = metric.replace(\"_\",\" \").capitalize()\n",
        "        plt.subplot(2,2,n+1)\n",
        "        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
        "        plt.plot(history.epoch, history.history['val_'+metric],\n",
        "                 color=colors[0], linestyle=\"--\", label='Val')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel(name)\n",
        "        if metric == 'loss':\n",
        "            plt.ylim([0, plt.ylim()[1]])\n",
        "        elif metric == 'auc':\n",
        "             plt.ylim([0.8,1])\n",
        "        else:\n",
        "            plt.ylim([0,1])\n",
        "\n",
        "        plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8PYgoGGoOIk"
      },
      "source": [
        "#per_char=pd.read_csv('/content/drive/My Drive/Char_perm.csv')\n",
        "per_level=pd.read_csv('/content/drive/My Drive/Mal-Prem.csv')\n",
        "import  tensorflow as tf \n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "encode_label=encoder.fit_transform(per_level['type'])\n",
        "labels_full=to_categorical(encode_label)\n",
        "labels=np.unique(per_level.type).tolist()\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wJGX4_YBGKt"
      },
      "source": [
        "Adware=per_level[per_level['type']=='SMS']\n",
        "Adware"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XjrLIgXvA_L"
      },
      "source": [
        "pivot=pd.pivot_table(\n",
        "                     per_level,\n",
        "                     values='Unnamed: 0.1',\n",
        "                    index=['permmisions'],\n",
        "                     columns=['type']\n",
        "                     )\n",
        "pivot=pivot.fillna(0).astype(np.int64)\n",
        "pivot['Riskware']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmtI1liQyAMd"
      },
      "source": [
        "#plot = pivot[1:5].plot(kind='bar',figsize=(3,6))\n",
        "#plot.tick_params(rotation=40)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJX498ojrXIR"
      },
      "source": [
        "binary_label=[ ]\n",
        " \n",
        "per_level=pd.read_csv('/content/drive/My Drive/Mal-prem.csv')\n",
        "per_level['binary']=' '\n",
        "for i,m in enumerate(per_level['type']) :\n",
        "  if m=='Benign':\n",
        "     binary_label.append('B')\n",
        "  else:\n",
        "    binary_label.append('M')\n",
        "per_level['binary']= binary_label\n",
        "per_level.binary\n",
        "import  tensorflow as tf \n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "encode_label=encoder.fit_transform(per_level['binary'])\n",
        "labels_full=to_categorical(encode_label)\n",
        "labels=np.unique(per_level.binary).tolist()\n",
        "labels\n",
        "per_level=per_level.drop('type',axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfQ3Rk82kWll"
      },
      "source": [
        "#permiss = per_char['char_level'].values\n",
        "per_level['permmisions']=per_level['permmisions'].apply(lambda x:str(x))\n",
        "permiss=per_level['permmisions']\n",
        "Count_perm=per_level['permmisions'].apply(lambda x:len(x.strip().replace('android.permission.','').replace('android.','').replace(',','.').replace('_','.').split('.')))\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_length = max([len(s.replace('.','-').split('_')) for s in permiss])\n",
        "max_length\n",
        "#permiss_char=per_char['char_level'].apply(lambda x:str(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFRveH1aXpHB"
      },
      "source": [
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "#max_length_char = max([len(s) for s in permiss_char])\n",
        "#max_length_char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMmBGEn8PP_0"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "def tokn(text):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(text)\n",
        "\n",
        "    train = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "    vocab_size\n",
        "    return train,vocab_size\n",
        "train,vocab_size_word=tokn(permiss)\n",
        "print (vocab_size_word)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( train, labels_full, test_size=0.33,random_state=42,shuffle=True)\n",
        "#perm_train_ch,perm_test_ch, y_train_ch, y_test_ch = train_test_split( train_char, labels_full, test_size=0.33, random_state=42,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoded_docs,vocab_size_word=tokn(permiss)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 249\n",
        "\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "def conv_1d():\n",
        "    model_c1 = Sequential()\n",
        "    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n",
        "    \n",
        "    model_c1.add(Dropout(0.4))\n",
        "   \n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Flatten())\n",
        "    \n",
        "    model_c1.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model_c1.add(Dense(2, activation='softmax'))\n",
        "    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "              tf. keras.metrics.FalsePositives(name='fp'),\n",
        "              tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "              tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "              tf.keras.metrics.Precision(name='precision'),\n",
        "              tf.keras.metrics.Recall(name='recall'),\n",
        "              tf.keras.metrics.AUC(name='auc')]\n",
        "\n",
        "    model_c1.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=metrics1)\n",
        "    return model_c1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_c1=conv_1d()\n",
        "metrics2=[\"accuracy\"]\n",
        "start = time.process_time()\n",
        "# fit network\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
        "history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])\n",
        "plot_history(history_c1)"
      ],
      "metadata": {
        "id": "hpaaeoQVqhT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn9_SxfTYm_2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6JLFlNgbF28"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "maxlen = 249\n",
        "\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "#maxlen_char  = max_length_char\n",
        "#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)\n",
        "#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)\n",
        "\n",
        "def model_lstm(embedding_dim ,vocab_size,maxlen):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(input_dim=vocab_size, \n",
        "                                output_dim=embedding_dim, \n",
        "                                input_length=maxlen))\n",
        "      model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n",
        "\n",
        "      model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "      model.add(SpatialDropout1D(0.7))\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "      model.add(Flatten())\n",
        "      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "      model.add(Dense(maxlen, activation='relu'))\n",
        "      model.add(Dropout(0.8))\n",
        "      model.add(Dense(2, activation='softmax'))\n",
        "      return model\n",
        "epochs = 50\n",
        "model=model_lstm(110,vocab_size_word,maxlen)\n",
        "batch_size = 256\n",
        "metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "metrics2=[\"accuracy\"]\n",
        "model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)\n",
        "model.summary()\n",
        "#print(model.layers[0].get_weights()[0])\n",
        "import time\n",
        "start = time.process_time()\n",
        "print(start)\n",
        "history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n",
        "\n",
        "#loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history_lstm)\n",
        "end = time.process_time()\n",
        "print(\"Time for LSTM model : {} \".format((end-start)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras[tensorflow]"
      ],
      "metadata": {
        "id": "odvzO-XW4zqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import scikeras\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "maxlen = 249\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "def tokn(text):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(text)\n",
        "\n",
        "    train = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "    vocab_size\n",
        "    return train,vocab_size\n",
        "train,vocab_size_word=tokn(permiss)\n",
        "print (vocab_size_word)\n",
        "X_train = pad_sequences(train, padding='post', maxlen=maxlen)\n",
        "\n",
        "#maxlen_char  = max_length_char\n",
        "#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)\n",
        "#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)\n",
        "import time\n",
        "start = time.process_time()\n",
        "print(start)\n",
        "def model_lstm(embedding_dim ,vocab_size,maxlen):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(input_dim=vocab_size, \n",
        "                                output_dim=embedding_dim, \n",
        "                                input_length=maxlen))\n",
        "      model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n",
        "\n",
        "      model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "      model.add(SpatialDropout1D(0.7))\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "      model.add(Flatten())\n",
        "      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "      model.add(Dense(maxlen, activation='relu'))\n",
        "      model.add(Dropout(0.8))\n",
        "      model.add(Dense(5, activation='softmax'))\n",
        "      epochs = 50\n",
        "     \n",
        "      metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "      metrics2=[\"accuracy\"]\n",
        "      model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)\n",
        "      return model\n",
        "for b in [32,64,128,256,512,1024]:\n",
        "    model=model_lstm(110,vocab_size_word,maxlen)\n",
        "    clf = KerasClassifier(model, epochs=50, batch_size=b, verbose=0)\n",
        "    from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "    trans = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    # Keras classifiers work with one hot encoded categorical columns (e.g. [[1 0 0], [0 1 0], ...]).\n",
        "    # StratifiedKFold works with categorical encoded columns (e.g. [1 2 3 1 ...]).\n",
        "    # This requires juggling the representation at shuffle time versus at runtime.\n",
        "    scores = []\n",
        "    history_model=[]\n",
        "    for train_idx, test_idx in trans.split(X_train, np.argmax(labels_full,axis=1)):\n",
        "        X_cv, y_cv = X_train[train_idx],pd.get_dummies(np.argmax(labels_full,axis=1)[train_idx]).values\n",
        "        test_cv,y_test_cv =X_train[test_idx],pd.get_dummies(np.argmax(labels_full,axis=1)[test_idx]).values\n",
        "        history=clf.fit(X_cv, y_cv,callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)],validation_data=(test_cv,y_test_cv))\n",
        "        history_model.append(history.history_)\n",
        "        scores.append(clf.score(X_cv, y_cv))\n",
        "    print(\"batches is \" + str(b))\n",
        "    print(np.mean(history.history_['val_recall']))\n",
        "    print(np.mean(history.history_['val_precision']))\n",
        "    print(np.mean(history.history_['val_accuracy']))\n",
        "    end = time.process_time()\n",
        "    print(\"Time for LSTM model : {} \".format((end-start)))"
      ],
      "metadata": {
        "id": "93-skjL_0KEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoded_docs,vocab_size_word=tokn(permiss)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 249\n",
        "\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "def conv_1d():\n",
        "    model_c1 = Sequential()\n",
        "    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n",
        "    \n",
        "    model_c1.add(Dropout(0.4))\n",
        "   \n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Flatten())\n",
        "    \n",
        "    model_c1.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model_c1.add(Dense(5, activation='softmax'))\n",
        "    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "              tf. keras.metrics.FalsePositives(name='fp'),\n",
        "              tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "              tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "              tf.keras.metrics.Precision(name='precision'),\n",
        "              tf.keras.metrics.Recall(name='recall'),\n",
        "              tf.keras.metrics.AUC(name='auc')]\n",
        "\n",
        "    model_c1.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=metrics1)\n",
        "    return model_c1\n",
        "    model=conv_1d()\n",
        "    for batch in [32,64,128,256,512,1024]:\n",
        "        model_c1=conv_1d(opt)\n",
        "        metrics2=[\"accuracy\"]\n",
        "        start = time.process_time()\n",
        "        # fit network\n",
        "        from keras.callbacks import ReduceLROnPlateau\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "        #model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
        "        clf_conv = KerasClassifier(model, epochs=50, batch_size=b, verbose=0)\n",
        "        from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "        trans = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "        import pandas as pd\n",
        "\n",
        "        # Keras classifiers work with one hot encoded categorical columns (e.g. [[1 0 0], [0 1 0], ...]).\n",
        "        # StratifiedKFold works with categorical encoded columns (e.g. [1 2 3 1 ...]).\n",
        "        # This requires juggling the representation at shuffle time versus at runtime.\n",
        "        scores = []\n",
        "        history_model=[]\n",
        "        for train_idx, test_idx in trans.split(X_train, np.argmax(labels_full,axis=1)):\n",
        "            X_cv, y_cv = X_train[train_idx],pd.get_dummies(np.argmax(labels_full,axis=1)[train_idx]).values\n",
        "            test_cv,y_test_cv =X_train[test_idx],pd.get_dummies(np.argmax(labels_full,axis=1)[test_idx]).values\n",
        "            history=clf_conv.fit(X_cv, y_cv,callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)],validation_data=(test_cv,y_test_cv),callbacks=[reduce_lr])\n",
        "            history_model.append(history.history_)\n",
        "            scores.append(clf.score(X_cv, y_cv))\n",
        "        print(\"batches is \" + str(b))\n",
        "        print(np.mean(history.history_['val_recall']))\n",
        "        print(np.mean(history.history_['val_precision']))\n",
        "        print(np.mean(history.history_['val_accuracy']))\n",
        "        end = time.process_time()\n",
        "        print(\"Time for LSTM model : {} \".format((end-start)))"
      ],
      "metadata": {
        "id": "T6t8d82bI75K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(np.mean(history.history_['val_recall']))\n",
        "print(np.mean(history.history_['val_precision']))\n",
        "print(np.mean(history.history_['val_accuracy']))"
      ],
      "metadata": {
        "id": "nZ22uIGQIHep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "#print(model.layers[0].get_weights()[0])\n",
        "import time\n",
        "start = time.process_time()\n",
        "print(start)\n",
        "history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n",
        "\n",
        "#loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history_lstm)\n",
        "end = time.process_time()\n",
        "print(\"Time for LSTM model : {} \".format((end-start)))"
      ],
      "metadata": {
        "id": "SjBAOmjm7SlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install pretty_confusion_matrix"
      ],
      "metadata": {
        "id": "QWIRd690u2ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "Y_prediction = model.predict(X_test, verbose=1)\n",
        "# Convert predictions classes to one hot vectors \n",
        "\n",
        "\n",
        "Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n",
        "Y_pred_classes"
      ],
      "metadata": {
        "id": "RjTrpQdqQILN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "Y_prediction = model.predict(X_test, verbose=1)\n",
        "# Convert predictions classes to one hot vectors \n",
        "\n",
        "\n",
        "Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_test,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
        "\n",
        "df_cm = pd.DataFrame(confusion_mtx, index=np.unique(per_level.type.values), columns=np.unique(per_level.type.values))\n",
        "from pretty_confusion_matrix import pp_matrix\n",
        "cmap = 'PuRd'\n",
        "cm=pp_matrix(df_cm,cmap=plt.cm.CMRmap_r)\n"
      ],
      "metadata": {
        "id": "vPRnUtN_ug2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_prediction"
      ],
      "metadata": {
        "id": "K9io5YKXRCbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.argmax(Y_prediction,axis=1)\n",
        "Y_true"
      ],
      "metadata": {
        "id": "UnrrUbNQRCqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.argmax(Y_prediction,axis=1)\n",
        "Y_true\n",
        "y=np.argmax(Y_prediction,axis=1)\n",
        "Y_true\n",
        "print(f1_score(Y_true,y,average='micro'))"
      ],
      "metadata": {
        "id": "yGspToX2RAcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end = time.process_time()\n",
        "print(\"Time for LSTM model : {} \".format((end-start)))"
      ],
      "metadata": {
        "id": "BHdI_LvvEH6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "Y_prediction = model.predict(X_test, verbose=1)\n",
        "# Convert predictions classes to one hot vectors \n",
        "\n",
        "\n",
        "Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_test,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "plt.figure(figsize=(10,8))\n",
        "#sns.heatmap(confusion_mtx/np.sum(confusion_mtx), annot=True, fmt=\"d\",xticklabels=np.unique(per_level.type.values),yticklabels=np.unique(per_level.type.values));\n",
        "sns.heatmap(confusion_mtx/np.sum(confusion_mtx), annot=True,xticklabels=np.unique(per_level.type.values),yticklabels=np.unique(per_level.type.values));\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "jSyKb9Efkg9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 249\n",
        "epochs = 50\n",
        "batch_size = 256\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "#maxlen_char  = max_length_char\n",
        "#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)\n",
        "#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)\n",
        "\n",
        "def model_lstm(embedding_dim,vocab_size,maxlen,opt):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(input_dim=vocab_size, \n",
        "                                output_dim=embedding_dim, \n",
        "                                input_length=maxlen))\n",
        "      model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n",
        "\n",
        "      model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "      model.add(SpatialDropout1D(0.7))\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "      model.add(Flatten())\n",
        "      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "      model.add(Dense(maxlen, activation='relu'))\n",
        "      model.add(Dropout(0.8))\n",
        "      model.add(Dense(5, activation='softmax'))\n",
        "     \n",
        "      metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "      metrics2=[\"accuracy\"]\n",
        "      model.compile(loss='categorical_crossentropy',optimizer=opt, metrics= metrics1)\n",
        "      return model\n",
        "for m in ['Adam','RMSprop','SGD','Nadam']:\n",
        "    for batch in [32,64,1,28,256,512,1024]:\n",
        "        model =model_lstm(110,vocab_size_word,maxlen,m)\n",
        "\n",
        "    #print(model.layers[0].get_weights()[0])\n",
        "        import time\n",
        "        start = time.process_time()\n",
        "        print(start)\n",
        "        history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n",
        "\n",
        "        #loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "        #print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "        plot_history_opt(history_lstm,m,batch)\n",
        "        end = time.process_time()\n",
        "        print(\"Time for LSTM model : {} \".format((end-start)))\n"
      ],
      "metadata": {
        "id": "128IoozIcCC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoded_docs,vocab_size_word=tokn(permiss)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 249\n",
        "\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "def conv_1d(opt):\n",
        "    model_c1 = Sequential()\n",
        "    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n",
        "    \n",
        "    model_c1.add(Dropout(0.4))\n",
        "   \n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Flatten())\n",
        "    \n",
        "    model_c1.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model_c1.add(Dense(5, activation='softmax'))\n",
        "    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "              tf. keras.metrics.FalsePositives(name='fp'),\n",
        "              tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "              tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "              tf.keras.metrics.Precision(name='precision'),\n",
        "              tf.keras.metrics.Recall(name='recall'),\n",
        "              tf.keras.metrics.AUC(name='auc')]\n",
        "\n",
        "    model_c1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=metrics1)\n",
        "    return model_c1\n",
        "\n",
        "\n",
        "for opt in ['Adam','RMSprop','SGD','Nadam']:\n",
        "   for batch in [32,64,128,256,512,1024]:\n",
        "\n",
        "        model_c1=conv_1d(opt)\n",
        "        metrics2=[\"accuracy\"]\n",
        "        start = time.process_time()\n",
        "        # fit network\n",
        "        from keras.callbacks import ReduceLROnPlateau\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "        #model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
        "        history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=batch,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])\n",
        "        plot_history_opt(history_c1,opt,str(batch))"
      ],
      "metadata": {
        "id": "sD6Rjk2yfuVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per_level['permmisions']=per_level['permmisions'].apply(lambda x:str(x).lower())\n",
        "permiss=per_level['permmisions']\n",
        "perm_split=per_level['permmisions'].apply(lambda x:x.replace(',','').replace('android.permission.',' ').strip().split('.')[-1])\n",
        "lis_value=set()\n",
        "lis_u=set()\n",
        "lis_length=[ ]\n",
        "for perm in perm_split:\n",
        "    for p in perm.split(' ') :\n",
        "        if p!='' or p!='nan' :\n",
        "             lis_u.add(p)\n",
        "\n",
        "dic={}\n",
        "for i,perm in enumerate(perm_split):\n",
        "      lis_value=[ ]\n",
        "      for p in list(lis_u) :\n",
        "           if p in perm:\n",
        "              lis_value.append(1) \n",
        "           else:\n",
        "               lis_value.append(0) \n",
        "      dic.update({i:lis_value})\n",
        "data=pd.DataFrame(dic.values(),columns=list(lis_u))\n",
        "data[data['bind_notification_listener_service']==1]"
      ],
      "metadata": {
        "id": "nVx0Nq4LOA9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( data,labels_full, test_size=0.33,shuffle=True)\n",
        "X_train=np.expand_dims(X_train,-1)\n",
        "X_test=np.expand_dims(X_test,-1)\n",
        "X_train.shape,X_test.shape"
      ],
      "metadata": {
        "id": "IazwC5lEPWdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "   \n",
        "\n",
        "def Lst_model_binary(X_t):\n",
        "    model=Sequential()  \n",
        "    model.add((LSTM(128,dropout=0.5,input_shape=X_t.shape[1:],return_sequences=True)))\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "    model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n",
        "\n",
        "    model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "    model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "    model.add(SpatialDropout1D(0.7))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dropout(0.8))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "   \n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "                            patience=5, verbose=1, mode='auto')\n",
        "    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)\n",
        "    return model\n",
        "\n",
        "for batch in [32,64,128,256,512,1024]:\n",
        "    model =Lst_model_binary(X_train)\n",
        "    print('optmizer'+ m+str(batch))\n",
        "#print(model.layers[0].get_weights()[0])\n",
        "    import time\n",
        "    start = time.process_time()\n",
        "    print(start)\n",
        "    history_lstm = model.fit(X_train, y_train, epochs=50, batch_size=batch,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n",
        "    end = time.process_time()\n",
        "    print(\"Time for LSTM model : {} \".format((end-start)))\n"
      ],
      "metadata": {
        "id": "fp6wWG6WKlbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lis_value=()\n",
        "dic={}\n",
        "for i,perm in enumerate(perm_split):\n",
        "      lis_value=[ ]\n",
        "      for p in list(lis_u) :\n",
        "           if p in perm:\n",
        "              lis_value.append(1) \n",
        "           else:\n",
        "               lis_value.append(0) \n",
        "      dic.update({i:lis_value})\n",
        "data=pd.DataFrame(dic.values(),columns=list(lis_u))\n",
        "Full_data=pd.concat([data,per_level['type']],axis=1)"
      ],
      "metadata": {
        "id": "y_KKu8R5LByu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import  tensorflow as tf \n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "encode_label=encoder.fit_transform(per_level['type'])\n",
        "labels_full=to_categorical(encode_label)\n",
        "labels_full.shape"
      ],
      "metadata": {
        "id": "jkv92ds1izD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train,vocab_size_word=tokn(permiss)\n",
        "Full_train = pad_sequences(train, padding='post', maxlen=maxlen)\n",
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "import numpy\n",
        "import tensorflow  \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import scikeras\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "# Function to create model, required for KerasClassifier\n",
        "def model_lstm( optimizer=\"SGD\" ):\n",
        "      embedding_dim=110\n",
        "      vocab_size=vocab_size_word\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(input_dim=vocab_size, \n",
        "                                output_dim=embedding_dim, \n",
        "                                input_length=maxlen))\n",
        "      model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n",
        "\n",
        "      model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "      model.add(SpatialDropout1D(0.7))\n",
        "      \n",
        "      \n",
        "      model.add(Flatten())\n",
        "      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "      model.add(Dense(maxlen, activation='relu'))\n",
        "      model.add(Dropout(0.8))\n",
        "      model.add(Dense(5, activation='softmax'))\n",
        "      metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "      metrics2=[\"accuracy\"]\n",
        "      model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics= metrics2)\n",
        "      return model\n",
        "# fix random seed for reproducibility\n",
        "\n",
        "start=time.process_time()\n",
        "# split into input (X) and output (Y) variables\n",
        "X = Full_train\n",
        "Y =labels_full\n",
        "# create model\n",
        "model_grid = KerasClassifier(build_fn=model_lstm, verbose=0)\n",
        "# define the grid search parameters\n",
        "\n",
        "batches=[64,128,256,512,1024]\n",
        "epoches=[50]\n",
        "param_grid = dict(epochs=epoches,batch_size=batches)\n",
        "grid = GridSearchCV(estimator=model_grid, param_grid=param_grid,cv=50)\n",
        "grid_result = grid.fit(X, Y)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "end = time.process_time()\n",
        "print(\"Time for LSTM model : {} \".format((end-start)))"
      ],
      "metadata": {
        "id": "808fnu_LjHKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras[tensorflow]\n",
        "import scikeras\n",
        "from scikeras.wrappers import KerasClassifier"
      ],
      "metadata": {
        "id": "Rl2DZ-FFJr71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "import numpy\n",
        "import tensorflow  \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import scikeras\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "# Function to create model, required for KerasClassifier\n",
        "def model_lstm( optimizer=\"adam\" ):\n",
        "      embedding_dim=110\n",
        "      vocab_size=vocab_size_word\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(input_dim=vocab_size, \n",
        "                                output_dim=embedding_dim, \n",
        "                                input_length=maxlen))\n",
        "      model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n",
        "\n",
        "      model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "      model.add(SpatialDropout1D(0.7))\n",
        "      \n",
        "      \n",
        "      model.add(Flatten())\n",
        "      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "      model.add(Dense(maxlen, activation='relu'))\n",
        "      model.add(Dropout(0.8))\n",
        "      model.add(Dense(5, activation='softmax'))\n",
        "      metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "      metrics2=[\"accuracy\"]\n",
        "      model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics= metrics2)\n",
        "      return model\n",
        "# fix random seed for reproducibility\n",
        "\n",
        "start=time.process_time()\n",
        "# split into input (X) and output (Y) variables\n",
        "X = Full_train\n",
        "Y =labels_full\n",
        "# create model\n",
        "model_grid = KerasClassifier(build_fn=model_lstm, verbose=0)\n",
        "# define the grid search parameters\n",
        "optimizers = ['SGD','Adam']\n",
        "batches=[64,128,256,512,1024]\n",
        "epoches=[1]\n",
        "param_grid = dict(optimizer=optimizers,epochs=epoches,batch_size=batches)\n",
        "grid = GridSearchCV(estimator=model_grid, param_grid=param_grid,cv=5)\n",
        "grid_result = grid.fit(X, Y)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "end = time.process_time()\n",
        "print(\"Time for LSTM model : {} \".format((end-start)))"
      ],
      "metadata": {
        "id": "YFuoN1T_RcWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv-M3nMHgJUA"
      },
      "source": [
        "plot_multiclass_roc(model, X_test, y_test, n_classes=5, figsize=(5, 5))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfefAfio6Jjr"
      },
      "source": [
        "plot_metrics(history_lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsJT9SIM5pJl"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "Y_prediction = model.predict(X_test, verbose=1)\n",
        "# Convert predictions classes to one hot vectors \n",
        "\n",
        "\n",
        "Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_test,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(confusion_mtx, annot=True, fmt=\"d\",xticklabels=np.unique(per_level.type.values),yticklabels=np.unique(per_level.type.values));\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP5cww57kFBJ"
      },
      "source": [
        "encoded_docs,vocab_size_word=tokn(permiss)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 249\n",
        "\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlLABv5JGGt1"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def conv_1d():\n",
        "    model_c1 = Sequential()\n",
        "    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n",
        "    \n",
        "    model_c1.add(Dropout(0.4))\n",
        "   \n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Flatten())\n",
        "    \n",
        "    model_c1.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model_c1.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    print(model_c1.summary())\n",
        "    return model_c1\n",
        "\n",
        "metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "              tf. keras.metrics.FalsePositives(name='fp'),\n",
        "              tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "              tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "              tf.keras.metrics.Precision(name='precision'),\n",
        "              tf.keras.metrics.Recall(name='recall'),\n",
        "              tf.keras.metrics.AUC(name='auc')]\n",
        "model_c1=conv_1d()\n",
        "metrics2=[\"accuracy\"]\n",
        "start = time.process_time()\n",
        "model_c1.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=\"accuracy\")\n",
        "# fit network\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
        "\n",
        "history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTF_N9GfG8K1"
      },
      "source": [
        "plot_history(history_c1)\n",
        "end = time.process_time()\n",
        "print(\"Time for LSTM model : {} \".format((end-start)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH49Yd2fCjpQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "Y_prediction = model_c1.predict(X_test, verbose=1)\n",
        "# Convert predictions classes to one hot vectors \n",
        "\n",
        "\n",
        "Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_test,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
        "df_cm = pd.DataFrame(confusion_mtx, index=np.unique(per_level.type.values), columns=np.unique(per_level.type.values))\n",
        "from pretty_confusion_matrix import pp_matrix\n",
        "cmap = 'PuRd'\n",
        "pp_matrix(df_cm,cmap=plt.cm.CMRmap_r)"
      ],
      "metadata": {
        "id": "9cUZTk_2CJsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.argmax(Y_prediction,axis=1)\n",
        "Y_true\n",
        "print(f1_score(Y_true,y,average='micro'))"
      ],
      "metadata": {
        "id": "p-KPvTWOQyAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMKsBVoXZC2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "import numpy\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load dataset\n",
        "\n",
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import scikeras\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "train,vocab_size_word=tokn(permiss)\n",
        "Full_train = pad_sequences(train, padding='post', maxlen=maxlen)\n",
        "# Function to create model, required for KerasClassifier\n",
        "def conv_1d(optimizer='adam'):\n",
        "    model_c1 = Sequential()\n",
        "    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n",
        "    \n",
        "    model_c1.add(Dropout(0.4))\n",
        "   \n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Flatten())\n",
        "    \n",
        "    model_c1.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model_c1.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    print(model_c1.summary())\n",
        "    model_c1.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=\"accuracy\")\n",
        "    return model_c1\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = Full_train\n",
        "Y =labels_full\n",
        "# create model\n",
        "create_model=conv_1d()\n",
        "model = KerasClassifier(build_fn=create_model,verbose=0)\n",
        "# define the grid search parameters\n",
        "epochs= [50]\n",
        "batches=[64,128,256,512,1024]\n",
        "#[ 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "param_grid = dict(epochs=epochs,batch_size=batches)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
        "grid_result = grid.fit(X, Y)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "hW_JCIr3Q5rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZcF8tP5wKX8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI--bl3_G39Y"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "metrics = ['tp', 'fp', 'tn','fn']\n",
        "for n, metric in enumerate(metrics):\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.bar(history_c1.epoch, history_c1.history[metric], color=colors[0], label='Train')\n",
        "   \n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pwk-mD76DQ-"
      },
      "source": [
        "plot_multiclass_roc(model_c1, X_test, y_test, n_classes=5, figsize=(5, 5))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK0LkT6d6uLr"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "Y_prediction = model.predict(X_test, verbose=1)\n",
        "# Convert predictions classes to one hot vectors \n",
        "\n",
        "\n",
        "Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_test,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(confusion_mtx, annot=True, fmt=\"d\",xticklabels=np.unique(per_level.type.values),yticklabels=np.unique(per_level.type.values));\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWkRMxOM4xPn"
      },
      "source": [
        "\n",
        "metrics = ['tp', 'fp', 'tn','fn']\n",
        "histoiry_model=['history_c1','history_CNN']\n",
        "metric_c1 = {}\n",
        "for i in metrics :\n",
        "    name=i.capitalize()+'_' +'CONV1D'\n",
        "    metric_c1.update({name:history_c1.history[i]})\n",
        "    \n",
        "  \n",
        "\n",
        "metric_lstm = {}\n",
        "for i in metrics :\n",
        "    name=i.capitalize()+'_' +'lstm'\n",
        "    metric_lstm.update({name:history_lstm.history[i]})\n",
        "metric_lstm['Fn_lstm']\n",
        "names=['Fp_CONV1D','Fp_lstm','FN_CONV1D','FN_Lstm','TN_CONV1D','TN_lstm','Tp_CONV1D','TP_lstm']\n",
        "plt.figure(figsize=[10,3])\n",
        "\n",
        "plt.bar(0,metric_c1['Fp_CONV1D'][-1],tick_label=names[0],width=0.2)\n",
        "plt.bar(1,metric_lstm['Fp_lstm'][-1],tick_label=names[1],width=0.2)\n",
        "plt.bar(2,metric_c1['Fn_CONV1D'][-1],tick_label=names[2],width=0.2)\n",
        "plt.bar(3,metric_lstm['Fn_lstm'][-1],tick_label=names[3],width=0.2)\n",
        "#plt.bar(2.5,np.min(metric_c1['Tn_CONV1D']),tick_label=names[4],width=0.2)\n",
        "#plt.bar(3,np.min(metric_lstm['Tn_lstm']),tick_label=names[5],width=0.2)\n",
        "#plt.bar(3.5,np.min(metric_c1['Tp_CONV1D']),tick_label=names[6],width=0.2)\n",
        "#plt.bar(4,np.mean(metric_lstm['Tp_lstm']),tick_label=names[7],width=0.2)\n",
        "\n",
        "\n",
        "\n",
        "plt.xticks(range(0,4),names)\n",
        "plt.ylabel('false postive rate and false negtive rate ')\n",
        "plt.xlabel('Models')\n",
        "\n",
        "plt.savefig('model.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk1eBI8nXrAs"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWwcOfBIE8i2"
      },
      "source": [
        "#per_char=pd.read_csv('/content/drive/My Drive/Char_perm.csv')\n",
        "per_level=pd.read_csv('/content/drive/My Drive/Mal-prem.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKtHKpSzEPfj"
      },
      "source": [
        "\n",
        "\n",
        "import  tensorflow as tf \n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "encode_label=encoder.fit_transform(per_level['type'])\n",
        "labels_full=to_categorical(encode_label)\n",
        "labels=np.unique(per_level.type).tolist()\n",
        "labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jcwmjK6Ea14"
      },
      "source": [
        "#permiss = per_char['char_level'].values\n",
        "per_level['permmisions']=per_level['permmisions'].apply(lambda x:str(x))\n",
        "permiss=per_level['permmisions']\n",
        "Count_perm=per_level['permmisions'].apply(lambda x:len(x.strip().replace('android.permission.','').replace('android.','').replace(',','.').replace('_','.').split('.')))\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_length = max([len(s.replace('.','-').split('_')) for s in permiss])\n",
        "max_length\n",
        "#permiss_char=per_char['char_level'].apply(lambda x:str(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnp4V7MaEfm4"
      },
      "source": [
        "train,vocab_size_word=tokn(permiss)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( train, labels_full, test_size=0.33,random_state=42,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASp5W8tEEh0T"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 249\n",
        "\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "#maxlen_char  = max_length_char\n",
        "#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)\n",
        "#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)\n",
        "\n",
        "def model_lstm(embedding_dim ,vocab_size,maxlen):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(input_dim=vocab_size, \n",
        "                                output_dim=embedding_dim, \n",
        "                                input_length=maxlen))\n",
        "      model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n",
        "\n",
        "      model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "      model.add(SpatialDropout1D(0.7))\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "      model.add(Flatten())\n",
        "      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "      model.add(Dense(maxlen, activation='relu'))\n",
        "      model.add(Dropout(0.8))\n",
        "      model.add(Dense(5, activation='softmax'))\n",
        "      return model\n",
        "epochs = 50\n",
        "model=model_lstm(110,vocab_size_word,maxlen)\n",
        "batch_size = 256\n",
        "metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "metrics2=[\"accuracy\"]\n",
        "model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)\n",
        "model.summary()\n",
        "\n",
        "history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n",
        "#loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history_lstm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC02ANyqEr0c"
      },
      "source": [
        "plot_multiclass_roc(model, X_test, y_test, n_classes=4, figsize=(5, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWbXhVPOkXr8"
      },
      "source": [
        "#per_char=pd.read_csv('/content/drive/My Drive/Char_perm.csv')\n",
        "per_level=pd.read_csv('/content/drive/My Drive/Mal-prem.csv')\n",
        "np.unique(per_level.type)\n",
        "per_level['binary']='Malicious'\n",
        "per_level\n",
        "\n",
        "for i,m in enumerate (per_level['type']):\n",
        "    if per_level['type'][i]=='Benign':\n",
        "        per_level['binary'][i]='Benign'\n",
        "    else: per_level['binary'][i]='Malicious'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfZ35KALkZib"
      },
      "source": [
        "per_level[per_level['binary']=='Benign']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF-Sdf7HEx4H"
      },
      "source": [
        "encoded_docs,vocab_size_word=tokn(permiss)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 249\n",
        "import  tensorflow as tf \n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "encode_label=encoder.fit_transform(per_level['binary'])\n",
        "labels_full=to_categorical(encode_label)\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "N0x3aYVFKTw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hykYnpQEKls"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def conv_1d():\n",
        "    model_c1 = Sequential()\n",
        "    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n",
        "    \n",
        "    model_c1.add(Dropout(0.4))\n",
        "   \n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Flatten())\n",
        "    \n",
        "    model_c1.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model_c1.add(Dense(2,activation='softmax'))\n",
        "\n",
        "    print(model_c1.summary())\n",
        "    return model_c1\n",
        "\n",
        "metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "              tf. keras.metrics.FalsePositives(name='fp'),\n",
        "              tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "              tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "              tf.keras.metrics.Precision(name='precision'),\n",
        "              tf.keras.metrics.Recall(name='recall'),\n",
        "              tf.keras.metrics.AUC(name='auc')]\n",
        "model_c1=conv_1d()\n",
        "metrics2=[\"accuracy\"]\n",
        "model_c1.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=\"accuracy\")\n",
        "# fit network\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
        "\n",
        "history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEwkL9ZrFLYx"
      },
      "source": [
        "plot_multiclass_roc(model_c1, X_test, y_test, n_classes=2, figsize=(5, 5))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMO-hhJwlnxi"
      },
      "source": [
        "\n",
        "per_level['binary']='Malicious'\n",
        "per_level\n",
        "\n",
        "for i,m in enumerate (per_level['type']):\n",
        "    if per_level['type'][i]=='Benign':\n",
        "        per_level['binary'][i]='B'\n",
        "    else: per_level['binary'][i]='M'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNRaKNNDltRH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikgb3NSKlVRj"
      },
      "source": [
        "lis_value=()\n",
        "dic={}\n",
        "for i,perm in enumerate(perm_split):\n",
        "      lis_value=[ ]\n",
        "      for p in list(lis_u) :\n",
        "           if p in perm:\n",
        "              lis_value.append(1) \n",
        "           else:\n",
        "               lis_value.append(0) \n",
        "      dic.update({i:lis_value})\n",
        "data=pd.DataFrame(dic.values(),columns=list(lis_u))\n",
        "Full_data=pd.concat([data,per_level['binary']],axis=1)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q_CzZ_V0E9d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEYXwpKpIi-3"
      },
      "source": [
        "\n",
        "lis_value=()\n",
        "dic={}\n",
        "for i,perm in enumerate(perm_split):\n",
        "      lis_value=[ ]\n",
        "      for p in list(lis_u) :\n",
        "           if p in perm:\n",
        "              lis_value.append(1) \n",
        "           else:\n",
        "               lis_value.append(0) \n",
        "      dic.update({i:lis_value})\n",
        "data=pd.DataFrame(dic.values(),columns=list(lis_u))\n",
        "Full_data=pd.concat([data,per_level['type']],axis=1)\n",
        "   \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( train_data,labels_full, test_size=0.33,shuffle=True)\n",
        "X_train=np.expand_dims(X_train,-1)\n",
        "X_test=np.expand_dims(X_test,-1)\n",
        "X_train.shape,X_test.shape\n",
        "def Lst_model(X_t,opt):\n",
        "    model=Sequential()  \n",
        "\n",
        "    model.add((LSTM(128,dropout=0.5,input_shape=X_t.shape[1:],return_sequences=True)))\n",
        "\n",
        "    model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "    model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "    model.add(SpatialDropout1D(0.7))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "    model.add(Dense(maxlen, activation='relu'))\n",
        "    model.add(Dropout(0.8))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "    adam = tf.keras.optimizers.Adam(0.0001)\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "                            patience=5, verbose=1, mode='auto')\n",
        "    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics= metrics1)\n",
        "    return model\n",
        "for m in ['Adam','RMSprop','SGD','Nadam']:\n",
        "    for batch in [32,64,1,28,256,512,1024]:\n",
        "        model =model_lstm(110,vocab_size_word,maxlen,m)\n",
        "\n",
        "    #print(model.layers[0].get_weights()[0])\n",
        "        import time\n",
        "        start = time.process_time()\n",
        "        print(start)\n",
        "        history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BjgHtOyLVGe"
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import Concatenate\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}