{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1am33bT2LEqwnFGtMRhqJs1gm136df9qD",
      "authorship_tag": "ABX9TyM6s7faVSMDmcqPDkjmVmXD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moathzyout/Project/blob/main/DL-approaches-text%20and%20binary%20encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtD0dyDshnX0"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two main tasks to classify Mal-prem dataset are developed in this work to classify malware-based binary encoding and text encoding for permissions using LSTM and Convd1d. In addition, different algorithms are presented on how select the best batch size in all tasks by the Keraseclassier python package.**"
      ],
      "metadata": {
        "id": "4g0-2N8N97HM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TCaee-GiRf9"
      },
      "source": [
        "\n",
        "from sklearn import metrics\n",
        "import re\n",
        "import time\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation,Bidirectional,MaxPool3D,MaxPool2D,SpatialDropout1D,UpSampling2D,Embedding,Reshape,MaxPooling2D,Convolution2D,Convolution1D,Input,LSTM,Dropout,Flatten,BatchNormalization,MaxPooling1D,GlobalMaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop\n",
        "import pandas as pd \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation,BatchNormalization,Masking,UpSampling2D,Reshape,MaxPooling2D,Convolution2D,Convolution1D,Input,LSTM,Dropout,Flatten,BatchNormalization,MaxPooling1D,GlobalMaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-RWSFfAb6Cn"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znQ0q_xCpl17"
      },
      "source": [
        "#from _typeshed import OpenTextMode\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.backend import batch_get_value\n",
        "def plot_history_opt(history,opt,batch):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy for '+ opt+' with '+batch)\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss for '+ opt+' with '+batch)\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "l4XNgQgijuDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCfV4RKmRbq1"
      },
      "source": [
        "import sklearn\n",
        "import seaborn as sns \n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "def plot_history_loss(history):\n",
        "   \n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(loss) + 1)\n",
        "  \n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "def plot_multiclass_roc(clf,x_test, y_test, n_classes, figsize=(17, 10)):\n",
        "\n",
        "    # structures\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    \n",
        "    roc_auc = dict()\n",
        "    y_score1 = clf.predict(x_test)\n",
        "    # calculate dummies once\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = sklearn.metrics.roc_curve(y_test[:, i].ravel(), y_score1[:, i])\n",
        "        roc_auc[i] = sklearn.metrics.auc(fpr[i], tpr[i])\n",
        "\n",
        "    # roc for each class\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.plot([0, 1], [0, 1], 'k--')\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title('Receiver operating characteristic example')\n",
        "    for i in range(n_classes):\n",
        "        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for label %s' % (roc_auc[i], labels[i]))\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.grid(alpha=.4)\n",
        "    sns.despine()\n",
        "    plt.show()\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 10)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "def plot_metrics(history):\n",
        "    import matplotlib.pyplot as plt\n",
        "    metrics = ['loss', 'auc', 'precision', 'recall']\n",
        "    for n, metric in enumerate(metrics):\n",
        "        name = metric.replace(\"_\",\" \").capitalize()\n",
        "        plt.subplot(2,2,n+1)\n",
        "        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
        "        plt.plot(history.epoch, history.history['val_'+metric],\n",
        "                 color=colors[0], linestyle=\"--\", label='Val')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel(name)\n",
        "        if metric == 'loss':\n",
        "            plt.ylim([0, plt.ylim()[1]])\n",
        "        elif metric == 'auc':\n",
        "             plt.ylim([0.8,1])\n",
        "        else:\n",
        "            plt.ylim([0,1])\n",
        "\n",
        "        plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8PYgoGGoOIk"
      },
      "source": [
        "#per_char=pd.read_csv('/content/drive/My Drive/Char_perm.csv')\n",
        "per_level=pd.read_csv('/content/drive/My Drive/permissions.csv')\n",
        "import  tensorflow as tf \n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "encode_label=encoder.fit_transform(per_level['type'])\n",
        "labels_full=to_categorical(encode_label)\n",
        "labels=np.unique(per_level.type).tolist()\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfQ3Rk82kWll"
      },
      "source": [
        "\n",
        "per_level['permmisions']=per_level['permmisions'].apply(lambda x:str(x))\n",
        "permiss=per_level['permmisions']\n",
        "Count_perm=per_level['permmisions'].apply(lambda x:len(x.strip().replace('android.permission.','').replace('android.','').replace(',','.').replace('_','.').split('.')))\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_length = max([len(s.replace('.','-').split('_')) for s in permiss])\n",
        "max_length\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMmBGEn8PP_0"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "def tokn(text):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(text)\n",
        "\n",
        "    train = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "    vocab_size\n",
        "    return train,vocab_size\n",
        "train,vocab_size_word=tokn(permiss)\n",
        "print (vocab_size_word)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( train, labels_full, test_size=0.33,random_state=42,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoded_docs,vocab_size_word=tokn(permiss)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 249\n",
        "\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "def conv_1d():\n",
        "    model_c1 = Sequential()\n",
        "    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n",
        "    \n",
        "    model_c1.add(Dropout(0.4))\n",
        "   \n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Flatten())\n",
        "    \n",
        "    model_c1.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model_c1.add(Dense(5, activation='softmax'))\n",
        "    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "              tf. keras.metrics.FalsePositives(name='fp'),\n",
        "              tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "              tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "              tf.keras.metrics.Precision(name='precision'),\n",
        "              tf.keras.metrics.Recall(name='recall'),\n",
        "              tf.keras.metrics.AUC(name='auc')]\n",
        "\n",
        "    model_c1.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=metrics1)\n",
        "    return model_c1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_c1=conv_1d()\n",
        "metrics2=[\"accuracy\"]\n",
        "start = time.process_time()\n",
        "# fit network\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
        "history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])"
      ],
      "metadata": {
        "id": "hpaaeoQVqhT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretty_confusion_matrix"
      ],
      "metadata": {
        "id": "JZbR0mAi6a21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "Y_prediction = model_c1.predict(X_test, verbose=1)\n",
        "# Convert predictions classes to one hot vectors \n",
        "\n",
        "\n",
        "Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_test,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
        "\n",
        "df_cm = pd.DataFrame(confusion_mtx, index=np.unique(per_level.type.values), columns=np.unique(per_level.type.values))\n",
        "from pretty_confusion_matrix import pp_matrix\n",
        "cmap = 'PuRd'\n",
        "cm=pp_matrix(df_cm,cmap=plt.cm.CMRmap_r)\n"
      ],
      "metadata": {
        "id": "LKYINUsm62I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.argmax(Y_pred_classes,axis=1)\n",
        "\n",
        "print(f1_score(Y_true,y,average='micro'))"
      ],
      "metadata": {
        "id": "5uln9l-m63OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn9_SxfTYm_2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6JLFlNgbF28"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "maxlen = 249\n",
        "\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "#maxlen_char  = max_length_char\n",
        "#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)\n",
        "#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)\n",
        "\n",
        "def model_lstm(embedding_dim ,vocab_size,maxlen):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(input_dim=vocab_size, \n",
        "                                output_dim=embedding_dim, \n",
        "                                input_length=maxlen))\n",
        "      model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n",
        "\n",
        "      model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "      model.add(SpatialDropout1D(0.7))\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "      model.add(Flatten())\n",
        "      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "      model.add(Dense(maxlen, activation='relu'))\n",
        "      model.add(Dropout(0.8))\n",
        "      model.add(Dense(5, activation='softmax'))\n",
        "      return model\n",
        "epochs = 50\n",
        "model=model_lstm(110,vocab_size_word,maxlen)\n",
        "batch_size = 256\n",
        "metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "metrics2=[\"accuracy\"]\n",
        "model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)\n",
        "model.summary()\n",
        "#print(model.layers[0].get_weights()[0])\n",
        "import time\n",
        "start = time.process_time()\n",
        "print(start)\n",
        "history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n",
        "\n",
        "#loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history_lstm)\n",
        "end = time.process_time()\n",
        "print(\"Time for LSTM model : {} \".format((end-start)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "Y_prediction = model.predict(X_test, verbose=1)\n",
        "# Convert predictions classes to one hot vectors \n",
        "\n",
        "\n",
        "Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_test,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
        "\n",
        "df_cm = pd.DataFrame(confusion_mtx, index=np.unique(per_level.type.values), columns=np.unique(per_level.type.values))\n",
        "from pretty_confusion_matrix import pp_matrix\n",
        "cmap = 'PuRd'\n",
        "cm=pp_matrix(df_cm,cmap=plt.cm.CMRmap_r)\n"
      ],
      "metadata": {
        "id": "vPRnUtN_ug2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y=np.argmax(Y_pred_classes,axis=1)\n",
        "\n",
        "print(f1_score(Y_true,y,average='micro'))"
      ],
      "metadata": {
        "id": "yGspToX2RAcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end = time.process_time()\n",
        "print(\"Time for LSTM model : {} \".format((end-start)))"
      ],
      "metadata": {
        "id": "BHdI_LvvEH6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 249\n",
        "epochs = 50\n",
        "batch_size = 256\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "#maxlen_char  = max_length_char\n",
        "#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)\n",
        "#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)\n",
        "\n",
        "def model_lstm(embedding_dim,vocab_size,maxlen,opt):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(input_dim=vocab_size, \n",
        "                                output_dim=embedding_dim, \n",
        "                                input_length=maxlen))\n",
        "      model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n",
        "\n",
        "      model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "      model.add(SpatialDropout1D(0.7))\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "      model.add(Flatten())\n",
        "      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "      model.add(Dense(maxlen, activation='relu'))\n",
        "      model.add(Dropout(0.8))\n",
        "      model.add(Dense(5, activation='softmax'))\n",
        "     \n",
        "      metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "      metrics2=[\"accuracy\"]\n",
        "      model.compile(loss='categorical_crossentropy',optimizer=opt, metrics= metrics1)\n",
        "      return model\n",
        "for m in ['Adam','RMSprop','SGD','Nadam']:\n",
        "    for batch in [32,64,1,28,256,512,1024]:\n",
        "        model =model_lstm(110,vocab_size_word,maxlen,m)\n",
        "\n",
        "    #print(model.layers[0].get_weights()[0])\n",
        "        import time\n",
        "        start = time.process_time()\n",
        "        print(start)\n",
        "        history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n",
        "\n",
        "        #loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "        #print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "        plot_history_opt(history_lstm,m,str(batch))\n",
        "        end = time.process_time()\n",
        "        print(\"Time for LSTM model : {} \".format((end-start)))\n"
      ],
      "metadata": {
        "id": "128IoozIcCC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoded_docs,vocab_size_word=tokn(permiss)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 249\n",
        "\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "def conv_1d(opt):\n",
        "    model_c1 = Sequential()\n",
        "    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n",
        "    \n",
        "    model_c1.add(Dropout(0.4))\n",
        "   \n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Flatten())\n",
        "    \n",
        "    model_c1.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model_c1.add(Dense(5, activation='softmax'))\n",
        "    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "              tf. keras.metrics.FalsePositives(name='fp'),\n",
        "              tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "              tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "              tf.keras.metrics.Precision(name='precision'),\n",
        "              tf.keras.metrics.Recall(name='recall'),\n",
        "              tf.keras.metrics.AUC(name='auc')]\n",
        "\n",
        "    model_c1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=metrics1)\n",
        "    return model_c1\n",
        "\n",
        "\n",
        "for opt in ['Adam','RMSprop','SGD','Nadam']:\n",
        "   for batch in [32,64,128,256,512,1024]:\n",
        "\n",
        "        model_c1=conv_1d(opt)\n",
        "        metrics2=[\"accuracy\"]\n",
        "        start = time.process_time()\n",
        "        # fit network\n",
        "        from keras.callbacks import ReduceLROnPlateau\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "        #model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
        "        history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=batch,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])\n",
        "        plot_history_opt(history_c1,opt,str(batch))"
      ],
      "metadata": {
        "id": "sD6Rjk2yfuVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per_level['permmisions']=per_level['permmisions'].apply(lambda x:str(x).lower())\n",
        "permiss=per_level['permmisions']\n",
        "perm_split=per_level['permmisions'].apply(lambda x:x.replace(',','').replace('android.permission.',' ').strip().split('.')[-1])\n",
        "lis_value=set()\n",
        "lis_u=set()\n",
        "lis_length=[ ]\n",
        "for perm in perm_split:\n",
        "    for p in perm.split(' ') :\n",
        "        if p!='' or p!='nan' :\n",
        "             lis_u.add(p)\n",
        "\n",
        "dic={}\n",
        "for i,perm in enumerate(perm_split):\n",
        "      lis_value=[ ]\n",
        "      for p in list(lis_u) :\n",
        "           if p in perm:\n",
        "              lis_value.append(1) \n",
        "           else:\n",
        "               lis_value.append(0) \n",
        "      dic.update({i:lis_value})\n",
        "data=pd.DataFrame(dic.values(),columns=list(lis_u))\n"
      ],
      "metadata": {
        "id": "nVx0Nq4LOA9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( data.values(),labels_full, test_size=0.33,shuffle=True)\n",
        "X_train=np.expand_dims(X_train,-1)\n",
        "X_test=np.expand_dims(X_test,-1)\n",
        "X_train.shape,X_test.shape"
      ],
      "metadata": {
        "id": "IazwC5lEPWdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "   \n",
        "\n",
        "def Lst_model_binary(X_t):\n",
        "    model=Sequential()  \n",
        "    model.add((LSTM(128,dropout=0.5,input_shape=X_t.shape[1:],return_sequences=True)))\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "    model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n",
        "\n",
        "    model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "    model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "    model.add(SpatialDropout1D(0.7))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dropout(0.8))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "   \n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "                            patience=5, verbose=1, mode='auto')\n",
        "    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)\n",
        "    return model\n",
        "\n",
        "for batch in [32,64,128,256,512,1024]:\n",
        "    model =Lst_model_binary(X_train)\n",
        "    print('optmizer'+ m+str(batch))\n",
        "#print(model.layers[0].get_weights()[0])\n",
        "    import time\n",
        "    start = time.process_time()\n",
        "    print(start)\n",
        "    history_lstm = model.fit(X_train, y_train, epochs=50, batch_size=batch,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n",
        "    end = time.process_time()\n",
        "    print(\"Time for LSTM model : {} \".format((end-start)))\n"
      ],
      "metadata": {
        "id": "fp6wWG6WKlbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lis_value=()\n",
        "dic={}\n",
        "for i,perm in enumerate(perm_split):\n",
        "      lis_value=[ ]\n",
        "      for p in list(lis_u) :\n",
        "           if p in perm:\n",
        "              lis_value.append(1) \n",
        "           else:\n",
        "               lis_value.append(0) \n",
        "      dic.update({i:lis_value})\n",
        "data=pd.DataFrame(dic.values(),columns=list(lis_u))\n",
        "Full_data=pd.concat([data,per_level['type']],axis=1)"
      ],
      "metadata": {
        "id": "y_KKu8R5LByu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import  tensorflow as tf \n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "encode_label=encoder.fit_transform(per_level['type'])\n",
        "labels_full=to_categorical(encode_label)\n",
        "labels_full.shape"
      ],
      "metadata": {
        "id": "jkv92ds1izD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras[tensorflow]\n",
        "import scikeras\n",
        "from scikeras.wrappers import KerasClassifier"
      ],
      "metadata": {
        "id": "Rl2DZ-FFJr71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "import numpy\n",
        "import tensorflow  \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import scikeras\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "# Function to create model, required for KerasClassifier\n",
        "def model_lstm( optimizer=\"adam\" ):\n",
        "      embedding_dim=110\n",
        "      vocab_size=vocab_size_word\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(input_dim=vocab_size, \n",
        "                                output_dim=embedding_dim, \n",
        "                                input_length=maxlen))\n",
        "      model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n",
        "\n",
        "      model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "      model.add(SpatialDropout1D(0.7))\n",
        "      \n",
        "      \n",
        "      model.add(Flatten())\n",
        "      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "      model.add(Dense(maxlen, activation='relu'))\n",
        "      model.add(Dropout(0.8))\n",
        "      model.add(Dense(5, activation='softmax'))\n",
        "      metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "      metrics2=[\"accuracy\"]\n",
        "      model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics= metrics2)\n",
        "      return model\n",
        "# fix random seed for reproducibility\n",
        "\n",
        "start=time.process_time()\n",
        "# split into input (X) and output (Y) variables\n",
        "X = Full_train\n",
        "Y =labels_full\n",
        "# create model\n",
        "model_grid = KerasClassifier(build_fn=model_lstm, verbose=0)\n",
        "# define the grid search parameters\n",
        "optimizers = ['SGD','Adam']\n",
        "batches=[64,128,256,512,1024]\n",
        "epoches=[1]\n",
        "param_grid = dict(optimizer=optimizers,epochs=epoches,batch_size=batches)\n",
        "grid = GridSearchCV(estimator=model_grid, param_grid=param_grid,cv=5)\n",
        "grid_result = grid.fit(X, Y)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "end = time.process_time()\n",
        "print(\"Time for LSTM model : {} \".format((end-start)))"
      ],
      "metadata": {
        "id": "YFuoN1T_RcWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP5cww57kFBJ"
      },
      "source": [
        "encoded_docs,vocab_size_word=tokn(permiss)\n",
        "perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 249\n",
        "\n",
        "X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlLABv5JGGt1"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def conv_1d():\n",
        "    model_c1 = Sequential()\n",
        "    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n",
        "    \n",
        "    model_c1.add(Dropout(0.4))\n",
        "   \n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Flatten())\n",
        "    \n",
        "    model_c1.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model_c1.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    print(model_c1.summary())\n",
        "    return model_c1\n",
        "\n",
        "metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "              tf. keras.metrics.FalsePositives(name='fp'),\n",
        "              tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "              tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "              tf.keras.metrics.Precision(name='precision'),\n",
        "              tf.keras.metrics.Recall(name='recall'),\n",
        "              tf.keras.metrics.AUC(name='auc')]\n",
        "model_c1=conv_1d()\n",
        "metrics2=[\"accuracy\"]\n",
        "start = time.process_time()\n",
        "model_c1.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=\"accuracy\")\n",
        "# fit network\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
        "\n",
        "history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTF_N9GfG8K1"
      },
      "source": [
        "plot_history(history_c1)\n",
        "end = time.process_time()\n",
        "print(\"Time for Conv1d model : {} \".format((end-start)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH49Yd2fCjpQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "Y_prediction = model_c1.predict(X_test, verbose=1)\n",
        "# Convert predictions classes to one hot vectors \n",
        "\n",
        "\n",
        "Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_test,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
        "df_cm = pd.DataFrame(confusion_mtx, index=np.unique(per_level.type.values), columns=np.unique(per_level.type.values))\n",
        "from pretty_confusion_matrix import pp_matrix\n",
        "cmap = 'PuRd'\n",
        "pp_matrix(df_cm,cmap=plt.cm.CMRmap_r)"
      ],
      "metadata": {
        "id": "9cUZTk_2CJsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.argmax(Y_prediction,axis=1)\n",
        "Y_true\n",
        "print(f1_score(Y_true,y,average='micro'))"
      ],
      "metadata": {
        "id": "p-KPvTWOQyAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMKsBVoXZC2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "import numpy\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load dataset\n",
        "\n",
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import scikeras\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "train,vocab_size_word=tokn(permiss)\n",
        "Full_train = pad_sequences(train, padding='post', maxlen=maxlen)\n",
        "# Function to create model, required for KerasClassifier\n",
        "def conv_1d(optimizer='adam'):\n",
        "    model_c1 = Sequential()\n",
        "    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n",
        "    \n",
        "    model_c1.add(Dropout(0.4))\n",
        "   \n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Flatten())\n",
        "    \n",
        "    model_c1.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model_c1.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    print(model_c1.summary())\n",
        "    model_c1.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=\"accuracy\")\n",
        "    return model_c1\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = Full_train\n",
        "Y =labels_full\n",
        "# create model\n",
        "create_model=conv_1d()\n",
        "model = KerasClassifier(build_fn=create_model,verbose=0)\n",
        "# define the grid search parameters\n",
        "epochs= [50]\n",
        "batches=[64,128,256,512,1024]\n",
        "#[ 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "param_grid = dict(epochs=epochs,batch_size=batches)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
        "grid_result = grid.fit(X, Y)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "hW_JCIr3Q5rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary vectorizations were used to convert the context of permissions into binary 0,1 for all categories in Mal-prem dataset **"
      ],
      "metadata": {
        "id": "QXhyTj9lBB68"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMO-hhJwlnxi"
      },
      "source": [
        "\n",
        "per_level['binary']='Malicious'\n",
        "per_level\n",
        "\n",
        "for i,m in enumerate (per_level['type']):\n",
        "    if per_level['type'][i]=='Benign':\n",
        "        per_level['binary'][i]='B'\n",
        "    else: per_level['binary'][i]='M'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikgb3NSKlVRj"
      },
      "source": [
        "lis_value=()\n",
        "dic={}\n",
        "for i,perm in enumerate(perm_split):\n",
        "      lis_value=[ ]\n",
        "      for p in list(lis_u) :\n",
        "           if p in perm:\n",
        "              lis_value.append(1) \n",
        "           else:\n",
        "               lis_value.append(0) \n",
        "      dic.update({i:lis_value})\n",
        "data=pd.DataFrame(dic.values(),columns=list(lis_u))\n",
        "Full_data=pd.concat([data,per_level['binary']],axis=1)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q_CzZ_V0E9d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEYXwpKpIi-3"
      },
      "source": [
        "\n",
        "lis_value=()\n",
        "dic={}\n",
        "for i,perm in enumerate(perm_split):\n",
        "      lis_value=[ ]\n",
        "      for p in list(lis_u) :\n",
        "           if p in perm:\n",
        "              lis_value.append(1) \n",
        "           else:\n",
        "               lis_value.append(0) \n",
        "      dic.update({i:lis_value})\n",
        "data=pd.DataFrame(dic.values(),columns=list(lis_u))\n",
        "Full_data=pd.concat([data,per_level['type']],axis=1)\n",
        "   \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data,labels_full, test_size=0.33,shuffle=True)\n",
        "X_train=np.expand_dims(X_train,-1)\n",
        "X_test=np.expand_dims(X_test,-1)\n",
        "X_train.shape,X_test.shape\n",
        "def Lst_model(X_t,opt):\n",
        "    model=Sequential()  \n",
        "\n",
        "    model.add((LSTM(128,dropout=0.5,input_shape=X_t.shape[1:],return_sequences=True)))\n",
        "\n",
        "    model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "    model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "    model.add(SpatialDropout1D(0.7))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "    model.add(Dense(maxlen, activation='relu'))\n",
        "    model.add(Dropout(0.8))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "    adam = tf.keras.optimizers.Adam(0.0001)\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "                            patience=5, verbose=1, mode='auto')\n",
        "    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics= metrics1)\n",
        "    return model\n",
        "for m in ['Adam','RMSprop','SGD','Nadam']:\n",
        "    for batch in [32,64,1,28,256,512,1024]:\n",
        "        model =model_lstm(110,vocab_size_word,maxlen,m)\n",
        "\n",
        "    #print(model.layers[0].get_weights()[0])\n",
        "        import time\n",
        "        start = time.process_time()\n",
        "        print(start)\n",
        "        history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suoiuumyJS-4"
      },
      "source": [
        "lis_value=()\n",
        "dic={}\n",
        "for i,perm in enumerate(perm_split):\n",
        "      lis_value=[ ]\n",
        "      for p in list(lis_u) :\n",
        "           if p in perm:\n",
        "              lis_value.append(1) \n",
        "           else:\n",
        "               lis_value.append(0) \n",
        "      dic.update({i:lis_value})\n",
        "data=pd.DataFrame(dic.values(),columns=list(lis_u))\n",
        "Full_data=pd.concat([data,per_level['binary']],axis=1)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDN0hEcEFquQ"
      },
      "source": [
        "'''\n",
        "per_level['binary']='Malicious'\n",
        "per_level\n",
        "\n",
        "for i,m in enumerate (per_level['type']):\n",
        "    if per_level['type'][i]=='Benign':\n",
        "        per_level['binary'][i]='B'\n",
        "    else: per_level['binary'][i]='M'''\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLEzlC68oVSe"
      },
      "source": [
        "#per_char=pd.read_csv('/content/drive/My Drive/Char_perm.csv')\n",
        "per_level=pd.read_csv('/content/drive/My Drive/permissions.csv')\n",
        "per_level['permmisions']=per_level['permmisions'].apply(lambda x:str(x).lower())\n",
        "permiss=per_level['permmisions']\n",
        "perm_split=per_level['permmisions'].apply(lambda x:x.replace(',','').replace('android.permission.',' ').strip().split('.')[-1])\n",
        "Full_permissions=[ ]\n",
        "lis_u=set()\n",
        "lis_length=[ ]\n",
        "for perm in perm_split:\n",
        "    for p in perm.split(' ') :\n",
        "        if p!='' or p!='nan' :\n",
        "             lis_u.add(p)\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDyQWoHqoa64"
      },
      "source": [
        "\n",
        "lis_value=()\n",
        "dic={}\n",
        "for i,perm in enumerate(perm_split):\n",
        "      lis_value=[ ]\n",
        "      for p in list(lis_u) :\n",
        "           if p in perm:\n",
        "              lis_value.append(1) \n",
        "           else:\n",
        "               lis_value.append(0) \n",
        "      dic.update({i:lis_value})\n",
        "  \n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEdJgASWvXMq"
      },
      "source": [
        "data=pd.DataFrame(dic.values(),columns=list(lis_u))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt7uxrOVpHZS"
      },
      "source": [
        "'''\n",
        "per_level['binary']='Malicious'\n",
        "per_level\n",
        "\n",
        "for i,m in enumerate (per_level['type']):\n",
        "    if per_level['type'][i]=='Benign':\n",
        "        per_level['binary'][i]='B'\n",
        "    else: per_level['binary'][i]='M'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz9lkirEDZ0a"
      },
      "source": [
        "#Full_data=pd.concat([data,per_level['binary']],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMqyQVMwDzlo"
      },
      "source": [
        "#Full_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppm7OI2RFCb2"
      },
      "source": [
        "\n",
        "\n",
        "#data=data.drop(['type'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL_2mYqLu-_L"
      },
      "source": [
        "train_data=data.values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHusp54zDyMR"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGlPaVaEGLnF"
      },
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( train_data,labels_full, test_size=0.33,shuffle=True)\n",
        "X_train=np.expand_dims(X_train,-1)\n",
        "X_test=np.expand_dims(X_test,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNk8ZI6rF_l-"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( train_data,labels_full, test_size=0.33,shuffle=True)\n",
        "X_train=np.expand_dims(X_train,-1)\n",
        "X_test=np.expand_dims(X_test,-1)\n",
        "X_train.shape,X_test.shape\n",
        "def Lst_model(X_t):\n",
        "    model=Sequential()  \n",
        "\n",
        "    model.add((LSTM(128,dropout=0.5,input_shape=X_t.shape[1:],return_sequences=True)))\n",
        "\n",
        "    model.add(SpatialDropout1D(0.6))\n",
        "\n",
        "\n",
        "    model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n",
        "    model.add(SpatialDropout1D(0.7))\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    model.add(Flatten())\n",
        "    #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n",
        "    model.add(Dense(maxlen, activation='relu'))\n",
        "    model.add(Dropout(0.8))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "    adam = tf.keras.optimizers.Adam(0.0001)\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "                            patience=5, verbose=1, mode='auto')\n",
        "    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "            tf. keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')]\n",
        "    model.compile( optimizer=adam,loss='categorical_crossentropy',metrics=metrics1)                        \n",
        "    model.summary()\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzF6p9ABGF62"
      },
      "source": [
        "model=Lst_model(X_train)\n",
        "history_lstm=model.fit(X_train, y_train, epochs =40, batch_size =256, validation_data = (X_test,y_test),shuffle=True)\n",
        "plot_history(history_lstm)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "Y_prediction = model.predict(X_test,verbose=1,batch_size=256)\n",
        "y=np.argmax(Y_pred_classes,axis=1)\n",
        "\n",
        "print(f1_score(Y_true,y,average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeV7UeEKGQzW"
      },
      "source": [
        "\n",
        "\n",
        "def conv_1d(X_t):\n",
        "    model_c1 = Sequential()\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,input_shape=X_t.shape[1:],padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n",
        "    model_c1.add(MaxPooling1D(pool_size=3))\n",
        "    model_c1.add(Dropout(0.5))\n",
        "    model_c1.add(Flatten())\n",
        "    \n",
        "    model_c1.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model_c1.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    print(model_c1.summary())\n",
        "    return model_c1\n",
        "\n",
        "metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n",
        "              tf. keras.metrics.FalsePositives(name='fp'),\n",
        "              tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "              tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "              tf.keras.metrics.Precision(name='precision'),\n",
        "              tf.keras.metrics.Recall(name='recall'),\n",
        "              tf.keras.metrics.AUC(name='auc')]\n",
        "model_c1=conv_1d(X_train)\n",
        "metrics2=[\"accuracy\"]\n",
        "model_c1.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=metrics1)\n",
        "# fit network\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
        "\n",
        "history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RPD2_31UGkH"
      },
      "source": [
        "plot_history(history_c1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "Y_prediction = model_c1.predict(X_test,verbose=1,batch_size=256)\n",
        "# Convert predictions classes to one hot vectors \n",
        "\n",
        "\n",
        "Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_test,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
        "y=np.argmax(Y_pred_classes,axis=1)\n",
        "\n",
        "print(f1_score(Y_true,y,average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH6rohkVPNHs"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNW9N69cVCA-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3lA_XiVVEji"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}