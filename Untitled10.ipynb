{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1uBf4N8-FrYUO66bjGr32Er2X9leRtb8p","authorship_tag":"ABX9TyOshr+ixG/OfSgKsUbivSmo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","metadata":{"id":"_A2fOnKV92Mm"},"source":["'''from androguard.core.bytecodes.apk import APK\n","import re\n","from androguard import misc\n","import os, sys\n","from androguard.core.analysis import  analysis\n","import csv\n","import re\n","import os, sys\n","import subprocess\n","from androguard.misc import AnalyzeAPK\n","path = 'U:\\\\Data_T\\\\Benign'\n","dirs = os.listdir(path)\n","#This would print all the files and directories\n","\n","#This would print all the files and directories\n","dic_perm={}\n","dic_feat={}\n","dic_act={}\n","dic_ser={}\n","dic_method={}\n","dic_class={}\n","\n","i=0\n","for subdir in dirs:\n","    for i,file in enumerate(os.listdir(os.path.join(path,subdir))):\n","        print(i)\n","        new=os.path.join(path,subdir)\n","        new=os.path.join(new,file)\n","\n","        try:\n","\n","                print(new)\n","                a, d, dx = AnalyzeAPK(new)\n","\n","               # d, dx = analysis(\"./apks/classes.dex\")\n","               # dic.update({'name':file,'permissions':a.get_permissions(),'activities':a.get_activities(),'Services':a.get_services(),'receivers':a.get_receivers(),'features':a.get_features()})\n","                print(i,file)\n","                string=' '\n","                for  m in a.get_permissions():\n","                     string= string+m+'  '\n","                dic_perm.update({file:string})\n","                feat=' '\n","                for  m in a.get_features():\n","                    feat=feat+m+'  '\n","                dic_feat.update({file:feat})\n","\n","                serv=' '\n","                for  m in a.get_services():\n","                    serv=serv+m+' '\n","                dic_ser.update({file:serv})\n","                act=' '\n","                for  m in a.get_activities():\n","                    act=act+m+' '\n","                dic_act.update({file:act})\n","                clas=' '\n","                for  m in  dx.get_classes():\n","                    clas = clas + str(m.name) +'  '\n","                dic_class.update({file:clas})\n","                method='  '\n","                for c in  dx.get_classes():\n","                    for mc in c.get_methods():\n","                        method = method + str(mc.name) +'  '\n","                dic_method.update({file:method})\n","        except FileNotFoundError:\n","                print('File not found!!')\n","                pass\n","                continue\n","        except :\n","                print('File not found!!')\n","                pass\n","                continue \n","\n","\n","        print('complete......'+str(subdir))\n","\n","import pandas as pd\n","df_class = pd.DataFrame(list(dic_class.items()),columns = ['APK_Name','Classes'])\n","df_method = pd.DataFrame(list(dic_method.items()),columns = ['APK_Name','methods'])\n","df_feat = pd.DataFrame(list(dic_feat.items()),columns = ['APK_Name','featuers'])\n","df_perm = pd.DataFrame(list(dic_perm.items()),columns = ['APK_Name','permissions'])\n","df_act = pd.DataFrame(list(dic_act.items()),columns = ['APK_Name','activities'])\n","df_ser= pd.DataFrame(list(dic_ser.items()),columns = ['APK_Name','Services'])\n","df_class.to_csv('U:\\\\Data_T\\\\Benign\\\\Class_SM2020_m'+str(subdir)+'.csv')\n","df_perm.to_csv('U:\\\\Data_T\\\\Benign\\\\perm_SM2020_m'+str(subdir)+'.csv')\n","df_method.to_csv('U:\\\\Data_T\\\\Benign\\\\Method_SM2020_m'+str(subdir)+'.csv')\n","#df_feat.to_csv('features_R2020_m'+str(subdir)+'.csv')\n","#df_act.to_csv('activites_R2020_m'+str(subdir)+'.csv')\n","#df_ser.to_csv('Services_R2020_m'+str(subdir)+'.csv')\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtD0dyDshnX0"},"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3TCaee-GiRf9"},"source":["\n","from sklearn import metrics\n","import re\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation,Bidirectional,MaxPool3D,MaxPool2D,SpatialDropout1D,UpSampling2D,Embedding,Reshape,MaxPooling2D,Convolution2D,Convolution1D,Input,LSTM,Dropout,Flatten,BatchNormalization,MaxPooling1D,GlobalMaxPooling2D\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam,RMSprop\n","import pandas as pd \n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation,BatchNormalization,Masking,UpSampling2D,Reshape,MaxPooling2D,Convolution2D,Convolution1D,Input,LSTM,Dropout,Flatten,BatchNormalization,MaxPooling1D,GlobalMaxPooling2D\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras import backend\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h-RWSFfAb6Cn"},"source":[]},{"cell_type":"code","metadata":{"id":"znQ0q_xCpl17"},"source":["#from _typeshed import OpenTextMode\n","\n","def plot_history(history):\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    x = range(1, len(acc) + 1)\n","\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(x, acc, 'b', label='Training acc')\n","    plt.plot(x, val_acc, 'r', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(x, loss, 'b', label='Training loss')\n","    plt.plot(x, val_loss, 'r', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.legend()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.backend import batch_get_value\n","def plot_history_opt(history,opt,batch):\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    x = range(1, len(acc) + 1)\n","\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(x, acc, 'b', label='Training acc')\n","    plt.plot(x, val_acc, 'r', label='Validation acc')\n","    plt.title('Training and validation accuracy for '+ opt+' with '+batch)\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(x, loss, 'b', label='Training loss')\n","    plt.plot(x, val_loss, 'r', label='Validation loss')\n","    plt.title('Training and validation loss for '+ opt+' with '+batch)\n","    plt.legend()"],"metadata":{"id":"l4XNgQgijuDC"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XCfV4RKmRbq1"},"source":["import sklearn\n","import seaborn as sns \n","\n","import matplotlib.pyplot as plt \n","def plot_history_loss(history):\n","   \n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    x = range(1, len(loss) + 1)\n","  \n","    plt.plot(x, loss, 'b', label='Training loss')\n","    plt.plot(x, val_loss, 'r', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.legend()\n","\n","def plot_multiclass_roc(clf,x_test, y_test, n_classes, figsize=(17, 10)):\n","\n","    # structures\n","    fpr = dict()\n","    tpr = dict()\n","    \n","    roc_auc = dict()\n","    y_score1 = clf.predict(x_test)\n","    # calculate dummies once\n","    for i in range(n_classes):\n","        fpr[i], tpr[i], _ = sklearn.metrics.roc_curve(y_test[:, i].ravel(), y_score1[:, i])\n","        roc_auc[i] = sklearn.metrics.auc(fpr[i], tpr[i])\n","\n","    # roc for each class\n","    fig, ax = plt.subplots(figsize=figsize)\n","    ax.plot([0, 1], [0, 1], 'k--')\n","    ax.set_xlim([0.0, 1.0])\n","    ax.set_ylim([0.0, 1.05])\n","    ax.set_xlabel('False Positive Rate')\n","    ax.set_ylabel('True Positive Rate')\n","    ax.set_title('Receiver operating characteristic example')\n","    for i in range(n_classes):\n","        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for label %s' % (roc_auc[i], labels[i]))\n","    ax.legend(loc=\"best\")\n","    ax.grid(alpha=.4)\n","    sns.despine()\n","    plt.show()\n","import matplotlib as mpl\n","mpl.rcParams['figure.figsize'] = (12, 10)\n","colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n","def plot_metrics(history):\n","    import matplotlib.pyplot as plt\n","    metrics = ['loss', 'auc', 'precision', 'recall']\n","    for n, metric in enumerate(metrics):\n","        name = metric.replace(\"_\",\" \").capitalize()\n","        plt.subplot(2,2,n+1)\n","        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n","        plt.plot(history.epoch, history.history['val_'+metric],\n","                 color=colors[0], linestyle=\"--\", label='Val')\n","        plt.xlabel('Epoch')\n","        plt.ylabel(name)\n","        if metric == 'loss':\n","            plt.ylim([0, plt.ylim()[1]])\n","        elif metric == 'auc':\n","             plt.ylim([0.8,1])\n","        else:\n","            plt.ylim([0,1])\n","\n","        plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8PYgoGGoOIk"},"source":["#per_char=pd.read_csv('/content/drive/My Drive/Char_perm.csv')\n","per_level=pd.read_csv('/content/drive/My Drive/permissions.csv')\n","import  tensorflow as tf \n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","encoder=LabelEncoder()\n","encode_label=encoder.fit_transform(per_level['type'])\n","labels_full=to_categorical(encode_label)\n","labels=np.unique(per_level.type).tolist()\n","labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2wJGX4_YBGKt"},"source":["Adware=per_level[per_level['type']=='SMS']\n","Adware"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1XjrLIgXvA_L"},"source":["pivot=pd.pivot_table(\n","                     per_level,\n","                     values='Unnamed: 0.1',\n","                    index=['permmisions'],\n","                     columns=['type']\n","                     )\n","pivot=pivot.fillna(0).astype(np.int64)\n","pivot['Riskware']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JmtI1liQyAMd"},"source":["#plot = pivot[1:5].plot(kind='bar',figsize=(3,6))\n","#plot.tick_params(rotation=40)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJX498ojrXIR"},"source":["binary_label=[ ]\n"," \n","per_level=pd.read_csv('/content/drive/My Drive/permissions.csv')\n","per_level['binary']=' '\n","for i,m in enumerate(per_level['type']) :\n","  if m=='Benign':\n","     binary_label.append('B')\n","  else:\n","    binary_label.append('M')\n","per_level['binary']= binary_label\n","per_level.binary\n","import  tensorflow as tf \n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","encoder=LabelEncoder()\n","encode_label=encoder.fit_transform(per_level['binary'])\n","labels_full=to_categorical(encode_label)\n","labels=np.unique(per_level.binary).tolist()\n","labels\n","per_level=per_level.drop('type',axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kfQ3Rk82kWll"},"source":["#permiss = per_char['char_level'].values\n","per_level['permmisions']=per_level['permmisions'].apply(lambda x:str(x))\n","permiss=per_level['permmisions']\n","Count_perm=per_level['permmisions'].apply(lambda x:len(x.strip().replace('android.permission.','').replace('android.','').replace(',','.').replace('_','.').split('.')))\n","from keras.preprocessing.sequence import pad_sequences\n","max_length = max([len(s.replace('.','-').split('_')) for s in permiss])\n","max_length\n","#permiss_char=per_char['char_level'].apply(lambda x:str(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NFRveH1aXpHB"},"source":["#from keras.preprocessing.sequence import pad_sequences\n","#max_length_char = max([len(s) for s in permiss_char])\n","#max_length_char"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMmBGEn8PP_0"},"source":["from keras.preprocessing.text import Tokenizer\n","def tokn(text):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(text)\n","\n","    train = tokenizer.texts_to_sequences(text)\n","\n","\n","    vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n","    vocab_size\n","    return train,vocab_size\n","train,vocab_size_word=tokn(permiss)\n","print (vocab_size_word)\n","perm_train,perm_test, y_train, y_test = train_test_split( train, labels_full, test_size=0.33,random_state=42,shuffle=True)\n","#perm_train_ch,perm_test_ch, y_train_ch, y_test_ch = train_test_split( train_char, labels_full, test_size=0.33, random_state=42,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","encoded_docs,vocab_size_word=tokn(permiss)\n","perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n","from keras.preprocessing.sequence import pad_sequences\n","maxlen = 249\n","\n","X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n","\n","def conv_1d():\n","    model_c1 = Sequential()\n","    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n","    \n","    model_c1.add(Dropout(0.4))\n","   \n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Flatten())\n","    \n","    model_c1.add(Dense(100, activation='relu'))\n","\n","    model_c1.add(Dense(2, activation='softmax'))\n","    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","              tf. keras.metrics.FalsePositives(name='fp'),\n","              tf.keras.metrics.TrueNegatives(name='tn'),\n","              tf.keras.metrics.FalseNegatives(name='fn'),\n","              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","              tf.keras.metrics.Precision(name='precision'),\n","              tf.keras.metrics.Recall(name='recall'),\n","              tf.keras.metrics.AUC(name='auc')]\n","\n","    model_c1.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=metrics1)\n","    return model_c1\n","\n","\n","\n","\n","model_c1=conv_1d()\n","metrics2=[\"accuracy\"]\n","start = time.process_time()\n","# fit network\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n","#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n","history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])\n","plot_history(history_c1)"],"metadata":{"id":"hpaaeoQVqhT3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mn9_SxfTYm_2"},"source":[]},{"cell_type":"code","metadata":{"id":"ME_0QjhBQyJi"},"source":["'''\n","def model_lstm(embedding_dim ,vocab_size,maxlen):\n","      model = Sequential()\n","      model.add(Embedding(input_dim=vocab_size, \n","                                output_dim=embedding_dim, \n","                                input_length=maxlen))\n","\n","      model.add((LSTM(256,dropout=0.1, return_sequences=True)))\n","\n","      model.add(SpatialDropout1D(0.6))\n","\n","\n","      model.add((LSTM(128,dropout=0.1, return_sequences=True)))\n","      model.add(SpatialDropout1D(0.6))\n","      model.add((LSTM(64,dropout=0.1, return_sequences=True)))\n","      model.add(SpatialDropout1D(0.4))\n","\n","      model.add(Flatten())\n","      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n","      model.add(Dense(5, activation='softmax'))\n","\n","      model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.0001), metrics=['accuracy'])\n","      model.summary()'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6JLFlNgbF28"},"source":["from keras.preprocessing.sequence import pad_sequences\n","from keras.wrappers.scikit_learn import KerasClassifier\n","maxlen = 249\n","\n","X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n","\n","#maxlen_char  = max_length_char\n","#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)\n","#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)\n","\n","def model_lstm(embedding_dim ,vocab_size,maxlen):\n","      model = Sequential()\n","      model.add(Embedding(input_dim=vocab_size, \n","                                output_dim=embedding_dim, \n","                                input_length=maxlen))\n","      model.add(SpatialDropout1D(0.3))\n","\n","      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n","\n","      model.add(SpatialDropout1D(0.6))\n","\n","\n","      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n","      model.add(SpatialDropout1D(0.7))\n","      \n","      \n","\n","\n","      model.add(Flatten())\n","      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n","      model.add(Dense(maxlen, activation='relu'))\n","      model.add(Dropout(0.8))\n","      model.add(Dense(2, activation='softmax'))\n","      return model\n","epochs = 50\n","model=model_lstm(110,vocab_size_word,maxlen)\n","batch_size = 256\n","metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","            tf. keras.metrics.FalsePositives(name='fp'),\n","            tf.keras.metrics.TrueNegatives(name='tn'),\n","            tf.keras.metrics.FalseNegatives(name='fn'),\n","            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","            tf.keras.metrics.Precision(name='precision'),\n","            tf.keras.metrics.Recall(name='recall'),\n","            tf.keras.metrics.AUC(name='auc')]\n","metrics2=[\"accuracy\"]\n","model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)\n","model.summary()\n","#print(model.layers[0].get_weights()[0])\n","import time\n","start = time.process_time()\n","print(start)\n","history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n","\n","#loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n","#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n","plot_history(history_lstm)\n","end = time.process_time()\n","print(\"Time for LSTM model : {} \".format((end-start)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install scikeras[tensorflow]"],"metadata":{"id":"odvzO-XW4zqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.sequence import pad_sequences\n","\n","import scikeras\n","from scikeras.wrappers import KerasClassifier\n","\n","maxlen = 249\n","from keras.preprocessing.text import Tokenizer\n","def tokn(text):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(text)\n","\n","    train = tokenizer.texts_to_sequences(text)\n","\n","\n","    vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n","    vocab_size\n","    return train,vocab_size\n","train,vocab_size_word=tokn(permiss)\n","print (vocab_size_word)\n","X_train = pad_sequences(train, padding='post', maxlen=maxlen)\n","\n","#maxlen_char  = max_length_char\n","#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)\n","#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)\n","import time\n","start = time.process_time()\n","print(start)\n","def model_lstm(embedding_dim ,vocab_size,maxlen):\n","      model = Sequential()\n","      model.add(Embedding(input_dim=vocab_size, \n","                                output_dim=embedding_dim, \n","                                input_length=maxlen))\n","      model.add(SpatialDropout1D(0.3))\n","\n","      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n","\n","      model.add(SpatialDropout1D(0.6))\n","\n","\n","      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n","      model.add(SpatialDropout1D(0.7))\n","      \n","      \n","\n","\n","      model.add(Flatten())\n","      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n","      model.add(Dense(maxlen, activation='relu'))\n","      model.add(Dropout(0.8))\n","      model.add(Dense(5, activation='softmax'))\n","      epochs = 50\n","     \n","      metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","            tf. keras.metrics.FalsePositives(name='fp'),\n","            tf.keras.metrics.TrueNegatives(name='tn'),\n","            tf.keras.metrics.FalseNegatives(name='fn'),\n","            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","            tf.keras.metrics.Precision(name='precision'),\n","            tf.keras.metrics.Recall(name='recall'),\n","            tf.keras.metrics.AUC(name='auc')]\n","      metrics2=[\"accuracy\"]\n","      model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)\n","      return model\n","for b in [32,64,128,256,512,1024]:\n","    model=model_lstm(110,vocab_size_word,maxlen)\n","    clf = KerasClassifier(model, epochs=50, batch_size=b, verbose=0)\n","    from sklearn.model_selection import StratifiedKFold, cross_val_score\n","\n","    trans = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","    import pandas as pd\n","\n","    # Keras classifiers work with one hot encoded categorical columns (e.g. [[1 0 0], [0 1 0], ...]).\n","    # StratifiedKFold works with categorical encoded columns (e.g. [1 2 3 1 ...]).\n","    # This requires juggling the representation at shuffle time versus at runtime.\n","    scores = []\n","    history_model=[]\n","    for train_idx, test_idx in trans.split(X_train, np.argmax(labels_full,axis=1)):\n","        X_cv, y_cv = X_train[train_idx],pd.get_dummies(np.argmax(labels_full,axis=1)[train_idx]).values\n","        test_cv,y_test_cv =X_train[test_idx],pd.get_dummies(np.argmax(labels_full,axis=1)[test_idx]).values\n","        history=clf.fit(X_cv, y_cv,callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)],validation_data=(test_cv,y_test_cv))\n","        history_model.append(history.history_)\n","        scores.append(clf.score(X_cv, y_cv))\n","    print(\"batches is \" + str(b))\n","    print(np.mean(history.history_['val_recall']))\n","    print(np.mean(history.history_['val_precision']))\n","    print(np.mean(history.history_['val_accuracy']))\n","    end = time.process_time()\n","    print(\"Time for LSTM model : {} \".format((end-start)))"],"metadata":{"id":"93-skjL_0KEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","encoded_docs,vocab_size_word=tokn(permiss)\n","perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n","from keras.preprocessing.sequence import pad_sequences\n","maxlen = 249\n","\n","X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n","\n","def conv_1d():\n","    model_c1 = Sequential()\n","    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n","    \n","    model_c1.add(Dropout(0.4))\n","   \n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Flatten())\n","    \n","    model_c1.add(Dense(100, activation='relu'))\n","\n","    model_c1.add(Dense(5, activation='softmax'))\n","    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","              tf. keras.metrics.FalsePositives(name='fp'),\n","              tf.keras.metrics.TrueNegatives(name='tn'),\n","              tf.keras.metrics.FalseNegatives(name='fn'),\n","              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","              tf.keras.metrics.Precision(name='precision'),\n","              tf.keras.metrics.Recall(name='recall'),\n","              tf.keras.metrics.AUC(name='auc')]\n","\n","    model_c1.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=metrics1)\n","    return model_c1\n","    model=conv_1d()\n","    for batch in [32,64,128,256,512,1024]:\n","        model_c1=conv_1d(opt)\n","        metrics2=[\"accuracy\"]\n","        start = time.process_time()\n","        # fit network\n","        from keras.callbacks import ReduceLROnPlateau\n","        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n","        #model.fit(X_train, Y_train, callbacks=[reduce_lr])\n","        clf_conv = KerasClassifier(model, epochs=50, batch_size=b, verbose=0)\n","        from sklearn.model_selection import StratifiedKFold, cross_val_score\n","\n","        trans = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","        import pandas as pd\n","\n","        # Keras classifiers work with one hot encoded categorical columns (e.g. [[1 0 0], [0 1 0], ...]).\n","        # StratifiedKFold works with categorical encoded columns (e.g. [1 2 3 1 ...]).\n","        # This requires juggling the representation at shuffle time versus at runtime.\n","        scores = []\n","        history_model=[]\n","        for train_idx, test_idx in trans.split(X_train, np.argmax(labels_full,axis=1)):\n","            X_cv, y_cv = X_train[train_idx],pd.get_dummies(np.argmax(labels_full,axis=1)[train_idx]).values\n","            test_cv,y_test_cv =X_train[test_idx],pd.get_dummies(np.argmax(labels_full,axis=1)[test_idx]).values\n","            history=clf_conv.fit(X_cv, y_cv,callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)],validation_data=(test_cv,y_test_cv),callbacks=[reduce_lr])\n","            history_model.append(history.history_)\n","            scores.append(clf.score(X_cv, y_cv))\n","        print(\"batches is \" + str(b))\n","        print(np.mean(history.history_['val_recall']))\n","        print(np.mean(history.history_['val_precision']))\n","        print(np.mean(history.history_['val_accuracy']))\n","        end = time.process_time()\n","        print(\"Time for LSTM model : {} \".format((end-start)))"],"metadata":{"id":"T6t8d82bI75K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","print(np.mean(history.history_['val_recall']))\n","print(np.mean(history.history_['val_precision']))\n","print(np.mean(history.history_['val_accuracy']))"],"metadata":{"id":"nZ22uIGQIHep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()\n","#print(model.layers[0].get_weights()[0])\n","import time\n","start = time.process_time()\n","print(start)\n","history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n","\n","#loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n","#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n","plot_history(history_lstm)\n","end = time.process_time()\n","print(\"Time for LSTM model : {} \".format((end-start)))"],"metadata":{"id":"SjBAOmjm7SlI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pip install pretty_confusion_matrix"],"metadata":{"id":"QWIRd690u2ZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score\n","\n","Y_prediction = model.predict(X_test, verbose=1)\n","# Convert predictions classes to one hot vectors \n","\n","\n","Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n","Y_pred_classes"],"metadata":{"id":"RjTrpQdqQILN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score\n","Y_prediction = model.predict(X_test, verbose=1)\n","# Convert predictions classes to one hot vectors \n","\n","\n","Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n","# Convert validation observations to one hot vectors\n","Y_true = np.argmax(y_test,axis = 1) \n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n","\n","df_cm = pd.DataFrame(confusion_mtx, index=np.unique(per_level.type.values), columns=np.unique(per_level.type.values))\n","from pretty_confusion_matrix import pp_matrix\n","cmap = 'PuRd'\n","cm=pp_matrix(df_cm,cmap=plt.cm.CMRmap_r)\n"],"metadata":{"id":"vPRnUtN_ug2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y_prediction"],"metadata":{"id":"K9io5YKXRCbx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y=np.argmax(Y_prediction,axis=1)\n","Y_true"],"metadata":{"id":"UnrrUbNQRCqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y=np.argmax(Y_prediction,axis=1)\n","Y_true\n","y=np.argmax(Y_prediction,axis=1)\n","Y_true\n","print(f1_score(Y_true,y,average='micro'))"],"metadata":{"id":"yGspToX2RAcL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["end = time.process_time()\n","print(\"Time for LSTM model : {} \".format((end-start)))"],"metadata":{"id":"BHdI_LvvEH6V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","Y_prediction = model.predict(X_test, verbose=1)\n","# Convert predictions classes to one hot vectors \n","\n","\n","Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n","# Convert validation observations to one hot vectors\n","Y_true = np.argmax(y_test,axis = 1) \n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n","plt.figure(figsize=(10,8))\n","#sns.heatmap(confusion_mtx/np.sum(confusion_mtx), annot=True, fmt=\"d\",xticklabels=np.unique(per_level.type.values),yticklabels=np.unique(per_level.type.values));\n","sns.heatmap(confusion_mtx/np.sum(confusion_mtx), annot=True,xticklabels=np.unique(per_level.type.values),yticklabels=np.unique(per_level.type.values));\n","plt.title('Confusion matrix')\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')"],"metadata":{"id":"jSyKb9Efkg9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.sequence import pad_sequences\n","maxlen = 249\n","epochs = 50\n","batch_size = 256\n","X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n","\n","#maxlen_char  = max_length_char\n","#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)\n","#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)\n","\n","def model_lstm(embedding_dim,vocab_size,maxlen,opt):\n","      model = Sequential()\n","      model.add(Embedding(input_dim=vocab_size, \n","                                output_dim=embedding_dim, \n","                                input_length=maxlen))\n","      model.add(SpatialDropout1D(0.3))\n","\n","      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n","\n","      model.add(SpatialDropout1D(0.6))\n","\n","\n","      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n","      model.add(SpatialDropout1D(0.7))\n","      \n","      \n","\n","\n","      model.add(Flatten())\n","      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n","      model.add(Dense(maxlen, activation='relu'))\n","      model.add(Dropout(0.8))\n","      model.add(Dense(5, activation='softmax'))\n","     \n","      metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","            tf. keras.metrics.FalsePositives(name='fp'),\n","            tf.keras.metrics.TrueNegatives(name='tn'),\n","            tf.keras.metrics.FalseNegatives(name='fn'),\n","            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","            tf.keras.metrics.Precision(name='precision'),\n","            tf.keras.metrics.Recall(name='recall'),\n","            tf.keras.metrics.AUC(name='auc')]\n","      metrics2=[\"accuracy\"]\n","      model.compile(loss='categorical_crossentropy',optimizer=opt, metrics= metrics1)\n","      return model\n","for m in ['Adam','RMSprop','SGD','Nadam']:\n","    for batch in [32,64,1,28,256,512,1024]:\n","        model =model_lstm(110,vocab_size_word,maxlen,m)\n","\n","    #print(model.layers[0].get_weights()[0])\n","        import time\n","        start = time.process_time()\n","        print(start)\n","        history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n","\n","        #loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n","        #print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n","        plot_history_opt(history_lstm,m,batch)\n","        end = time.process_time()\n","        print(\"Time for LSTM model : {} \".format((end-start)))\n"],"metadata":{"id":"128IoozIcCC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","encoded_docs,vocab_size_word=tokn(permiss)\n","perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n","from keras.preprocessing.sequence import pad_sequences\n","maxlen = 249\n","\n","X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n","\n","def conv_1d(opt):\n","    model_c1 = Sequential()\n","    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n","    \n","    model_c1.add(Dropout(0.4))\n","   \n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Flatten())\n","    \n","    model_c1.add(Dense(100, activation='relu'))\n","\n","    model_c1.add(Dense(5, activation='softmax'))\n","    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","              tf. keras.metrics.FalsePositives(name='fp'),\n","              tf.keras.metrics.TrueNegatives(name='tn'),\n","              tf.keras.metrics.FalseNegatives(name='fn'),\n","              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","              tf.keras.metrics.Precision(name='precision'),\n","              tf.keras.metrics.Recall(name='recall'),\n","              tf.keras.metrics.AUC(name='auc')]\n","\n","    model_c1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=metrics1)\n","    return model_c1\n","\n","\n","for opt in ['Adam','RMSprop','SGD','Nadam']:\n","   for batch in [32,64,128,256,512,1024]:\n","\n","        model_c1=conv_1d(opt)\n","        metrics2=[\"accuracy\"]\n","        start = time.process_time()\n","        # fit network\n","        from keras.callbacks import ReduceLROnPlateau\n","        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n","        #model.fit(X_train, Y_train, callbacks=[reduce_lr])\n","        history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=batch,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])\n","        plot_history_opt(history_c1,opt,str(batch))"],"metadata":{"id":"sD6Rjk2yfuVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["per_level['permmisions']=per_level['permmisions'].apply(lambda x:str(x).lower())\n","permiss=per_level['permmisions']\n","perm_split=per_level['permmisions'].apply(lambda x:x.replace(',','').replace('android.permission.',' ').strip().split('.')[-1])\n","lis_value=set()\n","lis_u=set()\n","lis_length=[ ]\n","for perm in perm_split:\n","    for p in perm.split(' ') :\n","        if p!='' or p!='nan' :\n","             lis_u.add(p)\n","\n","dic={}\n","for i,perm in enumerate(perm_split):\n","      lis_value=[ ]\n","      for p in list(lis_u) :\n","           if p in perm:\n","              lis_value.append(1) \n","           else:\n","               lis_value.append(0) \n","      dic.update({i:lis_value})\n","data=pd.DataFrame(dic.values(),columns=list(lis_u))\n","data[data['bind_notification_listener_service']==1]"],"metadata":{"id":"nVx0Nq4LOA9q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","X_train, X_test, y_train, y_test = train_test_split( data,labels_full, test_size=0.33,shuffle=True)\n","X_train=np.expand_dims(X_train,-1)\n","X_test=np.expand_dims(X_test,-1)\n","X_train.shape,X_test.shape"],"metadata":{"id":"IazwC5lEPWdx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","   \n","\n","def Lst_model_binary(X_t):\n","    model=Sequential()  \n","    model.add((LSTM(128,dropout=0.5,input_shape=X_t.shape[1:],return_sequences=True)))\n","    model.add(SpatialDropout1D(0.3))\n","\n","    model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n","\n","    model.add(SpatialDropout1D(0.6))\n","\n","\n","    model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n","    model.add(SpatialDropout1D(0.7))\n","    \n","    model.add(Flatten())\n","\n","    model.add(Dense(100, activation='relu'))\n","    model.add(Dropout(0.8))\n","    model.add(Dense(5, activation='softmax'))\n","   \n","    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n","                            patience=5, verbose=1, mode='auto')\n","    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","            tf. keras.metrics.FalsePositives(name='fp'),\n","            tf.keras.metrics.TrueNegatives(name='tn'),\n","            tf.keras.metrics.FalseNegatives(name='fn'),\n","            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","            tf.keras.metrics.Precision(name='precision'),\n","            tf.keras.metrics.Recall(name='recall'),\n","            tf.keras.metrics.AUC(name='auc')]\n","    model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)\n","    return model\n","\n","for batch in [32,64,128,256,512,1024]:\n","    model =Lst_model_binary(X_train)\n","    print('optmizer'+ m+str(batch))\n","#print(model.layers[0].get_weights()[0])\n","    import time\n","    start = time.process_time()\n","    print(start)\n","    history_lstm = model.fit(X_train, y_train, epochs=50, batch_size=batch,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n","    end = time.process_time()\n","    print(\"Time for LSTM model : {} \".format((end-start)))\n"],"metadata":{"id":"fp6wWG6WKlbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lis_value=()\n","dic={}\n","for i,perm in enumerate(perm_split):\n","      lis_value=[ ]\n","      for p in list(lis_u) :\n","           if p in perm:\n","              lis_value.append(1) \n","           else:\n","               lis_value.append(0) \n","      dic.update({i:lis_value})\n","data=pd.DataFrame(dic.values(),columns=list(lis_u))\n","Full_data=pd.concat([data,per_level['type']],axis=1)"],"metadata":{"id":"y_KKu8R5LByu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import  tensorflow as tf \n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","encoder=LabelEncoder()\n","encode_label=encoder.fit_transform(per_level['type'])\n","labels_full=to_categorical(encode_label)\n","labels_full.shape"],"metadata":{"id":"jkv92ds1izD8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train,vocab_size_word=tokn(permiss)\n","Full_train = pad_sequences(train, padding='post', maxlen=maxlen)\n","# Use scikit-learn to grid search the batch size and epochs\n","import numpy\n","import tensorflow  \n","from sklearn.model_selection import GridSearchCV\n","from keras.models import Sequential\n","from keras.layers import Dense\n","import scikeras\n","from scikeras.wrappers import KerasClassifier\n","#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","# Function to create model, required for KerasClassifier\n","def model_lstm( optimizer=\"SGD\" ):\n","      embedding_dim=110\n","      vocab_size=vocab_size_word\n","      model = Sequential()\n","      model.add(Embedding(input_dim=vocab_size, \n","                                output_dim=embedding_dim, \n","                                input_length=maxlen))\n","      model.add(SpatialDropout1D(0.3))\n","\n","      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n","\n","      model.add(SpatialDropout1D(0.6))\n","\n","\n","      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n","      model.add(SpatialDropout1D(0.7))\n","      \n","      \n","      model.add(Flatten())\n","      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n","      model.add(Dense(maxlen, activation='relu'))\n","      model.add(Dropout(0.8))\n","      model.add(Dense(5, activation='softmax'))\n","      metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","            tf. keras.metrics.FalsePositives(name='fp'),\n","            tf.keras.metrics.TrueNegatives(name='tn'),\n","            tf.keras.metrics.FalseNegatives(name='fn'),\n","            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","            tf.keras.metrics.Precision(name='precision'),\n","            tf.keras.metrics.Recall(name='recall'),\n","            tf.keras.metrics.AUC(name='auc')]\n","      metrics2=[\"accuracy\"]\n","      model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics= metrics2)\n","      return model\n","# fix random seed for reproducibility\n","\n","start=time.process_time()\n","# split into input (X) and output (Y) variables\n","X = Full_train\n","Y =labels_full\n","# create model\n","model_grid = KerasClassifier(build_fn=model_lstm, verbose=0)\n","# define the grid search parameters\n","\n","batches=[64,128,256,512,1024]\n","epoches=[50]\n","param_grid = dict(epochs=epoches,batch_size=batches)\n","grid = GridSearchCV(estimator=model_grid, param_grid=param_grid,cv=50)\n","grid_result = grid.fit(X, Y)\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","end = time.process_time()\n","print(\"Time for LSTM model : {} \".format((end-start)))"],"metadata":{"id":"808fnu_LjHKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install scikeras[tensorflow]\n","import scikeras\n","from scikeras.wrappers import KerasClassifier"],"metadata":{"id":"Rl2DZ-FFJr71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use scikit-learn to grid search the batch size and epochs\n","import numpy\n","import tensorflow  \n","from sklearn.model_selection import GridSearchCV\n","from keras.models import Sequential\n","from keras.layers import Dense\n","import scikeras\n","from scikeras.wrappers import KerasClassifier\n","#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","# Function to create model, required for KerasClassifier\n","def model_lstm( optimizer=\"adam\" ):\n","      embedding_dim=110\n","      vocab_size=vocab_size_word\n","      model = Sequential()\n","      model.add(Embedding(input_dim=vocab_size, \n","                                output_dim=embedding_dim, \n","                                input_length=maxlen))\n","      model.add(SpatialDropout1D(0.3))\n","\n","      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n","\n","      model.add(SpatialDropout1D(0.6))\n","\n","\n","      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n","      model.add(SpatialDropout1D(0.7))\n","      \n","      \n","      model.add(Flatten())\n","      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n","      model.add(Dense(maxlen, activation='relu'))\n","      model.add(Dropout(0.8))\n","      model.add(Dense(5, activation='softmax'))\n","      metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","            tf. keras.metrics.FalsePositives(name='fp'),\n","            tf.keras.metrics.TrueNegatives(name='tn'),\n","            tf.keras.metrics.FalseNegatives(name='fn'),\n","            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","            tf.keras.metrics.Precision(name='precision'),\n","            tf.keras.metrics.Recall(name='recall'),\n","            tf.keras.metrics.AUC(name='auc')]\n","      metrics2=[\"accuracy\"]\n","      model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics= metrics2)\n","      return model\n","# fix random seed for reproducibility\n","\n","start=time.process_time()\n","# split into input (X) and output (Y) variables\n","X = Full_train\n","Y =labels_full\n","# create model\n","model_grid = KerasClassifier(build_fn=model_lstm, verbose=0)\n","# define the grid search parameters\n","optimizers = ['SGD','Adam']\n","batches=[64,128,256,512,1024]\n","epoches=[1]\n","param_grid = dict(optimizer=optimizers,epochs=epoches,batch_size=batches)\n","grid = GridSearchCV(estimator=model_grid, param_grid=param_grid,cv=5)\n","grid_result = grid.fit(X, Y)\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","end = time.process_time()\n","print(\"Time for LSTM model : {} \".format((end-start)))"],"metadata":{"id":"YFuoN1T_RcWS"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zv-M3nMHgJUA"},"source":["plot_multiclass_roc(model, X_test, y_test, n_classes=5, figsize=(5, 5))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wfefAfio6Jjr"},"source":["plot_metrics(history_lstm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzAPV9ZgQBia"},"source":["dic={}\n","lis_sp=[]\n","perm_str=' '\n","for i,pre in enumerate(Adware['permmisions']) :\n","  for c,m in enumerate( pre.split(',')):\n","    if \"READ\" in m.replace('_',' '):\n","        lis_sp.append(c)\n","        print(lis_sp)\n","  lis=pre.split(',')\n","  print(lis)\n","  dic.update({i:lis})  \n","            \n","print(dic[10])\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwDONLh96eAg"},"source":["import matplotlib.pyplot as plt\n","metrics = ['tp', 'fp', 'tn','fn']\n","for n, metric in enumerate(metrics):\n","    name = metric.replace(\"_\",\" \").capitalize()\n","    plt.subplot(2,2,n+1)\n","    plt.bar(history_lstm.epoch, history_lstm.history[metric], color=colors[0], label='Train')\n","   \n","    plt.xlabel('Epoch')\n","    plt.ylabel(name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YsJT9SIM5pJl"},"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","Y_prediction = model.predict(X_test, verbose=1)\n","# Convert predictions classes to one hot vectors \n","\n","\n","Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n","# Convert validation observations to one hot vectors\n","Y_true = np.argmax(y_test,axis = 1) \n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n","plt.figure(figsize=(10,8))\n","sns.heatmap(confusion_mtx, annot=True, fmt=\"d\",xticklabels=np.unique(per_level.type.values),yticklabels=np.unique(per_level.type.values));\n","plt.title('Confusion matrix')\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oP5cww57kFBJ"},"source":["encoded_docs,vocab_size_word=tokn(permiss)\n","perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n","from keras.preprocessing.sequence import pad_sequences\n","maxlen = 249\n","\n","X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xlLABv5JGGt1"},"source":["\n","\n","\n","def conv_1d():\n","    model_c1 = Sequential()\n","    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n","    \n","    model_c1.add(Dropout(0.4))\n","   \n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Flatten())\n","    \n","    model_c1.add(Dense(100, activation='relu'))\n","\n","    model_c1.add(Dense(5, activation='softmax'))\n","\n","    print(model_c1.summary())\n","    return model_c1\n","\n","metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","              tf. keras.metrics.FalsePositives(name='fp'),\n","              tf.keras.metrics.TrueNegatives(name='tn'),\n","              tf.keras.metrics.FalseNegatives(name='fn'),\n","              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","              tf.keras.metrics.Precision(name='precision'),\n","              tf.keras.metrics.Recall(name='recall'),\n","              tf.keras.metrics.AUC(name='auc')]\n","model_c1=conv_1d()\n","metrics2=[\"accuracy\"]\n","start = time.process_time()\n","model_c1.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=\"accuracy\")\n","# fit network\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n","#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n","\n","history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTF_N9GfG8K1"},"source":["plot_history(history_c1)\n","end = time.process_time()\n","print(\"Time for LSTM model : {} \".format((end-start)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oH49Yd2fCjpQ"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","Y_prediction = model_c1.predict(X_test, verbose=1)\n","# Convert predictions classes to one hot vectors \n","\n","\n","Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n","# Convert validation observations to one hot vectors\n","Y_true = np.argmax(y_test,axis = 1) \n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n","df_cm = pd.DataFrame(confusion_mtx, index=np.unique(per_level.type.values), columns=np.unique(per_level.type.values))\n","from pretty_confusion_matrix import pp_matrix\n","cmap = 'PuRd'\n","pp_matrix(df_cm,cmap=plt.cm.CMRmap_r)"],"metadata":{"id":"9cUZTk_2CJsJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y=np.argmax(Y_prediction,axis=1)\n","Y_true\n","print(f1_score(Y_true,y,average='micro'))"],"metadata":{"id":"p-KPvTWOQyAm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PMKsBVoXZC2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fix random seed for reproducibility\n","import numpy\n","seed = 7\n","numpy.random.seed(seed)\n","# load dataset\n","\n","# Use scikit-learn to grid search the batch size and epochs\n","import numpy\n","from sklearn.model_selection import GridSearchCV\n","from keras.models import Sequential\n","from keras.layers import Dense\n","#from keras.wrappers.scikit_learn import KerasClassifier\n","import scikeras\n","from scikeras.wrappers import KerasClassifier\n","train,vocab_size_word=tokn(permiss)\n","Full_train = pad_sequences(train, padding='post', maxlen=maxlen)\n","# Function to create model, required for KerasClassifier\n","def conv_1d(optimizer='adam'):\n","    model_c1 = Sequential()\n","    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n","    \n","    model_c1.add(Dropout(0.4))\n","   \n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Flatten())\n","    \n","    model_c1.add(Dense(100, activation='relu'))\n","\n","    model_c1.add(Dense(5, activation='softmax'))\n","\n","    print(model_c1.summary())\n","    model_c1.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=\"accuracy\")\n","    return model_c1\n","\n","# split into input (X) and output (Y) variables\n","X = Full_train\n","Y =labels_full\n","# create model\n","create_model=conv_1d()\n","model = KerasClassifier(build_fn=create_model,verbose=0)\n","# define the grid search parameters\n","epochs= [50]\n","batches=[64,128,256,512,1024]\n","#[ 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n","param_grid = dict(epochs=epochs,batch_size=batches)\n","grid = GridSearchCV(estimator=model, param_grid=param_grid)\n","grid_result = grid.fit(X, Y)\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))"],"metadata":{"id":"hW_JCIr3Q5rv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZcF8tP5wKX8U"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HI--bl3_G39Y"},"source":["\n","import matplotlib.pyplot as plt\n","metrics = ['tp', 'fp', 'tn','fn']\n","for n, metric in enumerate(metrics):\n","    name = metric.replace(\"_\",\" \").capitalize()\n","    plt.subplot(2,2,n+1)\n","    plt.bar(history_c1.epoch, history_c1.history[metric], color=colors[0], label='Train')\n","   \n","    plt.xlabel('Epoch')\n","    plt.ylabel(name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Pwk-mD76DQ-"},"source":["plot_multiclass_roc(model_c1, X_test, y_test, n_classes=5, figsize=(5, 5))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AK0LkT6d6uLr"},"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","Y_prediction = model.predict(X_test, verbose=1)\n","# Convert predictions classes to one hot vectors \n","\n","\n","Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n","# Convert validation observations to one hot vectors\n","Y_true = np.argmax(y_test,axis = 1) \n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n","plt.figure(figsize=(10,8))\n","sns.heatmap(confusion_mtx, annot=True, fmt=\"d\",xticklabels=np.unique(per_level.type.values),yticklabels=np.unique(per_level.type.values));\n","plt.title('Confusion matrix')\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWkRMxOM4xPn"},"source":["\n","metrics = ['tp', 'fp', 'tn','fn']\n","histoiry_model=['history_c1','history_CNN']\n","metric_c1 = {}\n","for i in metrics :\n","    name=i.capitalize()+'_' +'CONV1D'\n","    metric_c1.update({name:history_c1.history[i]})\n","    \n","  \n","\n","metric_lstm = {}\n","for i in metrics :\n","    name=i.capitalize()+'_' +'lstm'\n","    metric_lstm.update({name:history_lstm.history[i]})\n","metric_lstm['Fn_lstm']\n","names=['Fp_CONV1D','Fp_lstm','FN_CONV1D','FN_Lstm','TN_CONV1D','TN_lstm','Tp_CONV1D','TP_lstm']\n","plt.figure(figsize=[10,3])\n","\n","plt.bar(0,metric_c1['Fp_CONV1D'][-1],tick_label=names[0],width=0.2)\n","plt.bar(1,metric_lstm['Fp_lstm'][-1],tick_label=names[1],width=0.2)\n","plt.bar(2,metric_c1['Fn_CONV1D'][-1],tick_label=names[2],width=0.2)\n","plt.bar(3,metric_lstm['Fn_lstm'][-1],tick_label=names[3],width=0.2)\n","#plt.bar(2.5,np.min(metric_c1['Tn_CONV1D']),tick_label=names[4],width=0.2)\n","#plt.bar(3,np.min(metric_lstm['Tn_lstm']),tick_label=names[5],width=0.2)\n","#plt.bar(3.5,np.min(metric_c1['Tp_CONV1D']),tick_label=names[6],width=0.2)\n","#plt.bar(4,np.mean(metric_lstm['Tp_lstm']),tick_label=names[7],width=0.2)\n","\n","\n","\n","plt.xticks(range(0,4),names)\n","plt.ylabel('false postive rate and false negtive rate ')\n","plt.xlabel('Models')\n","\n","plt.savefig('model.png')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vk1eBI8nXrAs"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWwcOfBIE8i2"},"source":["#per_char=pd.read_csv('/content/drive/My Drive/Char_perm.csv')\n","per_level=pd.read_csv('/content/drive/My Drive/permissions.csv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKtHKpSzEPfj"},"source":["\n","\n","import  tensorflow as tf \n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","encoder=LabelEncoder()\n","encode_label=encoder.fit_transform(per_level['type'])\n","labels_full=to_categorical(encode_label)\n","labels=np.unique(per_level.type).tolist()\n","labels\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8jcwmjK6Ea14"},"source":["#permiss = per_char['char_level'].values\n","per_level['permmisions']=per_level['permmisions'].apply(lambda x:str(x))\n","permiss=per_level['permmisions']\n","Count_perm=per_level['permmisions'].apply(lambda x:len(x.strip().replace('android.permission.','').replace('android.','').replace(',','.').replace('_','.').split('.')))\n","from keras.preprocessing.sequence import pad_sequences\n","max_length = max([len(s.replace('.','-').split('_')) for s in permiss])\n","max_length\n","#permiss_char=per_char['char_level'].apply(lambda x:str(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pnp4V7MaEfm4"},"source":["train,vocab_size_word=tokn(permiss)\n","perm_train,perm_test, y_train, y_test = train_test_split( train, labels_full, test_size=0.33,random_state=42,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ASp5W8tEEh0T"},"source":["from keras.preprocessing.sequence import pad_sequences\n","maxlen = 249\n","\n","X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n","\n","#maxlen_char  = max_length_char\n","#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)\n","#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)\n","\n","def model_lstm(embedding_dim ,vocab_size,maxlen):\n","      model = Sequential()\n","      model.add(Embedding(input_dim=vocab_size, \n","                                output_dim=embedding_dim, \n","                                input_length=maxlen))\n","      model.add(SpatialDropout1D(0.3))\n","\n","      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n","\n","      model.add(SpatialDropout1D(0.6))\n","\n","\n","      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n","      model.add(SpatialDropout1D(0.7))\n","      \n","      \n","\n","\n","      model.add(Flatten())\n","      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n","      model.add(Dense(maxlen, activation='relu'))\n","      model.add(Dropout(0.8))\n","      model.add(Dense(5, activation='softmax'))\n","      return model\n","epochs = 50\n","model=model_lstm(110,vocab_size_word,maxlen)\n","batch_size = 256\n","metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","            tf. keras.metrics.FalsePositives(name='fp'),\n","            tf.keras.metrics.TrueNegatives(name='tn'),\n","            tf.keras.metrics.FalseNegatives(name='fn'),\n","            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","            tf.keras.metrics.Precision(name='precision'),\n","            tf.keras.metrics.Recall(name='recall'),\n","            tf.keras.metrics.AUC(name='auc')]\n","metrics2=[\"accuracy\"]\n","model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)\n","model.summary()\n","\n","history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n","#loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n","#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n","plot_history(history_lstm)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pC02ANyqEr0c"},"source":["plot_multiclass_roc(model, X_test, y_test, n_classes=4, figsize=(5, 5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nWbXhVPOkXr8"},"source":["#per_char=pd.read_csv('/content/drive/My Drive/Char_perm.csv')\n","per_level=pd.read_csv('/content/drive/My Drive/permissions.csv')\n","np.unique(per_level.type)\n","per_level['binary']='Malicious'\n","per_level\n","\n","for i,m in enumerate (per_level['type']):\n","    if per_level['type'][i]=='Benign':\n","        per_level['binary'][i]='Benign'\n","    else: per_level['binary'][i]='Malicious'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pfZ35KALkZib"},"source":["per_level[per_level['binary']=='Benign']\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AF-Sdf7HEx4H"},"source":["encoded_docs,vocab_size_word=tokn(permiss)\n","from keras.preprocessing.sequence import pad_sequences\n","maxlen = 249\n","import  tensorflow as tf \n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","encoder=LabelEncoder()\n","encode_label=encoder.fit_transform(per_level['binary'])\n","labels_full=to_categorical(encode_label)\n","X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n","perm_train,perm_test, y_train, y_test = train_test_split( encoded_docs, labels_full, test_size=0.33,shuffle=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train"],"metadata":{"id":"N0x3aYVFKTw0"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hykYnpQEKls"},"source":["\n","\n","\n","def conv_1d():\n","    model_c1 = Sequential()\n","    model_c1.add(Embedding(vocab_size_word, 110, input_length=maxlen))\n","    \n","    model_c1.add(Dropout(0.4))\n","   \n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Flatten())\n","    \n","    model_c1.add(Dense(100, activation='relu'))\n","\n","    model_c1.add(Dense(2,activation='softmax'))\n","\n","    print(model_c1.summary())\n","    return model_c1\n","\n","metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","              tf. keras.metrics.FalsePositives(name='fp'),\n","              tf.keras.metrics.TrueNegatives(name='tn'),\n","              tf.keras.metrics.FalseNegatives(name='fn'),\n","              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","              tf.keras.metrics.Precision(name='precision'),\n","              tf.keras.metrics.Recall(name='recall'),\n","              tf.keras.metrics.AUC(name='auc')]\n","model_c1=conv_1d()\n","metrics2=[\"accuracy\"]\n","model_c1.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=\"accuracy\")\n","# fit network\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n","#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n","\n","history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CEwkL9ZrFLYx"},"source":["plot_multiclass_roc(model_c1, X_test, y_test, n_classes=2, figsize=(5, 5))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMO-hhJwlnxi"},"source":["\n","per_level['binary']='Malicious'\n","per_level\n","\n","for i,m in enumerate (per_level['type']):\n","    if per_level['type'][i]=='Benign':\n","        per_level['binary'][i]='B'\n","    else: per_level['binary'][i]='M'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xNRaKNNDltRH"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ikgb3NSKlVRj"},"source":["lis_value=()\n","dic={}\n","for i,perm in enumerate(perm_split):\n","      lis_value=[ ]\n","      for p in list(lis_u) :\n","           if p in perm:\n","              lis_value.append(1) \n","           else:\n","               lis_value.append(0) \n","      dic.update({i:lis_value})\n","data=pd.DataFrame(dic.values(),columns=list(lis_u))\n","Full_data=pd.concat([data,per_level['binary']],axis=1)\n","   "],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"q_CzZ_V0E9d7"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eEYXwpKpIi-3"},"source":["\n","lis_value=()\n","dic={}\n","for i,perm in enumerate(perm_split):\n","      lis_value=[ ]\n","      for p in list(lis_u) :\n","           if p in perm:\n","              lis_value.append(1) \n","           else:\n","               lis_value.append(0) \n","      dic.update({i:lis_value})\n","data=pd.DataFrame(dic.values(),columns=list(lis_u))\n","Full_data=pd.concat([data,per_level['type']],axis=1)\n","   \n","\n","X_train, X_test, y_train, y_test = train_test_split( train_data,labels_full, test_size=0.33,shuffle=True)\n","X_train=np.expand_dims(X_train,-1)\n","X_test=np.expand_dims(X_test,-1)\n","X_train.shape,X_test.shape\n","def Lst_model(X_t,opt):\n","    model=Sequential()  \n","\n","    model.add((LSTM(128,dropout=0.5,input_shape=X_t.shape[1:],return_sequences=True)))\n","\n","    model.add(SpatialDropout1D(0.6))\n","\n","\n","    model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n","    model.add(SpatialDropout1D(0.7))\n","    \n","    model.add(Flatten())\n","    #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n","    model.add(Dense(maxlen, activation='relu'))\n","    model.add(Dropout(0.8))\n","    model.add(Dense(5, activation='softmax'))\n","    adam = tf.keras.optimizers.Adam(0.0001)\n","    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n","                            patience=5, verbose=1, mode='auto')\n","    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","            tf. keras.metrics.FalsePositives(name='fp'),\n","            tf.keras.metrics.TrueNegatives(name='tn'),\n","            tf.keras.metrics.FalseNegatives(name='fn'),\n","            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","            tf.keras.metrics.Precision(name='precision'),\n","            tf.keras.metrics.Recall(name='recall'),\n","            tf.keras.metrics.AUC(name='auc')]\n","    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics= metrics1)\n","    return model\n","for m in ['Adam','RMSprop','SGD','Nadam']:\n","    for batch in [32,64,1,28,256,512,1024]:\n","        model =model_lstm(110,vocab_size_word,maxlen,m)\n","\n","    #print(model.layers[0].get_weights()[0])\n","        import time\n","        start = time.process_time()\n","        print(start)\n","        history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"suoiuumyJS-4"},"source":["lis_value=()\n","dic={}\n","for i,perm in enumerate(perm_split):\n","      lis_value=[ ]\n","      for p in list(lis_u) :\n","           if p in perm:\n","              lis_value.append(1) \n","           else:\n","               lis_value.append(0) \n","      dic.update({i:lis_value})\n","data=pd.DataFrame(dic.values(),columns=list(lis_u))\n","Full_data=pd.concat([data,per_level['binary']],axis=1)\n","   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BmZrAtS0sL4O"},"source":["!pip install visualkeras"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a155d9y_VIKs"},"source":["#   ELMo for Extracting Features from Text following sentence-transformers "]},{"cell_type":"code","metadata":{"id":"cniO4gdUZ-_L"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vqsY2P4lQhwC"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LFxGHaOSAhiJ"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhwRDlI2odkn"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KrUMGv1JPwNc"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuazEW0-O428"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMF7JqcGB2of"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzQW0k-P5VlB"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZzMjcbu7rSxH"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pA2zYHX-a1Tq"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OiHpGN2JuDdI"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NX5ysRT7Te-N"},"source":["'''\n","!pip install sentence-transformers\n","#per_char=pd.read_csv('/content/drive/My Drive/Char_perm.csv')\n","per_level=pd.read_csv('/content/drive/My Drive/permissions.csv')\n","import  tensorflow as tf \n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","encoder=LabelEncoder()\n","encode_label=encoder.fit_transform(per_level['type'])\n","labels_full=to_categorical(encode_label)\n","labels=np.unique(per_level.type).tolist()\n","labels\n","per_level['Context']=per_level['permmisions'].apply(lambda x:str(x).lower())\n","per_level['Context']=per_level['Context'].apply(lambda x:x.strip().replace('android.permission.','').replace('android.','').replace(',','.').replace('_','.').split('.'))\n","lis_3=[]\n","for perm in per_level['Context']:\n","    s=' '\n","    for m in perm:\n","      if len(m.strip()) >3:\n","          #print(m,len(m.strip()))\n","          s=s+'' +m +' '\n","          s= re.sub(\"[^-9A-Za-z ]\", \"\" ,s)\n","    lis_3.append(s.strip())\n","def cosine(u, v):\n","    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n","lis=[]\n","for i,perm in enumerate(per_level['Context']):\n","    s=' '    \n","    for m in perm:\n","        s = s +' ' +  m +' '\n","    lis.append(s.strip())\n","print(len(lis))\n","\n","per_level['binary']='Malicious'\n","per_level\n","\n","for i,m in enumerate (per_level['type']):\n","    if per_level['type'][i]=='Benign':\n","        per_level['binary'][i]=1\n","    else: per_level['binary'][i]=0\n","PREM=lis\n","\n","from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n","sentence_embeddings = model.encode(lis)\n","\n","for sentence, embedding in zip(PREM,sentence_embeddings):\n","    print(\"Sentence:\", sentence)\n","    print(\"Embedding:\",embedding)\n","    print(\"\")\n","    break\n","query = per_level['permmisions']\n","query_vec = model.encode([query[0]])[0]\n","for sent in lis:\n","  sim = cosine(query_vec, model.encode([sent])[0])\n","  print(\"Sentence = \", sent, \"; similarity = \", sim)\n","  break'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2Fc8kjdyIFo"},"source":["'''\n","from sklearn.model_selection import train_test_split\n","\n","xtrain, xvalid, ytrain, yvalid = train_test_split( sentence_embeddings,np.argmax(labels_full,axis=1), test_size=0.33,shuffle=True)'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8DtXUxAaD2jk"},"source":["'''\n","from keras.utils import np_utils\n","Y_train = ytrain\n","Y_test = yvalid\n","X_train = xtrain.reshape(xtrain.shape[0],16, 16, 3)\n","X_test = xvalid.reshape(xvalid.shape[0], 16, 16, 3)\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I7l8Lyey7siK"},"source":["\n","'''\n","\n","#plt.plot(history_CNN3.history['dense_4_accuracy'])\n","#plt.plot(history_CNN3.history['dense_4_accuracy'])\n","\n","\n","\n","'''\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BjWxsFr0kKql"},"source":["'''#X_train = xtrain.reshape(xtrain.shape[0],16, 48, 1).astype('float32')/255\n","#plt.imshow(X_train[0][:,:,0])'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CisTaJCMR3e7"},"source":["''''\n","\n","import keras\n","from tensorflow.python.framework.ops import disable_eager_execution\n","from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.losses import binary_crossentropy\n","from tensorflow.keras import backend as K\n","import numpy as np\n","import matplotlib.pyplot as plt\n","disable_eager_execution()\n","# Load MNIST dataset\n","input_train=X_train;target_train=Y_train;input_test=X_test;target_test=yvalid\n","\n","\n","# Data & model configuration\n","img_width, img_height = input_train.shape[1], input_train.shape[2]\n","batch_size = 32\n","no_epochs = 50\n","validation_split = 0.2\n","verbosity =0.9\n","latent_dim = 2\n","num_channels = 3\n","\n","# Reshape data\n","input_train = input_train.reshape(input_train.shape[0], img_height, img_width, num_channels)\n","input_test = input_test.reshape(input_test.shape[0], img_height, img_width, num_channels)\n","input_shape = (img_height, img_width, num_channels)\n","\n","# Parse numbers as floats\n","\n","\n","# Normalize data\n","input_train = input_train \n","input_test = input_test\n","\n","# # =================\n","# # Encoder\n","# # =================\n","\n","# Definition\n","i       = Input(shape=input_shape, name='encoder_input')\n","\n","cx      = Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(i)\n","cx      = Dropout(0.4)(cx)\n","\n","cx      = Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(cx)\n","cx      = BatchNormalization()(cx)\n","\n","\n","x       = Flatten()(cx)\n","x       = Dense(100, activation='relu')(x)\n","x       = BatchNormalization()(x)\n","mu      = Dense(latent_dim, name='latent_mu')(x)\n","sigma   = Dense(latent_dim, name='latent_sigma')(x)\n","\n","# Get Conv2D shape for Conv2DTranspose operation in decoder\n","conv_shape = K.int_shape(cx)\n","\n","# Define sampling with reparameterization trick\n","def sample_z(args):\n","  mu, sigma = args\n","  batch     = K.shape(mu)[0]\n","  dim       = K.int_shape(mu)[1]\n","  eps       = K.random_normal(shape=(batch, dim))\n","  return mu + K.exp(sigma / 2) * eps\n","\n","# Use reparameterization trick to ....??\n","z       = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([mu, sigma])\n","\n","# Instantiate encoder\n","encoder = Model(i, [mu, sigma, z], name='encoder')\n","encoder.summary()\n","\n","# =================\n","# Decoder\n","# =================\n","\n","# Definition\n","d_i   = Input(shape=(latent_dim, ), name='decoder_input')\n","x     = Dense(conv_shape[1] * conv_shape[2] * conv_shape[3], activation='relu')(d_i)\n","x     = BatchNormalization()(x)\n","\n","x     = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n","cx    = Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n","cx    = BatchNormalization()(cx)\n","\n","cx    = Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(cx)\n","cx    = Dropout(0.4)(cx)\n","\n","\n","\n","\n","\n","o     = Conv2DTranspose(filters=num_channels, kernel_size=3, activation='sigmoid', padding='same', name='decoder_output')(cx)\n","\n","# Instantiate decoder\n","decoder = Model(d_i, o, name='decoder')\n","decoder.summary()\n","\n","# =================\n","# VAE as a whole\n","# =================\n","\n","# Instantiate VAE\n","vae_outputs = decoder(encoder(i)[2])\n","vae         = Model(i, vae_outputs, name='vae')\n","vae.summary()\n","\n","# Define loss\n","def kl_reconstruction_loss(true, pred):\n","  # Reconstruction loss\n","  reconstruction_loss = tf.keras.losses.binary_crossentropy(K.flatten(true), K.flatten(pred)) * img_width * img_height * 3 \n","  # KL divergence loss\n","  kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n","  kl_loss = K.sum(kl_loss, axis=-1)\n","  kl_loss *= -0.5\n","  # Total loss = 50% rec + 50% KL divergence loss\n","  return K.mean(reconstruction_loss + kl_loss)\n","\n","# Compile VAE\n","vae.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss=kl_reconstruction_loss, metrics=[\"accuracy\"])\n","\n","# Train autoencoder\n","history_VAE=vae.fit(input_train, input_train, epochs = 500, batch_size = batch_size, validation_data = (input_test,input_test))\n","\n","# =================\n","# Results visualization\n","# Credits for original visualization code: https://keras.io/examples/variational_autoencoder_deconv/\n","# (François Chollet).\n","# Adapted to accomodate this VAE.\n","# =================\n","\n","# Plot results\n","data = (input_test, target_test)'''\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EHPgRtNPfNsP"},"source":[]},{"cell_type":"code","metadata":{"id":"dX4dmrJXZjon"},"source":["#plot_history(history_VAE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fws7lant7UE3"},"source":["#plot_history_loss(history_VAE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vE6iiwRhKs2B"},"source":["#Labels_name=np.unique(per_level['type'])\n","#Labels_name[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZ6sBYnxA_tw"},"source":["'''x = []\n","y = []\n","z = []\n","#test=np.argmax(target_test,axis=1)\n","test=target_test\n","\n","for i in range(len(input_test)):\n","    z.append(test[i])\n","    op,_,_= encoder.predict(np.array([input_test[i]]))\n","    x.append(op[0][0])\n","    y.append(op[0][1])\n","df = pd.DataFrame()\n","df['x'] = x\n","df['y'] = y\n","df['z'] = [str(Labels_name[k]) for k in z]\n","plt.figure(figsize=(15, 15))\n","sns.scatterplot(x='x', y='y', hue='z', data=df)\n","plt.show()'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-SH1_TJtcy6h"},"source":["  '''input_data, target_data = data\n","  mu1, _, _= encoder.predict(input_data)\n","Y_test.shape'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tXylyPaR9FR8"},"source":["#input_data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i2mwpNFGEvHW"},"source":["\n","\n","'''\n","Z_mean,_,_ = encoder.predict(input_data, batch_size=batch_size)\n","plt.figure(figsize=(15, 10))\n","plt.scatter(Z_mean[:, 0], Z_mean[:, 1], c=target_data)\n","plt.colorbar()\n","plt.show()'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1mMfU6M8FW6m"},"source":["#target_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PrTdJdmgZVSb"},"source":["'''def viz_latent_space(encoder, data):\n","  input_data, target_data = data\n","  mu, _, _ = encoder.predict(input_data,batch_size=batch_size)\n","  fig=plt.figure(figsize=(15, 15))\n","  ax = fig.add_subplot(111, projection='3d')\n","  plt_sct=ax.scatter(mu[:, 1], mu[:, 0],mu[:, 1], c=target_data,edgecolor='black')\n","  ax.set_xlabel('x - dim 1')\n","  ax.set_ylabel('y - dim 2')\n","  ax.set_zlabel('z - dim 3')\n","\n","  plt.colorbar(plt_sct)\n","  plt.show()\n","\n","def viz_decoded(encoder, decoder, data):\n","  num_samples = 20\n","  figure = np.zeros((img_width * num_samples, img_height * num_samples, num_channels))\n","  grid_x = np.linspace(-10, 10, num_samples)\n","  grid_y = np.linspace(-10, 10, num_samples)[::-1]\n","  for i, yi in enumerate(grid_y):\n","      for j, xi in enumerate(grid_x):\n","          z_sample = np.array([[xi, yi]])\n","          x_decoded = decoder.predict(z_sample)\n","          digit = x_decoded[0].reshape(img_width, img_height, num_channels)\n","          figure[i * img_width: (i + 1) * img_width,\n","                  j * img_height: (j + 1) * img_height] = digit\n","  plt.figure(figsize=(10, 10))\n","  start_range = img_width // 2\n","  end_range = num_samples * img_width + start_range + 1\n","  pixel_range = np.arange(start_range, end_range, img_width)\n","  sample_range_x = np.round(grid_x, 1)\n","  sample_range_y = np.round(grid_y, 1)\n","  plt.xticks(pixel_range, sample_range_x)\n","  plt.yticks(pixel_range, sample_range_y)\n","  plt.xlabel('z - dim 1')\n","  plt.ylabel('z - dim 2')\n","  # matplotlib.pyplot.imshow() needs a 2D array, or a 3D array with the third dimension being of shape 3 or 4!\n","  # So reshape if necessary\n","  fig_shape = np.shape(figure)\n","  if fig_shape[2] == 1:\n","    figure = figure.reshape((fig_shape[0], fig_shape[1]))\n","  # Show image\n","  plt.imshow(figure)\n","  plt.show()\n","  return figure'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pQNES_7mbSBe"},"source":["#viz_latent_space(encoder, data)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YjCTkvYwGo79"},"source":["#fiq=viz_decoded(encoder, decoder, data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-rn4vboJKQB"},"source":["  '''num_samples = 20\n","  samples=[]\n","  figure = np.zeros((img_width * num_samples, img_height * num_samples, num_channels))\n","  grid_x = np.linspace(-10, 10, num_samples)\n","  grid_y = np.linspace(-10, 10, num_samples)\n","  for i, yi in enumerate(grid_y):\n","      for j, xi in enumerate(grid_x):\n","          z_sample = np.array([[xi, yi]])\n","          x_decoded = decoder.predict(z_sample)\n","          digit = x_decoded[0].reshape(img_width, img_height, num_channels)'''\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1mhxHRmqWsMA"},"source":["#digit.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e53AlO4P2Rr9"},"source":["#plt.imshow(digit)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wStBRLnsgtas"},"source":["'''labels=per_level['type'].values\n","\n","image=digit.flatten()\n","from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n","sentence_embeddings = model.encode(lis)\n","query_vec = image\n","print(query_vec)'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xNdwTptEuss"},"source":["\n","'''image=digit.flatten()\n","from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n","sentence_embeddings = model.encode(lis)\n","query_vec = image\n","print(image.shape)\n","for i,sent in enumerate(lis):\n","  sim = cosine(query_vec, model.encode(sent))\n","  if sim >0.5:\n","      print(\"Sentence = \", i,sim,sent,labels[i])'''\n","  \n","  \n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZI6taKj2nlC"},"source":["'''X_train = xtrain.reshape(xtrain.shape[0],8, 8, 12)\n","X_test = xvalid.reshape(xvalid.shape[0], 8, 8, 12)\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"paACKmX22e09"},"source":["'''X_train=np.expand_dims(X_train,-1)\n","X_test=np.expand_dims(X_test,-1)\n","X_test.shape'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYxXUPYxB7q4"},"source":["'''import os\n","import zipfile\n","import numpy as np\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","def get_model(width=8, height=8, depth=12):\n","    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n","\n","    inputs = keras.Input((width, height, depth, 1))\n","\n","    x = layers.Conv3D(filters=128, kernel_size=3,padding='same', activation=\"relu\")(inputs)\n","    x = layers.MaxPool3D(pool_size=1)(x)\n","    x = layers.BatchNormalization()(x)\n","\n","\n","    x = layers.Conv3D(filters=256, kernel_size=3,padding='same', activation=\"relu\")(x)\n","    x = layers.MaxPool3D(pool_size=1)(x)\n","    x = layers.BatchNormalization()(x)\n","\n","    x = layers.GlobalAveragePooling3D()(x)\n","    x = layers.Dense(units=512, activation=\"relu\")(x)\n","    x = layers.Dropout(0.6)(x)\n","\n","    outputs = layers.Dense(units=5, activation=\"softmax\")(x)\n","\n","    # Define the model.\n","    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n","    return model\n","\n","\n","# Build model.\n","model = get_model(width=8, height=8, depth=12)\n","model.summary()'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xEIgISRxkCP"},"source":["initial_learning_rate = 0.00001\n","\n","model.compile(\n","    loss=\"categorical_crossentropy\",\n","    optimizer=keras.optimizers.Adam(learning_rate=initial_learning_rate),\n","    metrics=[\"accuracy\"])\n","\n","\n","\n","history_CNN3=model.fit(X_train, ytrain, epochs = 50, batch_size = 64, validation_data = (X_test,yvalid),shuffle=True)\n","# Train the model, doing validation at the end of each epoch\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0RV1Xz7z3urC"},"source":["plot_history(history_CNN3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o4Meor0Z2Cx4"},"source":["model = Sequential()\n","# convolutional layer\n","model.add(Convolution2D(128, kernel_size=(3,3),padding='same', activation='relu', input_shape=(16,16,3)))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","\n","model.add(Convolution2D(128, kernel_size=(3,3),padding='same', activation='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Dropout(0.5))\n","\n","# flatten output of conv\n","model.add(Flatten())\n","# hidden layer\n","\n","model.add(Dense(100, activation='relu'))\n","model.add(Dropout(0.7))\n","\n","\n","\n","\n","\n","# output layer\n","model.add(tf.keras.layers.Dense(5,activation='softmax'))\n","#myLoss='mean_squared_error'\n","\n","#IR_sch=tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3,decay_steps=1000,decay_rate=0.9)\n","\n","#optimizer=tf.keras.optimizers.Adam(learning_rate=IR_sch)\n","metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","              tf. keras.metrics.FalsePositives(name='fp'),\n","              tf.keras.metrics.TrueNegatives(name='tn'),\n","              tf.keras.metrics.FalseNegatives(name='fn'),\n","              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","              tf.keras.metrics.Precision(name='precision'),\n","              tf.keras.metrics.Recall(name='recall'),\n","              tf.keras.metrics.AUC(name='auc')]\n","metrics2=[\"accuracy\"]\n","model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03),metrics=metrics1)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"51zfpR9PhHUk"},"source":["#monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4,patience=4, verbose=4 , mode='min')\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n","#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n","\n","history_CNN=model.fit(X_train,Y_train,verbose=1,batch_size=256,epochs=100,validation_data=(X_test,Y_test),callbacks=[reduce_lr])\n","plot_metrics(history_CNN)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZDyE2YTdCiB"},"source":["**Classifying permissions using  Conv1d**"]},{"cell_type":"code","metadata":{"id":"0ThGuGHu7l0x"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J3DuDxU5A1Uh"},"source":["\n","metric_lstm = {}\n","for i in metrics :\n","    name=i.capitalize()+'_' +'lstm'\n","    metric_lstm.update({name:history_lstm.history[i]})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0GnquPNcEnrV"},"source":["metrics = ['tp', 'fp', 'tn','fn']\n","histoiry_model=['history_c1','history_CNN']\n","metric_c1 = {}\n","for i in metrics :\n","    name=i.capitalize()+'_' +'CONV1D'\n","    metric_c1.update({name:history_c1.history[i]})\n","    \n","  \n","\n","metric_lstm = {}\n","for i in metrics :\n","    name=i.capitalize()+'_' +'lstm'\n","    metric_lstm.update({name:history_lstm.history[i]})\n","metric_lstm['Fn_lstm']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_EK7bk3BnmZ"},"source":["\n","metrics = ['tp', 'fp', 'tn','fn']\n","histoiry_model=['history_c1','history_CNN']\n","metric_c1 = {}\n","for i in metrics :\n","    name=i.capitalize()+'_' +'CONV1D'\n","    metric_c1.update({name:history_c1.history[i]})\n","    \n","  \n","\n","metric_lstm = {}\n","for i in metrics :\n","    name=i.capitalize()+'_' +'lstm'\n","    metric_lstm.update({name:history_lstm.history[i]})\n","metric_lstm['Fn_lstm']\n","names=['Fp_CONV1D','Fp_lstm','FN_CONV1D','FN_Lstm','TN_CONV1D','TN_lstm','Tp_CONV1D','TP_lstm']\n","plt.figure(figsize=[10,3])\n","\n","plt.bar(0,metric_c1['Fp_CONV1D'][-1],tick_label=names[0],width=0.2)\n","plt.bar(1,metric_lstm['Fp_lstm'][-1],tick_label=names[1],width=0.2)\n","plt.bar(2,metric_c1['Fn_CONV1D'][-1],tick_label=names[2],width=0.2)\n","plt.bar(3,metric_lstm['Fn_lstm'][-1],tick_label=names[3],width=0.2)\n","#plt.bar(2.5,np.min(metric_c1['Tn_CONV1D']),tick_label=names[4],width=0.2)\n","#plt.bar(3,np.min(metric_lstm['Tn_lstm']),tick_label=names[5],width=0.2)\n","#plt.bar(3.5,np.min(metric_c1['Tp_CONV1D']),tick_label=names[6],width=0.2)\n","#plt.bar(4,np.mean(metric_lstm['Tp_lstm']),tick_label=names[7],width=0.2)\n","\n","\n","\n","plt.xticks(range(0,4),names)\n","plt.ylabel('false postive rate and false negtive rate ')\n","plt.xlabel('Models')\n","\n","plt.savefig('model.png')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pN-P8Pa4EVvj"},"source":["\n","metrics = ['tp', 'fp', 'tn','fn']\n","histoiry_model=['history_c1','history_CNN']\n","metric_c1 = {}\n","for i in metrics :\n","    name=i.capitalize()+'_' +'CONV1D'\n","    metric_c1.update({name:history_c1.history[i]})\n","    \n","  \n","\n","metric_lstm = {}\n","for i in metrics :\n","    name=i.capitalize()+'_' +'lstm'\n","    metric_lstm.update({name:history_lstm.history[i]})\n","metric_lstm['Fn_lstm']\n","names=['Fp_CONV1D','Fp_lstm','FN_CONV1D','FN_Lstm','TN_CONV1D','TN_lstm','Tp_CONV1D','TP_lstm']\n","plt.figure(figsize=[10,3])\n","\n","plt.bar(0,metric_c1['Fp_CONV1D'][-1],tick_label=names[0],width=0.2)\n","plt.bar(1,metric_lstm['Fp_lstm'][-1],tick_label=names[1],width=0.2)\n","plt.bar(2,metric_c1['Fn_CONV1D'][-1],tick_label=names[2],width=0.2)\n","plt.bar(3,metric_lstm['Fn_lstm'][-1],tick_label=names[3],width=0.2)\n","#plt.bar(2.5,np.min(metric_c1['Tn_CONV1D']),tick_label=names[4],width=0.2)\n","#plt.bar(3,np.min(metric_lstm['Tn_lstm']),tick_label=names[5],width=0.2)\n","#plt.bar(3.5,np.min(metric_c1['Tp_CONV1D']),tick_label=names[6],width=0.2)\n","#plt.bar(4,np.mean(metric_lstm['Tp_lstm']),tick_label=names[7],width=0.2)\n","\n","\n","\n","plt.xticks(range(0,4),names)\n","plt.ylabel('false postive rate and false negtive rate ')\n","plt.xlabel('Models')\n","\n","plt.savefig('model.png')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vm5SovtWZC97"},"source":["Data=pd.read_csv('/content/drive/My Drive/Data.csv')\n","Data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4YKvC559dqV6"},"source":["**classifying permissions+featuers +activites  using Glov pretrained model **"]},{"cell_type":"code","metadata":{"id":"Xa0efQ9yhaSN"},"source":["\n","Data['Count']=Data['Context'].apply(lambda x:len(x.strip().replace('android.permission.','').replace('android.','').replace(',','.').replace('_','.').split('.')))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6mEEiflqlqE"},"source":["Data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"code","id":"4ml-dK79gKg3"},"source":["\n","#@title Default title text\n","Data['Context']=Data['Context'].apply(lambda x: x.strip().replace('android.permission.','').replace('android.','').replace('sub','').replace('mumayi','').replace('permission','').replace('GET','').replace(' Activity','').replace('com','').replace('ACCESS','').replace(',','.').replace('_','.').split('.'))\n","lis=[]\n","for perm in Data['Context']:\n","    s=' '\n","    for m in perm:\n"," \n","      \n","      if len(m.strip()) >3:\n","          #print(m,len(m.strip()))\n","          s=s+' ' +m +''\n","          s= re.sub(\"[^-9A-Za-z ]\", \"\" ,s)\n","          s= re.sub(\"[Activity^]\", \"\" ,s)\n","    lis.append(s.strip())\n","permis=lis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Um7OMKsjF8Pk"},"source":["permis[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"glzAEnVWiBGT"},"source":["Data['process']=lis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r3hy5o4biLL7"},"source":["Data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ym-IVfHTZ0hk"},"source":["from keras.preprocessing.sequence import pad_sequences\n","max_length = max([len(s.split()) for s in Data['process']])\n","max_length"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jFFT5iuviJy0"},"source":["Data_ferq=Data[Data['Count']>1500]\n","Data_ferq['process']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xNJ7sCrZ5tu"},"source":["Data_ferq_m=Data[Data['Count']<301]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JvrN3Z1aKVLw"},"source":["import seaborn as sns\n","\n","sns.countplot(x='type', data=Data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ELojsSqlhWhn"},"source":["train,vocab=tokn(Data['process'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqtqIK9hmbCK"},"source":["import  tensorflow as tf \n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","encoder=LabelEncoder()\n","encode_label=encoder.fit_transform(Data['type'])\n","labels_full=to_categorical(encode_label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kqr9uJN8mPSX"},"source":["perm_train,perm_test, y_train, y_test = train_test_split( train, labels_full, test_size=0.33,shuffle=True)\n","perm_train[0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OVS5zy4LX_o"},"source":["perm_train_C.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5BjgHtOyLVGe"},"source":["from numpy import array\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers.core import Activation, Dropout, Dense\n","from keras.layers import Flatten, LSTM\n","from keras.layers import GlobalMaxPooling1D\n","from keras.models import Model\n","from keras.layers.embeddings import Embedding\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.layers import Input\n","from keras.layers.merge import Concatenate\n","\n","import pandas as pd\n","import numpy as np\n","import re\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXlAqhZRmqWD"},"source":["from keras.preprocessing.sequence import pad_sequences\n","\n","#maxlen_char  = max_length_char\n","#X_train_char = pad_sequences(perm_train_ch, padding='post', maxlen=maxlen_char)\n","#X_test_char = pad_sequences(perm_test_ch, padding='post', maxlen=maxlen_char)\n","\n","maxlen = 500\n","\n","X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n","X_train[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oPuKTbuMIj9H"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RTPXb8Qzmxtn"},"source":["\n","epochs = 50\n","model=model_lstm(200,vocab,maxlen)\n","\n","\n","batch_size = 256\n","\n","metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","              tf. keras.metrics.FalsePositives(name='fp'),\n","              tf.keras.metrics.TrueNegatives(name='tn'),\n","              tf.keras.metrics.FalseNegatives(name='fn'),\n","              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","              tf.keras.metrics.Precision(name='precision'),\n","              tf.keras.metrics.Recall(name='recall'),\n","              tf.keras.metrics.AUC(name='auc')]\n","metrics2=[\"accuracy\"]\n","model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03),metrics=metrics1)\n","model.summary()\n","history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n","#loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n","#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n","plot_history(history_lstm)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vo6BoY3awYyV"},"source":["def conv_1d():\n","    model_c1 = Sequential()\n","    model_c1.add(Embedding(vocab, 100, input_length=maxlen))\n","    \n","    model_c1.add(Dropout(0.5))\n","   \n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.9))\n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.9))\n","    model_c1.add(Flatten())\n","    \n","\n","    model_c1.add(Dense(5, activation='softmax'))\n","\n","    print(model_c1.summary())\n","    return model_c1\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tyJNeoiOnDwj"},"source":["metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","              tf. keras.metrics.FalsePositives(name='fp'),\n","              tf.keras.metrics.TrueNegatives(name='tn'),\n","              tf.keras.metrics.FalseNegatives(name='fn'),\n","              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","              tf.keras.metrics.Precision(name='precision'),\n","              tf.keras.metrics.Recall(name='recall'),\n","              tf.keras.metrics.AUC(name='auc')]\n","model_=conv_1d()\n","metrics2=[\"accuracy\"]\n","model_.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.NAdam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=metrics2)\n","# fit network\n","from keras.callbacks import ReduceLROnPlateau\n","#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n","\n","history_c1=model_.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O-6e2PJEw6NC"},"source":["plot_history(history_c1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dn7xMxAi2Ndp"},"source":["model = Sequential()\n","def cnn2():\n","\n","    #inp = Input(shape=(maxlen, ))\n","  #model.add(Embedding(vocab, 100, input_length=maxlen))\n","    #x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n","  \n","  model.add(Convolution1D(filters=128, kernel_size=3, padding='same', activation='relu'))\n","  model.add(MaxPooling1D(pool_size=3))\n","  model.add(Dropout(0.7))\n","  model.add(Convolution1D(filters=128, kernel_size=3, padding='same', activation='relu'))\n","  model.add(MaxPooling1D(pool_size=3))\n","  model.add(Dropout(0.7))\n","  \n","  model.add(tf.keras.layers.GRU(512, return_sequences=True, dropout=0.4,\n","                           recurrent_dropout=0.4))\n","                      \n","  model.add(Dropout(0.4))\n","    #x = Dropout(dropout_rate)(x)\n","\n","  model.add(Flatten())\n","  model.add(Dense(128, activation=\"relu\"))\n","  model.add(Dropout(0.6))\n","  model.add(Dense(5, activation=\"softmax\"))\n","\n","  model.summary()  \n"," \n","  return model "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJTZQaylU5n_"},"source":["model_=cnn2()\n","metrics2=[\"accuracy\"]\n","\n","# fit network\n","from keras.callbacks import ReduceLROnPlateau\n","#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","history_c1=model_.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a2BaVR0-eEuR"},"source":["**Clsiifying permissions using Glove pretrainned models **"]},{"cell_type":"code","metadata":{"id":"_buyb6b-WVhs"},"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip -q glove.6B.zip\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ByyvOLnLm5aY"},"source":["labels_full.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5PWpYMLxhwdm"},"source":["samples=PREM\n","samples[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DK8hLPNAmIqd"},"source":["'''# Shuffle the data\n","seed = 100\n","rng = np.random.RandomState(seed)\n","rng.shuffle(samples)\n","rng = np.random.RandomState(seed)\n","rng.shuffle(Data['encod_lab'].values)\n","labels=Data['encod_lab'].values\n","# Extract a training & validation split\n","validation_split = 0.32\n","num_validation_samples = int(validation_split * len(samples))\n","train_samples = samples[:-num_validation_samples]\n","val_samples = samples[-num_validation_samples:]\n","train_labels = labels[:-num_validation_samples]\n","val_labels = labels[-num_validation_samples:]'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DLrBxKsGiQ4D"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HF5vY50gUQG"},"source":["Datafrme=pd.DataFrame({'perm':samples,'Count':Count_perm})\n","Datafrme"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HK8_d2qif2QC"},"source":["train_samples, val_samples, train_labels, val_labels = train_test_split( Datafrme,labels_full, test_size=0.33,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7EEdZSM3cMwz"},"source":["train_samples_count=train_samples['Count'].values\n","train_samples=train_samples.drop(['Count'],axis=1)\n","val_samples_count=val_samples['Count'].values\n","val_samples=val_samples.drop(['Count'],axis=1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fIJRxqX8-0s8"},"source":["from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","\n","vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=250)\n","text_ds = tf.data.Dataset.from_tensor_slices(Datafrme['perm'])\n","vectorizer.adapt(text_ds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6QpJl8cdMp8"},"source":["vectorizer.get_vocabulary()[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XTKUcV9keEDo"},"source":["voc = vectorizer.get_vocabulary()\n","word_index = dict(zip(voc, range(len(voc))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5V12IBjueWNm"},"source":["test = [\"read\",\"access\"]\n","[word_index[w] for w in test]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wml1eiLBedGe"},"source":["embeddings_index = {}\n","with open('glove.6B.200d.txt') as f:\n","    for line in f:\n","        word, coefs = line.split(maxsplit=1)\n","        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","        embeddings_index[word] = coefs\n","\n","print(\"Found %s word vectors.\" % len(embeddings_index))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TkTHoN1EeuIb"},"source":["num_tokens = len(voc) + 2\n","embedding_dim = 200\n","hits = 0\n","misses = 0\n","\n","# Prepare embedding matrix\n","embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # Words not found in embedding index will be all-zeros.\n","        # This includes the representation for \"padding\" and \"OOV\"\n","        embedding_matrix[i] = embedding_vector\n","        hits += 1\n","    else:\n","        misses += 1\n","print(\"Converted %d words (%d misses)\" % (hits, misses))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GIXlFqVBe6Xc"},"source":["from tensorflow.keras.layers import Embedding\n","\n","embedding_layer = Embedding(\n","    num_tokens,\n","    embedding_dim,\n","    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n","    trainable=False,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OUl4BKNGpepm"},"source":["input_2 = Input(shape=(1,))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B19U_1h6e_GI"},"source":["\n","dense_layer_1 = Dense(5, activation='relu')(input_2)\n","dense_layer_2 = Dense(1, activation='relu')(dense_layer_1)\n","int_sequences_input =tf.keras.Input(shape=(None,), dtype=\"int64\")\n","embedded_sequences = embedding_layer(int_sequences_input)\n","x1 = tf.keras.layers.LSTM(256,dropout=0.1 ,return_sequences = True)(embedded_sequences)\n","x1 = tf.keras.layers.LSTM(256,dropout=0.1 ,return_sequences = True)(x1)\n","LSTM_Layer_1 = LSTM(512)(x1)\n","concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_2])\n","dense_layer_3 = Dense(256, activation='relu')(concat_layer)\n","#concatted = tf.keras.layers.Multiply()([x1, x2])\n","dd = Dense(150, activation=\"relu\")(dense_layer_3)\n","\n","preds = Dense(5, activation=\"softmax\")(dd)\n","#model = tf.keras.Model(int_sequences_input, preds)\n","model = Model(inputs=[int_sequences_input, input_2], outputs=preds)\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxk3yQVAmgN0"},"source":["x_train = vectorizer(np.array([[s] for s in train_samples['perm']])).numpy()\n","x_val = vectorizer(np.array([[s] for s in val_samples['perm']])).numpy()\n","\n","y_train = np.array(train_labels)\n","y_val = np.array(val_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2oNxifObmzCU"},"source":["from keras.utils import plot_model\n","plot_model(model, to_file='model_plot3.png', show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLELar3xUNDz"},"source":["model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UVEjGF_jTv5J"},"source":["history_count = model.fit([x_train, train_samples_count], y=y_train, batch_size=128, epochs=100, verbose=1, validation_data=([x_val,val_samples_count],y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jf0Fs96qrWU4"},"source":["plot_history(history_count)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hyt_AznNluw6"},"source":["labels=np.argmax(labels_full,axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vG1YhMoRTutm"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w_mXhbsLjvD2"},"source":["\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","\n","\n","class WeightedCategoricalCrossentropy(CategoricalCrossentropy):\n","    \n","    def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):\n","        assert cost_mat.ndim == 2\n","        assert cost_mat.shape[0] == cost_mat.shape[1]\n","        \n","        super().__init__(name=name, **kwargs)\n","        self.cost_mat = K.cast_to_floatx(cost_mat)\n","    \n","    def __call__(self, y_true, y_pred, sample_weight=None):\n","        assert sample_weight is None, \"should only be derived from the cost matrix\"\n","      \n","        return super().__call__(\n","            y_true=y_true,\n","            y_pred=y_pred,\n","            sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),\n","        )\n","\n","\n","def get_sample_weights(y_true, y_pred, cost_m):\n","    num_classes = len(cost_m)\n","   \n","    y_pred.shape.assert_has_rank(2)\n","    y_pred.shape[1:].assert_is_compatible_with(num_classes)\n","    y_pred.shape.assert_is_compatible_with(y_true.shape)\n","\n","    y_pred = K.one_hot(K.argmax(y_pred), num_classes)\n","\n","    y_true_nk1 = K.expand_dims(y_true, 2)\n","    y_pred_n1k = K.expand_dims(y_pred, 1)\n","    cost_m_1kk = K.expand_dims(cost_m, 0)\n","\n","    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k\n","    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])\n","\n","    return sample_weights_n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1YWGkEL1Mo0"},"source":["from sklearn.utils import class_weight\n","weight_arr = class_weight.compute_class_weight('balanced',\n","                                                 np.unique(labels),\n","                                                 labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bfJnx4Hk1Qw0"},"source":["weights=np.ones((5,5))\n","weights[0,:]=weight_arr[0]\n","weights[1,:]=weight_arr[1]\n","weights[2,:]=weight_arr[2]\n","weights[3,:]=weight_arr[3]\n","weights[4,:]=weight_arr[4]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"legRGv5X1dfB"},"source":["weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jFmBFclCzRJ"},"source":["batch_size=128\n","trainingsize =len(PREM)\n","validate_size = x_val.shape[0]\n","def calculate_spe(y):\n","  return int(np.ceil((1. * y) / batch_size)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cD5fLr2TDdSr"},"source":["steps_per_epoch = calculate_spe(trainingsize)\n","validation_steps = calculate_spe(validate_size)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mZLWw2Rjmluz"},"source":["\n","model.compile(loss=WeightedCategoricalCrossentropy(weights),optimizer=tf.keras.optimizers.Adam(0.001), metrics=[\"accuracy\"])\n","\n","history_Glov=model.fit(x_train, y_train, batch_size=128,epochs=50, steps_per_epoch=steps_per_epoch,validation_steps=validation_steps,validation_data=(x_val, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YgX60DmOe9ZY"},"source":["plot_history(history_Glov)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gjFNNNNIkHwh"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-XOYSyWgCwa"},"source":["label=np.argmax(labels_full,axis=1)\n","labels =np.unique(per_level['type'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ll9olpR2mq3Y"},"source":["type1, type2,type3,type4,type5 = np.bincount(label)\n","total=type1+type2+type3+type4+type5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lG6HdHPveuTs"},"source":["print('Examples:\\n    Total: {}\\n    adware: {} ({:.2f}% of total)\\n'.format(\n","    total, type1, 100 * type1 / total))\n","print('Examples:\\n    Total: {}\\n    Banking: {} ({:.2f}% of total)\\n'.format(\n","    total, type2, 100 * type2 / total))\n","print('Examples:\\n    Total: {}\\n    Benign: {} ({:.2f}% of total)\\n'.format(\n","    total, type3, 100 * type3 / total))\n","print('Examples:\\n    Total: {}\\n    Riskware: {} ({:.2f}% of total)\\n'.format(\n","    total, type4, 100 * type4 / total))\n","print('Examples:\\n    Total: {}\\n    SMS: {} ({:.2f}% of total)\\n'.format(\n","    total, type5, 100 * type5 / total))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M43gr9TUV_p4"},"source":["using Attention machnisem "]},{"cell_type":"code","metadata":{"id":"r-d1sALPXFGY"},"source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten\n","from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Convolution1D\n","from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5tbxEzEEhtE"},"source":["MAX_FEATURES = num_tokens\n","EMBED_SIZE = 256  \n","RNN_CELL_SIZE = 256\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QhkqrPFqZ99O"},"source":["class Attention(tf.keras.Model):\n","    def __init__(self, units):\n","        super(Attention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, features, hidden):\n","        # hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        # we are doing this to perform addition to calculate the score\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","          \n","        # score shape == (batch_size, max_length, 1)\n","        # we get 1 at the last axis because we are applying score to self.V\n","        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","        score = tf.nn.tanh(\n","            self.W1(features) + self.W2(hidden_with_time_axis))\n","        # attention_weights shape == (batch_size, max_length, 1)\n","        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n","          \n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * features\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        return context_vector, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FMGhVpxEZ-vK"},"source":["embedded_layer =  Embedding(\n","    num_tokens,\n","    embedding_dim,\n","    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n","    trainable=True,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUGfoCLlaPAD"},"source":["sequence_input =tf.keras.Input(shape=(None,), dtype=\"int64\")\n","embedded_sequences = embedded_layer(sequence_input)\n","\n","lstm = Bidirectional(LSTM(RNN_CELL_SIZE,dropout=0.5 ,return_sequences = True), name=\"bi_lstm_0\")(embedded_sequences)\n","dropout0 = SpatialDropout1D(0.7)(lstm)\n","\n","lstm2 = Bidirectional(LSTM(RNN_CELL_SIZE,dropout=0.5 ,return_sequences = True), name=\"bi_lstm_1\")(dropout0)\n","dropout1 = SpatialDropout1D(0.7)(lstm2)\n","\n","\n","\n","# Getting our LSTM outputs\n","(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(RNN_CELL_SIZE, dropout=0.,return_sequences=True, return_state=True), name=\"bi_lstm_2\")(dropout1)\n","state_h = Concatenate()([forward_h, backward_h])\n","state_c = Concatenate()([forward_c, backward_c])\n","context_vector, attention_weights = Attention(10)(lstm, state_h)\n","dense=Dense(30, activation=\"relu\")(context_vector)\n","dropout = Dropout(0.6)(dense)\n","output = Dense(5, activation=\"softmax\")(dropout)\n","  \n","model = keras.Model(inputs=sequence_input, outputs=output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"22dQsDYJa9sc"},"source":["print(model.summary())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rAQOfP4wbRm5"},"source":["keras.utils.plot_model(model, show_shapes=True, dpi=90)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldm2iTPkbZOI"},"source":["\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=tf.keras.optimizers.Adam(0.001),\n","              metrics='accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83Zlj2Dqbgza"},"source":["BATCH_SIZE = 256\n","EPOCHS = 20\n","history = model.fit(x_train, y_train,\n","                    batch_size=BATCH_SIZE,\n","                    epochs=EPOCHS,\n","                    validation_data=(x_val, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"49xCq9f8deqh"},"source":["plot_history(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZSG5PMSbsQ4"},"source":["prediction = model.predict(x_val)\n","y_pred = (prediction)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m3oCWAkLd8iZ"},"source":["plot_multiclass_roc(model, x_val, y_val, n_classes=5, figsize=(5,5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KRf233WIet6N"},"source":["plot_metrics(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3OVhJwhfBA3"},"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","Y_prediction = model.predict(x_val,verbose=1)\n","# Convert predictions classes to one hot vectors \n","\n","\n","Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n","# Convert validation observations to one hot vectors\n","Y_true = np.argmax(y_val,axis = 1) \n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n","plt.figure(figsize=(10,8))\n","sns.heatmap(confusion_mtx, annot=True, fmt=\"d\",xticklabels=np.unique(per_level.type.values),yticklabels=np.unique(per_level.type.values));\n","plt.title('Confusion matrix')\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uQuqyrkBGjL7"},"source":["**binary vectorizations **"]},{"cell_type":"code","metadata":{"id":"O1Xrzpn2PbdL"},"source":["per_level"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ojp5mmSdPcBW"},"source":["#per_char=pd.read_csv('/content/drive/My Drive/Char_perm.csv')\n","per_level=pd.read_csv('/content/drive/My Drive/permissions.csv')\n","np.unique(per_level.type)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lz_4ySeKFwOQ"},"source":["name='Adware'\n","per_level=per_level[per_level['type']==name]\n","\n","import  tensorflow as tf \n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","encoder=LabelEncoder()\n","encode_label=encoder.fit_transform(per_level['type'])\n","labels_full=to_categorical(encode_label)\n","labels=np.unique(per_level.type).tolist()\n","labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aN7687b-NNyy"},"source":["#per_char=pd.read_csv('/content/drive/My Drive/Char_perm.csv')\n","per_level=pd.read_csv('/content/drive/My Drive/permissions.csv')\n","np.unique(per_level.type)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"01Wajg9aXlOn"},"source":["per_level.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRcH7OfnFHTa"},"source":["'''adware=per_level[per_level['type']=='Adware']\n","adware=adware['permmisions'].apply(lambda x:x.split(','))\n","SMS=per_level[per_level['type']=='Adware']\n","SMS=SMS['permmisions'].apply(lambda x:x.split(','))\n","Riskware=per_level[per_level['type']=='Riskware']\n","Riskware=Riskware['permmisions'].apply(lambda x:x.split(','))\n","Banking=per_level[per_level['type']=='Banking']\n","Banking=Banking['permmisions'].apply(lambda x:x.split(','))'''\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8IrMonENQ1j"},"source":["\n","\n","Full_permissions=[ ]\n","lis_u=set()\n","lis_length=[ ]\n","for perm in perm_split:\n","    for p in perm.split(' ') :\n","        if p!='' or p!='nan' :\n","             lis_u.add(p)\n","     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eP4SNYNi_NbS"},"source":["data= pd.DataFrame(list(lis_u))\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SUgFd8RUoH14"},"source":["\n","lis_u=set()\n","lis_length=[ ]\n","for perm in perm_split:\n","    for p in perm.split(' ') :\n","        if p!='' or p!='nan' :\n","             lis_u.add(p)\n","     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BDN0hEcEFquQ"},"source":["'''\n","per_level['binary']='Malicious'\n","per_level\n","\n","for i,m in enumerate (per_level['type']):\n","    if per_level['type'][i]=='Benign':\n","        per_level['binary'][i]='B'\n","    else: per_level['binary'][i]='M'''\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O3ZyY4IQdqpD"},"source":["\n","lis_value=()\n","dic={}\n","for i,perm in enumerate(perm_split):\n","      lis_value=[ ]\n","      for p in list(lis_u) :\n","           if p in perm:\n","              lis_value.append(1) \n","           else:\n","               lis_value.append(0) \n","      dic.update({i:lis_value})\n","  \n","   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VW23_X3O79Sn"},"source":["!pip install nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_zjX0qSK71J6"},"source":["import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(perm_split)\n","\n","freq = np.ravel(X.sum(axis=0))\n","import operator\n","# get vocabulary keys, sorted by value\n","vocab = [v[0] for v in sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))]\n","fdist = dict(zip(vocab, freq)) # return same format as nltk\n","fdist\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v_iCGNdVcXXU"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2hRkrU249T3y"},"source":["data=pd.DataFrame(fdist,columns=['permissions','Count'])\n","data['permissions']=fdist.keys()\n","data['Count']=fdist.values()\n","data.to_csv(name+'.csv')\n","data=data[data['Count']>300]\n","plt.figure(figsize=(10,20))\n","data.Count.plot(kind='pie',startangle=90,autopct='%1.1f%%',colors=['C0','C1','C2','C3'],labels = fdist.keys(),textprops={'fontweight':'bold','fontsize': 8});\n","plt.legend(loc=3,fontsize=8)\n","plt.ylabel('')\n","plt.title('permissions in Riskware ',fontweight=\"bold\",fontsize = 15)\n","plt.axis('equal')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oLEzlC68oVSe"},"source":["#per_char=pd.read_csv('/content/drive/My Drive/Char_perm.csv')\n","per_level=pd.read_csv('/content/drive/My Drive/permissions.csv')\n","per_level['permmisions']=per_level['permmisions'].apply(lambda x:str(x).lower())\n","permiss=per_level['permmisions']\n","perm_split=per_level['permmisions'].apply(lambda x:x.replace(',','').replace('android.permission.',' ').strip().split('.')[-1])\n","Full_permissions=[ ]\n","lis_u=set()\n","lis_length=[ ]\n","for perm in perm_split:\n","    for p in perm.split(' ') :\n","        if p!='' or p!='nan' :\n","             lis_u.add(p)\n","     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDyQWoHqoa64"},"source":["\n","lis_value=()\n","dic={}\n","for i,perm in enumerate(perm_split):\n","      lis_value=[ ]\n","      for p in list(lis_u) :\n","           if p in perm:\n","              lis_value.append(1) \n","           else:\n","               lis_value.append(0) \n","      dic.update({i:lis_value})\n","  \n","   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hEdJgASWvXMq"},"source":["data=pd.DataFrame(dic.values(),columns=list(lis_u))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pt7uxrOVpHZS"},"source":["'''\n","per_level['binary']='Malicious'\n","per_level\n","\n","for i,m in enumerate (per_level['type']):\n","    if per_level['type'][i]=='Benign':\n","        per_level['binary'][i]='B'\n","    else: per_level['binary'][i]='M'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jz9lkirEDZ0a"},"source":["#Full_data=pd.concat([data,per_level['binary']],axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TMqyQVMwDzlo"},"source":["#Full_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPJCJAKM5xeX"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppm7OI2RFCb2"},"source":["\n","\n","#data=data.drop(['type'],axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aL_2mYqLu-_L"},"source":["train_data=data.values\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iHusp54zDyMR"},"source":["labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGlPaVaEGLnF"},"source":["\n","X_train, X_test, y_train, y_test = train_test_split( train_data,labels_full, test_size=0.33,shuffle=True)\n","X_train=np.expand_dims(X_train,-1)\n","X_test=np.expand_dims(X_test,-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yNk8ZI6rF_l-"},"source":["\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split( train_data,labels_full, test_size=0.33,shuffle=True)\n","X_train=np.expand_dims(X_train,-1)\n","X_test=np.expand_dims(X_test,-1)\n","X_train.shape,X_test.shape\n","def Lst_model(X_t):\n","    model=Sequential()  \n","\n","    model.add((LSTM(128,dropout=0.5,input_shape=X_t.shape[1:],return_sequences=True)))\n","\n","    model.add(SpatialDropout1D(0.6))\n","\n","\n","    model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n","    model.add(SpatialDropout1D(0.7))\n","    \n","    \n","\n","\n","    model.add(Flatten())\n","    #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n","    model.add(Dense(maxlen, activation='relu'))\n","    model.add(Dropout(0.8))\n","    model.add(Dense(5, activation='softmax'))\n","    adam = tf.keras.optimizers.Adam(0.0001)\n","    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n","                            patience=5, verbose=1, mode='auto')\n","    metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","            tf. keras.metrics.FalsePositives(name='fp'),\n","            tf.keras.metrics.TrueNegatives(name='tn'),\n","            tf.keras.metrics.FalseNegatives(name='fn'),\n","            tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","            tf.keras.metrics.Precision(name='precision'),\n","            tf.keras.metrics.Recall(name='recall'),\n","            tf.keras.metrics.AUC(name='auc')]\n","    model.compile( optimizer=adam,loss='categorical_crossentropy',metrics=metrics1)                        \n","    model.summary()\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nzF6p9ABGF62"},"source":["model=Lst_model(X_train)\n","history_lstm=model.fit(X_train, y_train, epochs =40, batch_size =256, validation_data = (X_test,y_test),shuffle=True)\n","plot_history(history_lstm)\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","Y_prediction = model.predict(X_test,verbose=1,batch_size=256)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JeV7UeEKGQzW"},"source":["\n","\n","def conv_1d(X_t):\n","    model_c1 = Sequential()\n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,input_shape=X_t.shape[1:],padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Flatten())\n","    \n","    model_c1.add(Dense(100, activation='relu'))\n","\n","    model_c1.add(Dense(5, activation='softmax'))\n","\n","    print(model_c1.summary())\n","    return model_c1\n","\n","metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","              tf. keras.metrics.FalsePositives(name='fp'),\n","              tf.keras.metrics.TrueNegatives(name='tn'),\n","              tf.keras.metrics.FalseNegatives(name='fn'),\n","              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","              tf.keras.metrics.Precision(name='precision'),\n","              tf.keras.metrics.Recall(name='recall'),\n","              tf.keras.metrics.AUC(name='auc')]\n","model_c1=conv_1d(X_train)\n","metrics2=[\"accuracy\"]\n","model_c1.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=metrics1)\n","# fit network\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n","#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n","\n","history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4RPD2_31UGkH"},"source":["plot_history(history_c1)\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","Y_prediction = model.predict(X_test,verbose=1,batch_size=256)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QH6rohkVPNHs"},"source":["\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KNW9N69cVCA-"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A3lA_XiVVEji"},"source":["\n","# Convert predictions classes to one hot vectors \n","\n","\n","Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n","# Convert validation observations to one hot vectors\n","Y_true = np.argmax(y_test,axis = 1) \n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3MSLYpJ6VNPx"},"source":["plt.figure(figsize=(10,8))\n","sns.heatmap(confusion_mtx, annot=True, fmt=\"d\",xticklabels=np.unique(per_level.binary.values),yticklabels=np.unique(per_level.binary.values));\n","plt.title('Confusion matrix')\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CEL1u7512PZu"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn.datasets import load_digits\n","from sklearn.svm import SVC\n","from sklearn.model_selection import validation_curve\n","\n","param_range = np.logspace(-10, -1, 20)\n","train_scores, test_scores = validation_curve(\n","    SVC(),train_data,np.argmax(labels_full,axis=1), param_name=\"gamma\", param_range=param_range,\n","    scoring=\"accuracy\",cv=6, n_jobs=1)\n","train_scores_mean = np.mean(train_scores, axis=1)\n","train_scores_std = np.std(train_scores, axis=1)\n","test_scores_mean = np.mean(test_scores, axis=1)\n","test_scores_std = np.std(test_scores, axis=1)\n","\n","plt.title(\"Validation Curve with SVM\")\n","plt.xlabel(r\"$\\gamma$\")\n","plt.ylabel(\"Score\")\n","plt.ylim(0.0, 1.1)\n","lw = 2\n","plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n","             color=\"darkorange\", lw=lw)\n","plt.fill_between(param_range, train_scores_mean - train_scores_std,\n","                 train_scores_mean + train_scores_std, alpha=0.2,\n","                 color=\"darkorange\", lw=lw)\n","plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n","             color=\"navy\", lw=lw)\n","plt.fill_between(param_range, test_scores_mean - test_scores_std,\n","                 test_scores_mean + test_scores_std, alpha=0.2,\n","                 color=\"navy\", lw=lw)\n","plt.legend(loc=\"best\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pPvB160ab5qv"},"source":["def test_gamma():\n","\n","    from sklearn import svm \n","    from sklearn.model_selection import train_test_split\n","    from sklearn.linear_model import LogisticRegression\n","    from sklearn.metrics import accuracy_score\n","    from sklearn.svm import SVC\n","    from sklearn.metrics import mean_squared_error,log_loss\n","    import random\n","    from sklearn.preprocessing import label_binarize\n","    \n","    X_train, X_test, y_train, y_test = train_test_split(train_data,np.argmax(labels_full,axis=1),test_size=0.30,shuffle=True)\n","    model = svm.SVC(C=10)\n","    fig,ax=plt.subplots(ncols=2,figsize=(20,10))\n","    kernels=['rbf','linear']\n","    for i,k in enumerate(kernels):\n","        gamma = np.linspace(0.01,1.0, num = 100)\n","        scores = []\n","        train=[]\n","        for n in gamma:\n","            print(i,n,k)\n","\n","            model.set_params(gamma=n,kernel=k)\n","            model.fit(X_train,y_train)\n","            scores.append(model.score(X_test, y_test))\n","            train.append(model.score(X_train, y_train))\n","        print(scores)\n","        print(train)\n","        ax[i].set_title('Effect of gamma'+' '+k)\n","        ax[i].set_xlabel('gamma value ')\n","        ax[i].set_ylabel('train /test score')\n","        ax[i].plot(gamma,train)\n","        ax[i].plot(gamma, scores)\n","        ax[i].legend([\"train_Score\",\"Accurecy_Score\"],loc=\"upper left\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7z1M_BvKhSfg"},"source":["def test_Error():\n","\n","    from sklearn import svm \n","    from sklearn.model_selection import train_test_split\n","    from sklearn.linear_model import LogisticRegression\n","    from sklearn.metrics import accuracy_score\n","    from sklearn.svm import SVC\n","    from sklearn.metrics import mean_squared_error,log_loss\n","    import random\n","    from sklearn.preprocessing import label_binarize\n","    \n","    X_train, X_test, y_train, y_test = train_test_split(train_data,labels_full,test_size=0.30,shuffle=True)\n","    model = svm.SVC(C=1)\n","    fig,ax=plt.subplots(ncols=3,figsize=(10,20))\n","    kernels=['rbf','linear']\n","    for i,k in enumerate(kernels):\n","        C = np.arange(1,10, num = 10)\n","        scores = []\n","        train=[]\n","        for n in Error:\n","            print(i,n,k)\n","            model.set_params(C=n,kernel=k)\n","            model.fit(X_train,y_train)\n","            scores.append(model.score(X_test, y_test))\n","            train.append(model.score(X_train, y_train))\n","        print(scores)\n","        print(train)\n","        ax[i].set_title('Effect of Error'+' '+k)\n","        ax[i].set_xlabel('Error value ')\n","        ax[i].set_ylabel('train /test score')\n","        ax[i].plot(gamma,train)\n","        ax[i].plot(gamma, scores)\n","        ax[i].legend([\"train_Score\",\"Accurecy_Score\"],loc=\"upper left\")\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJ_q6oHghsOO"},"source":["test_Error()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3oRIqubz7rsZ"},"source":["X_train, X_test, y_train, y_test = train_test_split( train_data,labels_full, test_size=0.33,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ywbm3bCZX4Yi"},"source":["from numpy import loadtxt\n","from xgboost import XGBClassifier\n","from xgboost import plot_importance\n","from matplotlib import pyplot\n","# load data\n","\n","X = X_train\n","y = np.argmax(y_train,axis=1)\n","# fit model no training data\n","model_featuers = XGBClassifier(learning_rate =0.1,\n"," n_estimators=1000,\n"," max_depth=5,\n"," min_child_weight=1,\n"," gamma=0,\n"," subsample=0.8,\n"," colsample_bytree=0.8,\n"," nthread=4,\n"," scale_pos_weight=1,\n"," seed=27)\n","model_featuers.fit(X, y)\n","plot_importance(model_featuers)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-O-lvR8MqUzC"},"source":["list_featuers=[]\n","for i,f in enumerate(model_featuers.feature_importances_):\n","   if f > 0:\n","      list_featuers.append(i)\n","print(list_featuers)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"efXWT1fvr52U"},"source":["featuers=data.columns.tolist()\n","fliter=[]\n","for i,m in enumerate(featuers):\n","  for m in list_featuers:\n","    if i == m:\n","       fliter.append(featuers[m])\n","fliter\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sdhFm3fx4lDi"},"source":["X_train=np.expand_dims(X_train,-1)\n","X_test=np.expand_dims(X_test,-1)\n","X_train.shape,X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Of1ucNiAf3m"},"source":["  def cnnn(X_t):\n","      model=Sequential()\n","      model.add(Input(shape=(X_t.shape[1:])))\n","      model.add(Convolution1D(filters=256, kernel_size=3, padding='same', activation='relu'))\n","      model.add(MaxPooling1D(pool_size=3))\n","      model.add(Dropout(0.2))\n"," \n","      #x = Dropout(dropout_rate)(x)\n","\n","      model.add(Flatten())\n","      model.add(Dense(5, activation=\"softmax\"))\n","      initial_learning_rate = 0.001\n","\n","      model.compile(\n","          loss=\"categorical_crossentropy\",\n","          optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),\n","          metrics=[\"accuracy\"])\n","      return model\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uBTxMCH4iBkD"},"source":["X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXzHqFXAqH_1"},"source":["X_train, X_test, y_train, y_test = train_test_split( train_data,labels_full, test_size=0.33,shuffle=True)\n","y_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-R0vFvzZ6Fgx"},"source":["\n","# Fit model using each importance as a threshold\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_selection import SelectFromModel\n","\n","thresholds_m= np.sort(model_featuers.feature_importances_)\n","li=[]\n","for th in thresholds_m :\n","    if th > 0.001:\n","      li.append(th)\n","thresholds=np.sort(li)       "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXYI0vEK-Mvm"},"source":["plt.hist(thresholds,bins=len(li))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5BI9njQ9REBJ"},"source":["X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HK3iZog1eCad"},"source":["history_value={}\n","for thresh in thresholds:\n","    # select features using threshold\n","    \n","    selection = SelectFromModel(model_featuers, threshold=thresh, prefit=True)\n","    select_X_train = selection.transform(X_train)\n","    print(select_X_train.shape)\n","    # train model\n","    selection_model = Lst_model(np.expand_dims(select_X_train,axis=1))\n","    select_X_test = selection.transform(X_test)\n","    \n","    hist=selection_model.fit(np.expand_dims(select_X_train,axis=1), y_train,epochs = 100, batch_size = 256, \n","                    validation_data = (np.expand_dims(select_X_test,axis=1),y_test),shuffle=True)\n","    history_value.update({str(thresh):hist})\n","    # eval model\n","    y_pred = selection_model.predict(np.expand_dims(select_X_test,axis=1))\n","    predictions = [np.round(value) for value in y_pred]\n","    accuracy = accuracy_score(y_test, predictions)\n","    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1a_w6Xt7HCC3"},"source":["for n in history_value.keys():\n","    plot_history(history_value[n])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3h4W2dQ6EiAf"},"source":["names = list(history.keys())\n","values = list(history.values())\n","plt.figure(figsize=[10,5])\n","for n in names :\n","    print(n)\n","    plt.plot(history[n].history)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hWsGOg46992"},"source":["\n","\n","\n","\n","history_CNN1=model.fit(X_train, y_train, epochs = 50, batch_size = 256, validation_data = (X_test,y_test),shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OwoJv8MQ9fnO"},"source":["plot_history(history_CNN1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9RdL09cejuA3"},"source":["pip install google-play-scraper\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oITfDGLr5Qm3"},"source":["import pandas as pd\n","data1=pd.read_csv('mal.csv')\n","featuers=pd.read_csv('driben_featuer.csv')\n","data2=pd.read_csv('driben.csv').fillna(0)\n","data=pd.concat([data1,data2],axis=0).fillna(0)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mpI2_rhZlzTc"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wE7gncIVCQTo"},"source":["data#perm=featuers[featuers['API call signature']=='Manifest Permission']['transact'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I4ZrrZetEMdy"},"source":["m ={}\n","c=0\n","for j in range(data1.shape[0]-1):\n","  s=' '\n","  for per in data1.head(0):\n","     if data1[per][j]==1  :\n","        s= per +' '+s\n","  c=j\n","  m.update({j:s})\n","print(s)\n","    \n","\n","        \n","            "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9HcXPXro1Tb"},"source":["d={}\n","for j in range(data2.shape[0]-1):\n","  s=' '\n","  for per in data2.head(0):\n","     if data2[per][j]==1  :\n","        s= per +' '+s\n","  d.update({j:s})\n","print(s)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJnF3oMYNH2I"},"source":["d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k8_tG6n2GDA_"},"source":["M_data=pd.DataFrame({\"index\": m.keys(), \"perm\": m.values()})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zzPN8gvTqdQz"},"source":["d_data=pd.DataFrame({\"index\": d.keys(), \"perm\": d.values()})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4rL5tT-Q-qu"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bz8Hz-MTOigB"},"source":["M_data['type']=data1['class']\n","d_data['type']=data2['class']\n","M_data=pd.concat([M_data,d_data],axis=0)\n","M_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ar41e6CWQCAM"},"source":[" import numpy as np \n"," np.unique(M_data['type'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2MoT7uh8QXFG"},"source":["per_level=M_data\n","import  tensorflow as tf \n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","encoder=LabelEncoder()\n","Full=encoder.fit_transform(M_data['type'])\n","Full_=to_categorical(Full)\n","labels=np.unique(per_level.type).tolist()\n","labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-eYlgIigUg7w"},"source":["Full_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O2Af8cBNQtjn"},"source":["per_level['perm']=per_level['perm'].apply(lambda x:str(x))\n","permiss=per_level['perm']\n","from keras.preprocessing.sequence import pad_sequences\n","max_length = max([len(s.replace('.','-').split('_')) for s in permiss])\n","max_length"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gSMWFqbJS06m"},"source":["train,vocab_size_word=tokn(permiss)\n","\n","train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ijZ6jPqSRRSn"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9yTXYqnmTqpt"},"source":["train,vocab_size_word=tokn(permiss)\n","perm_train,perm_test, y_train, y_test = train_test_split( train, Full_, test_size=0.30,random_state=42,shuffle=True)\n","from keras.preprocessing.sequence import pad_sequences\n","maxlen = 200\n","X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n","X_train[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yqRuQb3pRpug"},"source":["\n","def model_lstm(embedding_dim ,vocab_size,maxlen):\n","      model = Sequential()\n","      model.add(Embedding(input_dim=vocab_size, \n","                                output_dim=embedding_dim, \n","                                input_length=maxlen))\n","      model.add(SpatialDropout1D(0.3))\n","\n","      model.add((LSTM(128,dropout=0.5,return_sequences=True)))\n","\n","      model.add(SpatialDropout1D(0.6))\n","\n","\n","      model.add((LSTM(128,dropout=0.5, return_sequences=True)))\n","      model.add(SpatialDropout1D(0.7))\n","      \n","      \n","\n","\n","      model.add(Flatten())\n","      #model.add(RandomFourierFeatures(output_dim=500, scale=1.0, kernel_initializer=\"gaussian\"))\n","      model.add(Dense(maxlen, activation='relu'))\n","      model.add(Dropout(0.8))\n","      model.add(Dense(2, activation='softmax'))\n","      return model\n","epochs = 50\n","model=model_lstm(100,vocab_size_word,maxlen)\n","model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics= metrics1)\n","model.summary()\n","batch_size = 256\n","history_lstm = model.fit(X_train, y_train, epochs=epochs, batch_size=256,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)])\n","#loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n","#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FFeAz2m3TRzA"},"source":["plot_history(history_lstm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JROzP3wulBAc"},"source":["plot_multiclass_roc(model,X_test,y_test,2,[10,10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eg_TkTy5a8fN"},"source":["train,vocab_size_word=tokn(permiss)\n","perm_train,perm_test, y_train, y_test = train_test_split( train, Full_, test_size=0.33,random_state=42,shuffle=True)\n","from keras.preprocessing.sequence import pad_sequences\n","maxlen = 200\n","X_train = pad_sequences(perm_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(perm_test, padding='post', maxlen=maxlen)\n","X_train[0]\n","\n","def conv_1d():\n","    model_c1 = Sequential()\n","    model_c1.add(Embedding(vocab_size_word,100, input_length=maxlen))\n","    \n","    model_c1.add(Dropout(0.4))\n","   \n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Convolution1D(filters=128, kernel_size=3,padding='same', activation='relu'))\n","    model_c1.add(MaxPooling1D(pool_size=3))\n","    model_c1.add(Dropout(0.5))\n","    model_c1.add(Flatten())\n","    \n","    model_c1.add(Dense(100, activation='relu'))\n","\n","    model_c1.add(Dense(2, activation='softmax'))\n","\n","    print(model_c1.summary())\n","    return model_c1\n","\n","metrics1=[tf.keras.metrics.TruePositives(name='tp'),\n","              tf. keras.metrics.FalsePositives(name='fp'),\n","              tf.keras.metrics.TrueNegatives(name='tn'),\n","              tf.keras.metrics.FalseNegatives(name='fn'),\n","              tf. keras.metrics.CategoricalAccuracy(name='accuracy'),\n","              tf.keras.metrics.Precision(name='precision'),\n","              tf.keras.metrics.Recall(name='recall'),\n","              tf.keras.metrics.AUC(name='auc')]\n","model_c1=conv_1d()\n","metrics2=[\"accuracy\"]\n","model_c1.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001,beta_1=0.9, beta_2=0.9, epsilon=1e-03), metrics=metrics1)\n","# fit network\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n","#model.fit(X_train, Y_train, callbacks=[reduce_lr])\n","\n","history_c1=model_c1.fit(X_train,y_train,verbose=1,batch_size=256,epochs=50,validation_data=(X_test,y_test),callbacks=[reduce_lr])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqDB5oe4f9LQ"},"source":["plot_multiclass_roc(model_c1,X_test,y_test,2,[10,10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-AF4e_9TkOdn"},"source":["plot_history(history_c1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eR0rg_xLAdN0"},"source":["Riskware=pd.read_csv('Riskware.csv')['permissions']\n","\n","SMS=pd.read_csv('SMS.csv')['permissions']\n","Adware=pd.read_csv('Adware.csv')['permissions']\n","Benign=pd.read_csv('Benign.csv')['permissions']\n","Bancking=pd.read_csv('Banking.csv')['permissions']\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cF5yEuPsf9WR"},"source":["def m (s):\n","   b=set()\n","   for i,p in enumerate(s):\n","     b.add(p)\n","   return b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zh-2dDhmf-EB"},"source":["SM=m(SMS)\n","Ad=m(Adware)\n","BN=m(Bancking)\n","Rk=m(Riskware)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gK9JdwjBqik-"},"source":["SM"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d4t8CTMzeqW3"},"source":["Malware = Rk a\n","Malware"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qsQSQ5fbfRWO"},"source":["len(SM)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8FjlG8CyjX-7"},"source":["len(BN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ObCTsYTjhJM"},"source":["!git clone https://github.com/AdamGreenhill/VirusShare-Search.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"911QhMXRsOMQ"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VaXShBu-scH8"},"source":["import argparse\n","import os\n","import time\n","from urllib.request import urlopen\n","\n","def downloader(directory, iteration):\n","\t# Downloads given URL\n","\turl = 'https://virusshare.com/hashfiles/VirusShare_%05d.md5' % iteration\n","\texists = os.path.isfile(directory + (\"\\VirusShare_%05d.md5\" % iteration))\n","\t\n","\tif not exists:\n","\t\tprint(\"  Downloading {0} into {1}...\".format(url, directory))\n","\n","\t\tfile_path = os.path.join(directory, os.path.basename(url))\n","\t\tcontents = urlopen(url)\n","\t\tfile_output = open(file_path,'wb')\n","\t\tfile_output.write(contents.read())\n","\t\tfile_output.close()\n","\telse: \tprint(\"Skipping \" + directory + (\"\\VirusShare_%05d.md5\" % iteration))\n","\n","\ttime.sleep(1)\n","\n","\n","def find_missing(directory, latest):\n","\t# find all files, parse files for end number, remove any files from 'to_find'\n","\tto_find = list(range(0,latest+1))\n","\tfor i in os.listdir(directory):\n","\t\tto_find.remove(int(''.join(c for c in i if c.isdigit())[:5]))\n","\treturn to_find\n","\n","def parse_amount(amount):\n","\tto_find = []\n","\ttry:\n","\t\tif ',' in amount:\n","\t\t\t# if a comma seperation (e.g. 10,11,12) is specified\n","\t\t\ttemp = amount.split(',')\n","\t\t\tfor i in temp:\n","\t\t\t\tto_find.append(int(i))\n","\t\t\treturn to_find\n","\t\telif '-' in amount:\n","\t\t\t# if a range (e.g. 10-20) is specified\n","\t\t\ttemp = amount.split('-')\n","\t\t\tfor i in range(int(temp[0]),int(temp[1]) + 1):\n","\t\t\t\tto_find.append(i)\n","\t\t\treturn to_find\n","\t\telse:\n","\t\t\t# if a single number (e.g. 123) is specified\n","\t\t\tto_find.append(int(amount))\n","\t\t\treturn to_find\n","\texcept ValueError:\n","\t\tprint(\"  ERROR: incorrect value given for update range.\")\n","\t\texit()\n","\n","def update(directory, amount, latest):\n","\ttry:\n","\t\tl = int(latest)\n","\texcept ValueError:\n","\t\tprint(\"  ERROR: incorrect value given for latest hash release.\")\n","\t\texit()\n","\n","\tif amount == \"all\":\n","\t\t# Downloads all md5 files\n","\t\tfor i in range(0,l):\n","\t\t\tdownloader(directory,i)\n","\telif amount == \"missing\":\n","\t\t# Finds all md5 files not in a directory\n","\t\tto_find = find_missing(directory, l)\n","\t\tfor i in to_find:\n","\t\t\tdownloader(directory,i)\n","\telse:\n","\t\t# Parses amount...\n","\t\tto_find = parse_amount(amount)\n","\t\tfor i in to_find:\n","\t\t\tdownloader(directory,i)\n","\n","def search(directory, term):\n","\tcounter = 1\n","\tfor file_to_search in os.listdir(directory):\n","\t\tfull_file_path = os.path.join(directory, file_to_search)\n","\t\tif os.path.isfile(full_file_path):\n","\t\t\twith open(full_file_path) as f:\n","\t\t\t\tfor line in f:\n","\t\t\t\t\tif term in line:\n","\t\t\t\t\t\tprint('FOUND|{0}|{1}|{2}'.format(term,file_to_search, counter))\n","\t\t\t\t\t\treturn\n","\t\t\t\t\tcounter += 1\n","\t\tcounter = 1\n","\tprint('     |{0}|{1}|{2}'.format(term,\"None                \", -1))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I-VjMoxHw64F"},"source":["!git clone https://github.com/AdamGreenhill/VirusShare-Search.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oXiTQ97aw15Y"},"source":["!python '/content/VirusShare-Search/VirusShare-Search.py' --update all"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1XafbwnTx5FG"},"source":["!python '/content/VirusShare-Search/VirusShare-Search.py' -s VirusShare_00003.md5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HsbaBf-EvVgx"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KFyEMXlwvW8u"},"source":[],"execution_count":null,"outputs":[]}]}